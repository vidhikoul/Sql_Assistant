URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax.html
SQL Syntax - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SQL Syntax Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements. DDL Statements Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements: ALTER DATABASE ALTER TABLE ALTER VIEW CREATE DATABASE CREATE FUNCTION CREATE TABLE CREATE VIEW DROP DATABASE DROP FUNCTION DROP TABLE DROP VIEW REPAIR TABLE TRUNCATE TABLE USE DATABASE DML Statements Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements: INSERT TABLE INSERT OVERWRITE DIRECTORY LOAD Data Retrieval Statements Spark supports SELECT statement that is used to retrieve rows from one or more tables according to the specified clauses. The full syntax and brief description of supported clauses are explained in SELECT section. The SQL statements related to SELECT are also included in this section. Spark also provides the ability to generate logical and physical plan for a given query using EXPLAIN statement. SELECT Statement Common Table Expression CLUSTER BY Clause DISTRIBUTE BY Clause GROUP BY Clause HAVING Clause Hints Inline Table File JOIN LIKE Predicate LIMIT Clause OFFSET Clause ORDER BY Clause Set Operators SORT BY Clause TABLESAMPLE Table-valued Function WHERE Clause Aggregate Function Window Function CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause LATERAL SUBQUERY TRANSFORM Clause EXPLAIN Auxiliary Statements ADD FILE ADD JAR ANALYZE TABLE CACHE TABLE CLEAR CACHE DESCRIBE DATABASE DESCRIBE FUNCTION DESCRIBE QUERY DESCRIBE TABLE LIST FILE LIST JAR REFRESH REFRESH TABLE REFRESH FUNCTION RESET SET SHOW COLUMNS SHOW CREATE TABLE SHOW DATABASES SHOW FUNCTIONS SHOW PARTITIONS SHOW TABLE EXTENDED SHOW TABLES SHOW TBLPROPERTIES SHOW VIEWS UNCACHE TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax.html#ddl-statements
SQL Syntax - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SQL Syntax Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements. DDL Statements Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements: ALTER DATABASE ALTER TABLE ALTER VIEW CREATE DATABASE CREATE FUNCTION CREATE TABLE CREATE VIEW DROP DATABASE DROP FUNCTION DROP TABLE DROP VIEW REPAIR TABLE TRUNCATE TABLE USE DATABASE DML Statements Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements: INSERT TABLE INSERT OVERWRITE DIRECTORY LOAD Data Retrieval Statements Spark supports SELECT statement that is used to retrieve rows from one or more tables according to the specified clauses. The full syntax and brief description of supported clauses are explained in SELECT section. The SQL statements related to SELECT are also included in this section. Spark also provides the ability to generate logical and physical plan for a given query using EXPLAIN statement. SELECT Statement Common Table Expression CLUSTER BY Clause DISTRIBUTE BY Clause GROUP BY Clause HAVING Clause Hints Inline Table File JOIN LIKE Predicate LIMIT Clause OFFSET Clause ORDER BY Clause Set Operators SORT BY Clause TABLESAMPLE Table-valued Function WHERE Clause Aggregate Function Window Function CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause LATERAL SUBQUERY TRANSFORM Clause EXPLAIN Auxiliary Statements ADD FILE ADD JAR ANALYZE TABLE CACHE TABLE CLEAR CACHE DESCRIBE DATABASE DESCRIBE FUNCTION DESCRIBE QUERY DESCRIBE TABLE LIST FILE LIST JAR REFRESH REFRESH TABLE REFRESH FUNCTION RESET SET SHOW COLUMNS SHOW CREATE TABLE SHOW DATABASES SHOW FUNCTIONS SHOW PARTITIONS SHOW TABLE EXTENDED SHOW TABLES SHOW TBLPROPERTIES SHOW VIEWS UNCACHE TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax.html#dml-statements
SQL Syntax - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SQL Syntax Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements. DDL Statements Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements: ALTER DATABASE ALTER TABLE ALTER VIEW CREATE DATABASE CREATE FUNCTION CREATE TABLE CREATE VIEW DROP DATABASE DROP FUNCTION DROP TABLE DROP VIEW REPAIR TABLE TRUNCATE TABLE USE DATABASE DML Statements Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements: INSERT TABLE INSERT OVERWRITE DIRECTORY LOAD Data Retrieval Statements Spark supports SELECT statement that is used to retrieve rows from one or more tables according to the specified clauses. The full syntax and brief description of supported clauses are explained in SELECT section. The SQL statements related to SELECT are also included in this section. Spark also provides the ability to generate logical and physical plan for a given query using EXPLAIN statement. SELECT Statement Common Table Expression CLUSTER BY Clause DISTRIBUTE BY Clause GROUP BY Clause HAVING Clause Hints Inline Table File JOIN LIKE Predicate LIMIT Clause OFFSET Clause ORDER BY Clause Set Operators SORT BY Clause TABLESAMPLE Table-valued Function WHERE Clause Aggregate Function Window Function CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause LATERAL SUBQUERY TRANSFORM Clause EXPLAIN Auxiliary Statements ADD FILE ADD JAR ANALYZE TABLE CACHE TABLE CLEAR CACHE DESCRIBE DATABASE DESCRIBE FUNCTION DESCRIBE QUERY DESCRIBE TABLE LIST FILE LIST JAR REFRESH REFRESH TABLE REFRESH FUNCTION RESET SET SHOW COLUMNS SHOW CREATE TABLE SHOW DATABASES SHOW FUNCTIONS SHOW PARTITIONS SHOW TABLE EXTENDED SHOW TABLES SHOW TBLPROPERTIES SHOW VIEWS UNCACHE TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax.html#data-retrieval-statements
SQL Syntax - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SQL Syntax Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements. DDL Statements Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements: ALTER DATABASE ALTER TABLE ALTER VIEW CREATE DATABASE CREATE FUNCTION CREATE TABLE CREATE VIEW DROP DATABASE DROP FUNCTION DROP TABLE DROP VIEW REPAIR TABLE TRUNCATE TABLE USE DATABASE DML Statements Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements: INSERT TABLE INSERT OVERWRITE DIRECTORY LOAD Data Retrieval Statements Spark supports SELECT statement that is used to retrieve rows from one or more tables according to the specified clauses. The full syntax and brief description of supported clauses are explained in SELECT section. The SQL statements related to SELECT are also included in this section. Spark also provides the ability to generate logical and physical plan for a given query using EXPLAIN statement. SELECT Statement Common Table Expression CLUSTER BY Clause DISTRIBUTE BY Clause GROUP BY Clause HAVING Clause Hints Inline Table File JOIN LIKE Predicate LIMIT Clause OFFSET Clause ORDER BY Clause Set Operators SORT BY Clause TABLESAMPLE Table-valued Function WHERE Clause Aggregate Function Window Function CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause LATERAL SUBQUERY TRANSFORM Clause EXPLAIN Auxiliary Statements ADD FILE ADD JAR ANALYZE TABLE CACHE TABLE CLEAR CACHE DESCRIBE DATABASE DESCRIBE FUNCTION DESCRIBE QUERY DESCRIBE TABLE LIST FILE LIST JAR REFRESH REFRESH TABLE REFRESH FUNCTION RESET SET SHOW COLUMNS SHOW CREATE TABLE SHOW DATABASES SHOW FUNCTIONS SHOW PARTITIONS SHOW TABLE EXTENDED SHOW TABLES SHOW TBLPROPERTIES SHOW VIEWS UNCACHE TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax.html#auxiliary-statements
SQL Syntax - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SQL Syntax Spark SQL is Apache Spark’s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements. DDL Statements Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements: ALTER DATABASE ALTER TABLE ALTER VIEW CREATE DATABASE CREATE FUNCTION CREATE TABLE CREATE VIEW DROP DATABASE DROP FUNCTION DROP TABLE DROP VIEW REPAIR TABLE TRUNCATE TABLE USE DATABASE DML Statements Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements: INSERT TABLE INSERT OVERWRITE DIRECTORY LOAD Data Retrieval Statements Spark supports SELECT statement that is used to retrieve rows from one or more tables according to the specified clauses. The full syntax and brief description of supported clauses are explained in SELECT section. The SQL statements related to SELECT are also included in this section. Spark also provides the ability to generate logical and physical plan for a given query using EXPLAIN statement. SELECT Statement Common Table Expression CLUSTER BY Clause DISTRIBUTE BY Clause GROUP BY Clause HAVING Clause Hints Inline Table File JOIN LIKE Predicate LIMIT Clause OFFSET Clause ORDER BY Clause Set Operators SORT BY Clause TABLESAMPLE Table-valued Function WHERE Clause Aggregate Function Window Function CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause LATERAL SUBQUERY TRANSFORM Clause EXPLAIN Auxiliary Statements ADD FILE ADD JAR ANALYZE TABLE CACHE TABLE CLEAR CACHE DESCRIBE DATABASE DESCRIBE FUNCTION DESCRIBE QUERY DESCRIBE TABLE LIST FILE LIST JAR REFRESH REFRESH TABLE REFRESH FUNCTION RESET SET SHOW COLUMNS SHOW CREATE TABLE SHOW DATABASES SHOW FUNCTIONS SHOW PARTITIONS SHOW TABLE EXTENDED SHOW TABLES SHOW TBLPROPERTIES SHOW VIEWS UNCACHE TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-alter-database.html
ALTER DATABASE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions ALTER DATABASE Description ALTER DATABASE statement changes the properties or location of a database. Please note that the usage of DATABASE , SCHEMA and NAMESPACE are interchangeable and one can be used in place of the others. An error message is issued if the database is not found in the system. ALTER PROPERTIES ALTER DATABASE SET DBPROPERTIES statement changes the properties associated with a database. The specified property values override any existing value with the same property name. This command is mostly used to record the metadata for a database and may be used for auditing purposes. Syntax ALTER { DATABASE | SCHEMA | NAMESPACE } database_name SET { DBPROPERTIES | PROPERTIES } ( property_name = property_value [ , ... ] ) Parameters database_name Specifies the name of the database to be altered. ALTER LOCATION ALTER DATABASE SET LOCATION statement changes the default parent-directory where new tables will be added for a database. Please note that it does not move the contents of the database’s current directory to the newly specified location or change the locations associated with any tables/partitions under the specified database (available since Spark 3.0.0 with the Hive metastore version 3.0.0 and later). Syntax ALTER { DATABASE | SCHEMA | NAMESPACE } database_name SET LOCATION 'new_location' Parameters database_name Specifies the name of the database to be altered. Examples -- Creates a database named `inventory`. CREATE DATABASE inventory ; -- Alters the database to set properties `Edited-by` and `Edit-date`. ALTER DATABASE inventory SET DBPROPERTIES ( 'Edited-by' = 'John' , 'Edit-date' = '01/01/2001' ); -- Verify that properties are set. DESCRIBE DATABASE EXTENDED inventory ; + -------------------------+------------------------------------------+ | database_description_item | database_description_value | + -------------------------+------------------------------------------+ | Database Name | inventory | | Description | | | Location | file : / temp / spark - warehouse / inventory . db | | Properties | (( Edit - date , 01 / 01 / 2001 ), ( Edited - by , John )) | + -------------------------+------------------------------------------+ -- Alters the database to set a new location. ALTER DATABASE inventory SET LOCATION 'file:/temp/spark-warehouse/new_inventory.db' ; -- Verify that a new location is set. DESCRIBE DATABASE EXTENDED inventory ; + -------------------------+-------------------------------------------+ | database_description_item | database_description_value | + -------------------------+-------------------------------------------+ | Database Name | inventory | | Description | | | Location | file : / temp / spark - warehouse / new_inventory . db | | Properties | (( Edit - date , 01 / 01 / 2001 ), ( Edited - by , John )) | + -------------------------+-------------------------------------------+ Related Statements DESCRIBE DATABASE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-alter-table.html
ALTER TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions ALTER TABLE Description ALTER TABLE statement changes the schema or properties of a table. RENAME ALTER TABLE RENAME TO statement changes the table name of an existing table in the database. The table rename command cannot be used to move a table between databases, only to rename a table within the same database. If the table is cached, the commands clear cached data of the table. The cache will be lazily filled when the next time the table is accessed. Additionally: the table rename command uncaches all table’s dependents such as views that refer to the table. The dependents should be cached again explicitly. the partition rename command clears caches of all table dependents while keeping them as cached. So, their caches will be lazily filled when the next time they are accessed. Syntax ALTER TABLE table_identifier RENAME TO table_identifier ALTER TABLE table_identifier partition_spec RENAME TO partition_spec Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec Partition to be renamed. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) ADD COLUMNS ALTER TABLE ADD COLUMNS statement adds mentioned columns to an existing table. Syntax ALTER TABLE table_identifier ADD COLUMNS ( col_spec [ , ... ] ) Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name COLUMNS ( col_spec ) Specifies the columns to be added. DROP COLUMNS ALTER TABLE DROP COLUMNS statement drops mentioned columns from an existing table. Note that this statement is only supported with v2 tables. Syntax ALTER TABLE table_identifier DROP { COLUMN | COLUMNS } [ ( ] col_name [ , ... ] [ ) ] Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name col_name Specifies the name of the column. RENAME COLUMN ALTER TABLE RENAME COLUMN statement changes the column name of an existing table. Note that this statement is only supported with v2 tables. Syntax ALTER TABLE table_identifier RENAME COLUMN col_name TO col_name Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name col_name Specifies the name of the column. ALTER OR CHANGE COLUMN ALTER TABLE ALTER COLUMN or ALTER TABLE CHANGE COLUMN statement changes column’s definition. Syntax ALTER TABLE table_identifier { ALTER | CHANGE } [ COLUMN ] col_name alterColumnAction Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name col_name Specifies the name of the column. alterColumnAction Change column’s definition. REPLACE COLUMNS ALTER TABLE REPLACE COLUMNS statement removes all existing columns and adds the new set of columns. Note that this statement is only supported with v2 tables. Syntax ALTER TABLE table_identifier [ partition_spec ] REPLACE COLUMNS [ ( ] qualified_col_type_with_position_list [ ) ] Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec Partition to be replaced. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) qualified_col_type_with_position_list The list of the column(s) to be added Syntax: col_name col_type [ col_comment ] [ col_position ] [ , ... ] ADD AND DROP PARTITION ADD PARTITION ALTER TABLE ADD statement adds partition to the partitioned table. If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed. Syntax ALTER TABLE table_identifier ADD [ IF NOT EXISTS ] ( partition_spec [ partition_spec ... ] ) Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec Partition to be added. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) DROP PARTITION ALTER TABLE DROP statement drops the partition of the table. If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed. Syntax ALTER TABLE table_identifier DROP [ IF EXISTS ] partition_spec [ PURGE ] Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec Partition to be dropped. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) SET AND UNSET SET TABLE PROPERTIES ALTER TABLE SET command is used for setting the table properties. If a particular property was already set, this overrides the old value with the new one. ALTER TABLE UNSET is used to drop the table property. Syntax -- Set Table Properties ALTER TABLE table_identifier SET TBLPROPERTIES ( key1 = val1 , key2 = val2 , ... ) -- Unset Table Properties ALTER TABLE table_identifier UNSET TBLPROPERTIES [ IF EXISTS ] ( key1 , key2 , ... ) SET SERDE ALTER TABLE SET command is used for setting the SERDE or SERDE properties in Hive tables. If a particular property was already set, this overrides the old value with the new one. Syntax -- Set SERDE Properties ALTER TABLE table_identifier [ partition_spec ] SET SERDEPROPERTIES ( key1 = val1 , key2 = val2 , ... ) ALTER TABLE table_identifier [ partition_spec ] SET SERDE serde_class_name [ WITH SERDEPROPERTIES ( key1 = val1 , key2 = val2 , ... ) ] SET LOCATION And SET FILE FORMAT ALTER TABLE SET command can also be used for changing the file location and file format for existing tables. If the table is cached, the ALTER TABLE .. SET LOCATION command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed. Syntax -- Changing File Format ALTER TABLE table_identifier [ partition_spec ] SET FILEFORMAT file_format -- Changing File Location ALTER TABLE table_identifier [ partition_spec ] SET LOCATION 'new_location' Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec Specifies the partition on which the property has to be set. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) SERDEPROPERTIES ( key1 = val1, key2 = val2, … ) Specifies the SERDE properties to be set. RECOVER PARTITIONS ALTER TABLE RECOVER PARTITIONS statement recovers all the partitions in the directory of a table and updates the Hive metastore. Another way to recover partitions is to use MSCK REPAIR TABLE . Syntax ALTER TABLE table_identifier RECOVER PARTITIONS Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name Examples -- RENAME table DESC student ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | name | string | NULL | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ ALTER TABLE Student RENAME TO StudentInfo ; -- After Renaming the table DESC StudentInfo ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | name | string | NULL | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ -- RENAME partition SHOW PARTITIONS StudentInfo ; + ---------+ | partition | + ---------+ | age = 10 | | age = 11 | | age = 12 | + ---------+ ALTER TABLE default . StudentInfo PARTITION ( age = '10' ) RENAME TO PARTITION ( age = '15' ); -- After renaming Partition SHOW PARTITIONS StudentInfo ; + ---------+ | partition | + ---------+ | age = 11 | | age = 12 | | age = 15 | + ---------+ -- Add new columns to a table DESC StudentInfo ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | name | string | NULL | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ ALTER TABLE StudentInfo ADD columns ( LastName string , DOB timestamp ); -- After Adding New columns to the table DESC StudentInfo ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | name | string | NULL | | rollno | int | NULL | | LastName | string | NULL | | DOB | timestamp | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ -- Drop columns of a table DESC StudentInfo ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | name | string | NULL | | rollno | int | NULL | | LastName | string | NULL | | DOB | timestamp | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ ALTER TABLE StudentInfo DROP columns ( LastName , DOB ); -- After dropping columns of the table DESC StudentInfo ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | name | string | NULL | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ -- Rename a column of a table DESC StudentInfo ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | name | string | NULL | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ ALTER TABLE StudentInfo RENAME COLUMN name TO FirstName ; -- After renaming a column of the table DESC StudentInfo ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | FirstName | string | NULL | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ -- ALTER OR CHANGE COLUMNS DESC StudentInfo ; + -----------------------+---------+-------+ | col_name | data_type | comment | + -----------------------+---------+-------+ | FirstName | string | NULL | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-------+ ALTER TABLE StudentInfo ALTER COLUMN FirstName COMMENT "new comment" ; -- After ALTER or CHANGE COLUMNS DESC StudentInfo ; + -----------------------+---------+-----------+ | col_name | data_type | comment | + -----------------------+---------+-----------+ | FirstName | string | new comment | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-----------+ -- REPLACE COLUMNS DESC StudentInfo ; + -----------------------+---------+-----------+ | col_name | data_type | comment | + -----------------------+---------+-----------+ | FirstName | string | new comment | | rollno | int | NULL | | age | int | NULL | |# Partition Information | | | | # col_name | data_type | comment | | age | int | NULL | + -----------------------+---------+-----------+ ALTER TABLE StudentInfo REPLACE COLUMNS ( name string , ID int COMMENT 'new comment' ); -- After replacing COLUMNS DESC StudentInfo ; + -----=---------+---------+-----------+ | col_name | data_type | comment | + ---------------+---------+-----------+ | name | string | NULL | | ID | int | new comment | | # Partitioning | | | | Not partitioned | | | + ---------------+---------+-----------+ -- Add a new partition to a table SHOW PARTITIONS StudentInfo ; + ---------+ | partition | + ---------+ | age = 11 | | age = 12 | | age = 15 | + ---------+ ALTER TABLE StudentInfo ADD IF NOT EXISTS PARTITION ( age = 18 ); -- After adding a new partition to the table SHOW PARTITIONS StudentInfo ; + ---------+ | partition | + ---------+ | age = 11 | | age = 12 | | age = 15 | | age = 18 | + ---------+ -- Drop a partition from the table SHOW PARTITIONS StudentInfo ; + ---------+ | partition | + ---------+ | age = 11 | | age = 12 | | age = 15 | | age = 18 | + ---------+ ALTER TABLE StudentInfo DROP IF EXISTS PARTITION ( age = 18 ); -- After dropping the partition of the table SHOW PARTITIONS StudentInfo ; + ---------+ | partition | + ---------+ | age = 11 | | age = 12 | | age = 15 | + ---------+ -- Adding multiple partitions to the table SHOW PARTITIONS StudentInfo ; + ---------+ | partition | + ---------+ | age = 11 | | age = 12 | | age = 15 | + ---------+ ALTER TABLE StudentInfo ADD IF NOT EXISTS PARTITION ( age = 18 ) PARTITION ( age = 20 ); -- After adding multiple partitions to the table SHOW PARTITIONS StudentInfo ; + ---------+ | partition | + ---------+ | age = 11 | | age = 12 | | age = 15 | | age = 18 | | age = 20 | + ---------+ -- Change the fileformat ALTER TABLE loc_orc SET fileformat orc ; ALTER TABLE p1 partition ( month = 2 , day = 2 ) SET fileformat parquet ; -- Change the file Location ALTER TABLE dbx . tab1 PARTITION ( a = '1' , b = '2' ) SET LOCATION '/path/to/part/ways' -- SET SERDE/ SERDE Properties ALTER TABLE test_tab SET SERDE 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe' ; ALTER TABLE dbx . tab1 SET SERDE 'org.apache.hadoop' WITH SERDEPROPERTIES ( 'k' = 'v' , 'kay' = 'vee' ) -- SET TABLE PROPERTIES ALTER TABLE dbx . tab1 SET TBLPROPERTIES ( 'winner' = 'loser' ); -- SET TABLE COMMENT Using SET PROPERTIES ALTER TABLE dbx . tab1 SET TBLPROPERTIES ( 'comment' = 'A table comment.' ); -- Alter TABLE COMMENT Using SET PROPERTIES ALTER TABLE dbx . tab1 SET TBLPROPERTIES ( 'comment' = 'This is a new comment.' ); -- DROP TABLE PROPERTIES ALTER TABLE dbx . tab1 UNSET TBLPROPERTIES ( 'winner' ); -- RECOVER PARTITIONS ALTER TABLE dbx . tab1 RECOVER PARTITIONS ; Related Statements CREATE TABLE DROP TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-alter-view.html
ALTER VIEW - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions ALTER VIEW Description The ALTER VIEW statement can alter metadata associated with the view. It can change the definition of the view, change the name of a view to a different name, set and unset the metadata of the view by setting TBLPROPERTIES . RENAME View Renames the existing view. If the new view name already exists in the source database, a TableAlreadyExistsException is thrown. This operation does not support moving the views across databases. If the view is cached, the command clears cached data of the view and all its dependents that refer to it. View’s cache will be lazily filled when the next time the view is accessed. The command leaves view’s dependents as uncached. Syntax ALTER VIEW view_identifier RENAME TO view_identifier Parameters view_identifier Specifies a view name, which may be optionally qualified with a database name. Syntax: [ database_name. ] view_name SET View Properties Set one or more properties of an existing view. The properties are the key value pairs. If the properties’ keys exist, the values are replaced with the new values. If the properties’ keys do not exist, the key value pairs are added into the properties. Syntax ALTER VIEW view_identifier SET TBLPROPERTIES ( property_key = property_val [ , ... ] ) Parameters view_identifier Specifies a view name, which may be optionally qualified with a database name. Syntax: [ database_name. ] view_name property_key Specifies the property key. The key may consists of multiple parts separated by dot. Syntax: [ key_part1 ] [ .key_part2 ] [ ... ] UNSET View Properties Drop one or more properties of an existing view. If the specified keys do not exist, an exception is thrown. Use IF EXISTS to avoid the exception. Syntax ALTER VIEW view_identifier UNSET TBLPROPERTIES [ IF EXISTS ] ( property_key [ , ... ] ) Parameters view_identifier Specifies a view name, which may be optionally qualified with a database name. Syntax: [ database_name. ] view_name property_key Specifies the property key. The key may consists of multiple parts separated by dot. Syntax: [ key_part1 ] [ .key_part2 ] [ ... ] ALTER View AS SELECT ALTER VIEW view_identifier AS SELECT statement changes the definition of a view. The SELECT statement must be valid, and the view_identifier must exist. Syntax ALTER VIEW view_identifier AS select_statement Note that ALTER VIEW statement does not support SET SERDE or SET SERDEPROPERTIES properties. Parameters view_identifier Specifies a view name, which may be optionally qualified with a database name. Syntax: [ database_name. ] view_name select_statement Specifies the definition of the view. Check select_statement for details. Examples -- Rename only changes the view name. -- The source and target databases of the view have to be the same. -- Use qualified or unqualified name for the source and target view. ALTER VIEW tempdb1 . v1 RENAME TO tempdb1 . v2 ; -- Verify that the new view is created. DESCRIBE TABLE EXTENDED tempdb1 . v2 ; + ----------------------------+----------+-------+ | col_name | data_type | comment | + ----------------------------+----------+-------+ | c1 | int | null | | c2 | string | null | | | | | |# Detailed Table Information | | | | Database | tempdb1 | | | Table | v2 | | + ----------------------------+----------+-------+ -- Before ALTER VIEW SET TBLPROPERTIES DESC TABLE EXTENDED tempdb1 . v2 ; + ----------------------------+----------+-------+ | col_name | data_type | comment | + ----------------------------+----------+-------+ | c1 | int | null | | c2 | string | null | | | | | |# Detailed Table Information | | | | Database | tempdb1 | | | Table | v2 | | | Table Properties | [....] | | + ----------------------------+----------+-------+ -- Set properties in TBLPROPERTIES ALTER VIEW tempdb1 . v2 SET TBLPROPERTIES ( 'created.by.user' = "John" , 'created.date' = '01-01-2001' ); -- Use `DESCRIBE TABLE EXTENDED tempdb1.v2` to verify DESC TABLE EXTENDED tempdb1 . v2 ; + ----------------------------+-----------------------------------------------------+-------+ | col_name | data_type | comment | + ----------------------------+-----------------------------------------------------+-------+ | c1 | int | null | | c2 | string | null | | | | | |# Detailed Table Information | | | | Database | tempdb1 | | | Table | v2 | | | Table Properties | [ created . by . user = John , created . date = 01 - 01 - 2001 , ....] | | + ----------------------------+-----------------------------------------------------+-------+ -- Remove the key `created.by.user` and `created.date` from `TBLPROPERTIES` ALTER VIEW tempdb1 . v2 UNSET TBLPROPERTIES ( 'created.by.user' , 'created.date' ); --Use `DESC TABLE EXTENDED tempdb1.v2` to verify the changes DESC TABLE EXTENDED tempdb1 . v2 ; + ----------------------------+----------+-------+ | col_name | data_type | comment | + ----------------------------+----------+-------+ | c1 | int | null | | c2 | string | null | | | | | |# Detailed Table Information | | | | Database | tempdb1 | | | Table | v2 | | | Table Properties | [....] | | + ----------------------------+----------+-------+ -- Change the view definition ALTER VIEW tempdb1 . v2 AS SELECT * FROM tempdb1 . v1 ; -- Use `DESC TABLE EXTENDED` to verify DESC TABLE EXTENDED tempdb1 . v2 ; + ----------------------------+---------------------------+-------+ | col_name | data_type | comment | + ----------------------------+---------------------------+-------+ | c1 | int | null | | c2 | string | null | | | | | |# Detailed Table Information | | | | Database | tempdb1 | | | Table | v2 | | | Type | VIEW | | | View Text | select * from tempdb1 . v1 | | | View Original Text | select * from tempdb1 . v1 | | + ----------------------------+---------------------------+-------+ Related Statements describe-table create-view drop-view show-views

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-create-database.html
CREATE DATABASE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions CREATE DATABASE Description Creates a database with the specified name. If database with the same name already exists, an exception will be thrown. Syntax CREATE { DATABASE | SCHEMA } [ IF NOT EXISTS ] database_name [ COMMENT database_comment ] [ LOCATION database_directory ] [ WITH DBPROPERTIES ( property_name = property_value [ , ... ] ) ] Parameters database_name Specifies the name of the database to be created. IF NOT EXISTS Creates a database with the given name if it does not exist. If a database with the same name already exists, nothing will happen. database_directory Path of the file system in which the specified database is to be created. If the specified path does not exist in the underlying file system, this command creates a directory with the path. If the location is not specified, the database will be created in the default warehouse directory, whose path is configured by the static configuration spark.sql.warehouse.dir. database_comment Specifies the description for the database. WITH DBPROPERTIES ( property_name=property_value [ , … ] ) Specifies the properties for the database in key-value pairs. Examples -- Create database `customer_db`. This throws exception if database with name customer_db -- already exists. CREATE DATABASE customer_db ; -- Create database `customer_db` only if database with same name doesn't exist. CREATE DATABASE IF NOT EXISTS customer_db ; -- Create database `customer_db` only if database with same name doesn't exist with -- `Comments`,`Specific Location` and `Database properties`. CREATE DATABASE IF NOT EXISTS customer_db COMMENT 'This is customer database' LOCATION '/user' WITH DBPROPERTIES ( ID = 001 , Name = 'John' ); -- Verify that properties are set. DESCRIBE DATABASE EXTENDED customer_db ; + -------------------------+--------------------------+ | database_description_item | database_description_value | + -------------------------+--------------------------+ | Database Name | customer_db | | Description | This is customer database | | Location | hdfs : // hacluster / user | | Properties | (( ID , 001 ), ( Name , John )) | + -------------------------+--------------------------+ Related Statements DESCRIBE DATABASE DROP DATABASE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-create-function.html
CREATE FUNCTION - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions CREATE FUNCTION Description The CREATE FUNCTION statement is used to create a temporary or permanent function in Spark. Temporary functions are scoped at a session level where as permanent functions are created in the persistent catalog and are made available to all sessions. The resources specified in the USING clause are made available to all executors when they are executed for the first time. In addition to the SQL interface, spark allows users to create custom user defined scalar and aggregate functions using Scala, Python and Java APIs. Please refer to Scalar UDFs and UDAFs for more information. Syntax CREATE [ OR REPLACE ] [ TEMPORARY ] FUNCTION [ IF NOT EXISTS ] function_name AS class_name [ resource_locations ] Parameters OR REPLACE If specified, the resources for the function are reloaded. This is mainly useful to pick up any changes made to the implementation of the function. This parameter is mutually exclusive to IF NOT EXISTS and can not be specified together. TEMPORARY Indicates the scope of function being created. When TEMPORARY is specified, the created function is valid and visible in the current session. No persistent entry is made in the catalog for these kind of functions. IF NOT EXISTS If specified, creates the function only when it does not exist. The creation of function succeeds (no error is thrown) if the specified function already exists in the system. This parameter is mutually exclusive to OR REPLACE and can not be specified together. function_name Specifies a name of function to be created. The function name may be optionally qualified with a database name. Syntax: [ database_name. ] function_name class_name Specifies the name of the class that provides the implementation for function to be created. The implementing class should extend one of the base classes as follows: Should extend UDF or UDAF in org.apache.hadoop.hive.ql.exec package. Should extend AbstractGenericUDAFResolver , GenericUDF , or GenericUDTF in org.apache.hadoop.hive.ql.udf.generic package. Should extend UserDefinedAggregateFunction in org.apache.spark.sql.expressions package. resource_locations Specifies the list of resources that contain the implementation of the function along with its dependencies. Syntax: USING { { (JAR | FILE | ARCHIVE) resource_uri } , ... } Examples -- 1. Create a simple UDF `SimpleUdf` that increments the supplied integral value by 10. -- import org.apache.hadoop.hive.ql.exec.UDF; -- public class SimpleUdf extends UDF { -- public int evaluate(int value) { -- return value + 10; -- } -- } -- 2. Compile and place it in a JAR file called `SimpleUdf.jar` in /tmp. -- Create a table called `test` and insert two rows. CREATE TABLE test ( c1 INT ); INSERT INTO test VALUES ( 1 ), ( 2 ); -- Create a permanent function called `simple_udf`. CREATE FUNCTION simple_udf AS 'SimpleUdf' USING JAR '/tmp/SimpleUdf.jar' ; -- Verify that the function is in the registry. SHOW USER FUNCTIONS ; + ------------------+ | function | + ------------------+ | default . simple_udf | + ------------------+ -- Invoke the function. Every selected value should be incremented by 10. SELECT simple_udf ( c1 ) AS function_return_value FROM test ; + ---------------------+ | function_return_value | + ---------------------+ | 11 | | 12 | + ---------------------+ -- Created a temporary function. CREATE TEMPORARY FUNCTION simple_temp_udf AS 'SimpleUdf' USING JAR '/tmp/SimpleUdf.jar' ; -- Verify that the newly created temporary function is in the registry. -- Please note that the temporary function does not have a qualified -- database associated with it. SHOW USER FUNCTIONS ; + ------------------+ | function | + ------------------+ | default . simple_udf | | simple_temp_udf | + ------------------+ -- 1. Modify `SimpleUdf`'s implementation to add supplied integral value by 20. -- import org.apache.hadoop.hive.ql.exec.UDF; -- public class SimpleUdfR extends UDF { -- public int evaluate(int value) { -- return value + 20; -- } -- } -- 2. Compile and place it in a jar file called `SimpleUdfR.jar` in /tmp. -- Replace the implementation of `simple_udf` CREATE OR REPLACE FUNCTION simple_udf AS 'SimpleUdfR' USING JAR '/tmp/SimpleUdfR.jar' ; -- Invoke the function. Every selected value should be incremented by 20. SELECT simple_udf ( c1 ) AS function_return_value FROM test ; + ---------------------+ | function_return_value | + ---------------------+ | 21 | | 22 | + ---------------------+ Related Statements SHOW FUNCTIONS DESCRIBE FUNCTION DROP FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-create-table.html
CREATE TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions CREATE TABLE Description CREATE TABLE statement is used to define a table in an existing database. The CREATE statements: CREATE TABLE USING DATA_SOURCE CREATE TABLE USING HIVE FORMAT CREATE TABLE LIKE Related Statements ALTER TABLE DROP TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-create-view.html
CREATE VIEW - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions CREATE VIEW Description Views are based on the result-set of an SQL query. CREATE VIEW constructs a virtual table that has no physical data therefore other operations like ALTER VIEW and DROP VIEW only change metadata. Syntax CREATE [ OR REPLACE ] [ [ GLOBAL ] TEMPORARY ] VIEW [ IF NOT EXISTS ] view_identifier create_view_clauses AS query Parameters OR REPLACE If a view of same name already exists, it will be replaced. [ GLOBAL ] TEMPORARY TEMPORARY views are session-scoped and will be dropped when session ends because it skips persisting the definition in the underlying metastore, if any. GLOBAL TEMPORARY views are tied to a system preserved temporary database global_temp . IF NOT EXISTS Creates a view if it does not exist. view_identifier Specifies a view name, which may be optionally qualified with a database name. Syntax: [ database_name. ] view_name create_view_clauses These clauses are optional and order insensitive. It can be of following formats. [ ( column_name [ COMMENT column_comment ], ... ) ] to specify column-level comments. [ COMMENT view_comment ] to specify view-level comments. [ TBLPROPERTIES ( property_name = property_value [ , ... ] ) ] to add metadata key-value pairs. query A SELECT statement that constructs the view from base tables or other views. Examples -- Create or replace view for `experienced_employee` with comments. CREATE OR REPLACE VIEW experienced_employee ( ID COMMENT 'Unique identification number' , Name ) COMMENT 'View for experienced employees' AS SELECT id , name FROM all_employee WHERE working_years > 5 ; -- Create a global temporary view `subscribed_movies` if it does not exist. CREATE GLOBAL TEMPORARY VIEW IF NOT EXISTS subscribed_movies AS SELECT mo . member_id , mb . full_name , mo . movie_title FROM movies AS mo INNER JOIN members AS mb ON mo . member_id = mb . id ; Related Statements ALTER VIEW DROP VIEW SHOW VIEWS

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-drop-database.html
DROP DATABASE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DROP DATABASE Description Drop a database and delete the directory associated with the database from the file system. An exception will be thrown if the database does not exist in the system. Syntax DROP { DATABASE | SCHEMA } [ IF EXISTS ] dbname [ RESTRICT | CASCADE ] Parameters DATABASE | SCHEMA DATABASE and SCHEMA mean the same thing, either of them can be used. IF EXISTS If specified, no exception is thrown when the database does not exist. RESTRICT If specified, will restrict dropping a non-empty database and is enabled by default. CASCADE If specified, will drop all the associated tables and functions. Examples -- Create `inventory_db` Database CREATE DATABASE inventory_db COMMENT 'This database is used to maintain Inventory' ; -- Drop the database and it's tables DROP DATABASE inventory_db CASCADE ; -- Drop the database using IF EXISTS DROP DATABASE IF EXISTS inventory_db CASCADE ; Related Statements CREATE DATABASE DESCRIBE DATABASE SHOW DATABASES

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-drop-function.html
DROP FUNCTION - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DROP FUNCTION Description The DROP FUNCTION statement drops a temporary or user defined function (UDF). An exception will be thrown if the function does not exist. Syntax DROP [ TEMPORARY ] FUNCTION [ IF EXISTS ] function_name Parameters function_name Specifies the name of an existing function. The function name may be optionally qualified with a database name. Syntax: [ database_name. ] function_name TEMPORARY Should be used to delete the TEMPORARY function. IF EXISTS If specified, no exception is thrown when the function does not exist. Examples -- Create a permanent function `test_avg` CREATE FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage' ; -- List user functions SHOW USER FUNCTIONS ; + ----------------+ | function | + ----------------+ | default . test_avg | + ----------------+ -- Create Temporary function `test_avg` CREATE TEMPORARY FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage' ; -- List user functions SHOW USER FUNCTIONS ; + ----------------+ | function | + ----------------+ | default . test_avg | | test_avg | + ----------------+ -- Drop Permanent function DROP FUNCTION test_avg ; -- Try to drop Permanent function which is not present DROP FUNCTION test_avg ; Error : Error running query : org . apache . spark . sql . catalyst . analysis . NoSuchPermanentFunctionException : Function 'default.test_avg' not found in database 'default' ; ( state = , code = 0 ) -- List the functions after dropping, it should list only temporary function SHOW USER FUNCTIONS ; + --------+ | function | + --------+ | test_avg | + --------+ -- Drop Temporary function DROP TEMPORARY FUNCTION IF EXISTS test_avg ; Related Statements CREATE FUNCTION DESCRIBE FUNCTION SHOW FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-drop-table.html
DROP TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DROP TABLE Description DROP TABLE deletes the table and removes the directory associated with the table from the file system if the table is not EXTERNAL table. If the table is not present it throws an exception. In case of an external table, only the associated metadata information is removed from the metastore database. If the table is cached, the command uncaches the table and all its dependents. Syntax DROP TABLE [ IF EXISTS ] table_identifier [ PURGE ] Parameter IF EXISTS If specified, no exception is thrown when the table does not exist. table_identifier Specifies the table name to be dropped. The table name may be optionally qualified with a database name. Syntax: [ database_name. ] table_name PURGE If specified, completely purge the table skipping trash while dropping table(Note: PURGE available in Hive Metastore 0.14.0 and later). Examples -- Assumes a table named `employeetable` exists. DROP TABLE employeetable ; -- Assumes a table named `employeetable` exists in the `userdb` database DROP TABLE userdb . employeetable ; -- Assumes a table named `employeetable` does not exist. -- Throws exception DROP TABLE employeetable ; Error : org . apache . spark . sql . AnalysisException : Table or view not found : employeetable ; ( state = , code = 0 ) -- Assumes a table named `employeetable` does not exist,Try with IF EXISTS -- this time it will not throw exception DROP TABLE IF EXISTS employeetable ; -- Completely purge the table skipping trash. DROP TABLE employeetable PURGE ; Related Statements CREATE TABLE CREATE DATABASE DROP DATABASE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-drop-view.html
DROP VIEW - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DROP VIEW Description DROP VIEW removes the metadata associated with a specified view from the catalog. Syntax DROP VIEW [ IF EXISTS ] view_identifier Parameter IF EXISTS If specified, no exception is thrown when the view does not exist. view_identifier Specifies the view name to be dropped. The view name may be optionally qualified with a database name. Syntax: [ database_name. ] view_name Examples -- Assumes a view named `employeeView` exists. DROP VIEW employeeView ; -- Assumes a view named `employeeView` exists in the `userdb` database DROP VIEW userdb . employeeView ; -- Assumes a view named `employeeView` does not exist. -- Throws exception DROP VIEW employeeView ; Error : org . apache . spark . sql . AnalysisException : Table or view not found : employeeView ; ( state = , code = 0 ) -- Assumes a view named `employeeView` does not exist,Try with IF EXISTS -- this time it will not throw exception DROP VIEW IF EXISTS employeeView ; Related Statements CREATE VIEW ALTER VIEW SHOW VIEWS CREATE DATABASE DROP DATABASE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-repair-table.html
REPAIR TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions REPAIR TABLE Description REPAIR TABLE recovers all the partitions in the directory of a table and updates the Hive metastore. When creating a table using PARTITIONED BY clause, partitions are generated and registered in the Hive metastore. However, if the partitioned table is created from existing data, partitions are not registered automatically in the Hive metastore. User needs to run REPAIRÂ TABLE Â to register the partitions. REPAIR TABLE on a non-existent table or a table without partitions throws an exception. Another way to recover partitions is to use ALTER TABLE RECOVER PARTITIONS . This command can also be invoked using MSCK REPAIR TABLE , for Hive compatibility. If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed. Syntax [ MSCK ] REPAIR TABLE table_identifier [ { ADD | DROP | SYNC } PARTITIONS ] Parameters table_identifier Specifies the name of the table to be repaired. The table name may be optionally qualified with a database name. Syntax: [ database_name. ] table_name {ADD|DROP|SYNC} PARTITIONS Specifies how to recover partitions. If not specified, ADD is the default. ADD , the command adds new partitions to the session catalog for all sub-folder in the base table folder that don’t belong to any table partitions. DROP , the command drops all partitions from the session catalog that have non-existing locations in the file system. SYNC is the combination of DROP and ADD . Examples -- create a partitioned table from existing data /tmp/namesAndAges.parquet CREATE TABLE t1 ( name STRING , age INT ) USING parquet PARTITIONED BY ( age ) LOCATION "/tmp/namesAndAges.parquet" ; -- SELECT * FROM t1 does not return results SELECT * FROM t1 ; -- run REPAIR TABLE to recovers all the partitions REPAIR TABLE t1 ; -- SELECT * FROM t1 returns results SELECT * FROM t1 ; + -------+---+ | name | age | + -------+---+ | Michael | 20 | + -------+---+ | Justin | 19 | + -------+---+ | Andy | 30 | + -------+---+ Related Statements ALTER TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-truncate-table.html
TRUNCATE TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions TRUNCATE TABLE Description The TRUNCATE TABLE statement removes all the rows from a table or partition(s). The table must not be a view or an external/temporary table. In order to truncate multiple partitions at once, the user can specify the partitions in partition_spec . If no partition_spec is specified it will remove all partitions in the table. If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed. Syntax TRUNCATE TABLE table_identifier [ partition_spec ] Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec An optional parameter that specifies a comma separated list of key and value pairs for partitions. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) Examples -- Create table Student with partition CREATE TABLE Student ( name STRING , rollno INT ) PARTITIONED BY ( age INT ); SELECT * FROM Student ; + ----+------+---+ | name | rollno | age | + ----+------+---+ | ABC | 1 | 10 | | DEF | 2 | 10 | | XYZ | 3 | 12 | + ----+------+---+ -- Removes all rows from the table in the partition specified TRUNCATE TABLE Student partition ( age = 10 ); -- After truncate execution, records belonging to partition age=10 are removed SELECT * FROM Student ; + ----+------+---+ | name | rollno | age | + ----+------+---+ | XYZ | 3 | 12 | + ----+------+---+ -- Removes all rows from the table from all partitions TRUNCATE TABLE Student ; SELECT * FROM Student ; + ----+------+---+ | name | rollno | age | + ----+------+---+ + ----+------+---+ Related Statements DROP TABLE ALTER TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-ddl-usedb.html
USE Database - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions USE Database Description USE statement is used to set the current database. After the current database is set, the unqualified database artifacts such as tables, functions and views that are referenced by SQLs are resolved from the current database. The default database name is ‘default’. Syntax USE database_name Parameter database_name Name of the database will be used. If the database does not exist, an exception will be thrown. Examples -- Use the 'userdb' which exists. USE userdb ; -- Use the 'userdb1' which doesn't exist USE userdb1 ; Error : org . apache . spark . sql . catalyst . analysis . NoSuchDatabaseException : Database 'userdb1' not found ; ( state = , code = 0 ) Related Statements CREATE DATABASE DROP DATABASE CREATE TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-dml-insert-table.html
INSERT TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions INSERT TABLE Description The INSERT statement inserts new rows into a table or overwrites the existing data in the table. The inserted rows can be specified by value expressions or result from a query. Syntax INSERT [ INTO | OVERWRITE ] [ TABLE ] table_identifier [ partition_spec ] [ ( column_list ) ] { VALUES ( { value | NULL } [ , ... ] ) [ , ( ... ) ] | query } INSERT INTO [ TABLE ] table_identifier REPLACE WHERE boolean_expression query Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec An optional parameter that specifies a comma-separated list of key and value pairs for partitions. Note that one can use a typed literal (e.g., date’2019-01-02’) in the partition spec. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) column_list An optional parameter that specifies a comma-separated list of columns belonging to the table_identifier table. Spark will reorder the columns of the input query to match the table schema according to the specified column list. Note: The current behaviour has some limitations: All specified columns should exist in the table and not be duplicated from each other. It includes all columns except the static partition columns. The size of the column list should be exactly the size of the data from VALUES clause or query. VALUES ( { value | NULL } [ , … ] ) [ , ( … ) ] Specifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted. A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows. boolean_expression Specifies any expression that evaluates to a result type boolean . Two or more expressions may be combined together using the logical operators ( AND , OR ). query A query that produces the rows to be inserted. It can be in one of following formats: a SELECT statement a Inline Table statement a FROM statement Examples Insert Into Single Row Insert Using a VALUES Clause CREATE TABLE students ( name VARCHAR ( 64 ), address VARCHAR ( 64 )) USING PARQUET PARTITIONED BY ( student_id INT ); INSERT INTO students VALUES ( 'Amy Smith' , '123 Park Ave, San Jose' , 111111 ); SELECT * FROM students ; + ---------+----------------------+----------+ | name | address | student_id | + ---------+----------------------+----------+ | Amy Smith | 123 Park Ave , San Jose | 111111 | + ---------+----------------------+----------+ Multi-Row Insert Using a VALUES Clause INSERT INTO students VALUES ( 'Bob Brown' , '456 Taylor St, Cupertino' , 222222 ), ( 'Cathy Johnson' , '789 Race Ave, Palo Alto' , 333333 ); SELECT * FROM students ; + -------------+------------------------+----------+ | name | address | student_id | + -------------+------------------------+----------+ | Amy Smith | 123 Park Ave , San Jose | 111111 | + -------------+------------------------+----------+ | Bob Brown | 456 Taylor St , Cupertino | 222222 | + -------------+------------------------+----------+ | Cathy Johnson | 789 Race Ave , Palo Alto | 333333 | + --------------+-----------------------+----------+ Insert Using a SELECT Statement -- Assuming the persons table has already been created and populated. SELECT * FROM persons ; + -------------+--------------------------+---------+ | name | address | ssn | + -------------+--------------------------+---------+ | Dora Williams | 134 Forest Ave , Menlo Park | 123456789 | + -------------+--------------------------+---------+ | Eddie Davis | 245 Market St , Milpitas | 345678901 | + -------------+--------------------------+---------+ INSERT INTO students PARTITION ( student_id = 444444 ) SELECT name , address FROM persons WHERE name = "Dora Williams" ; SELECT * FROM students ; + -------------+--------------------------+----------+ | name | address | student_id | + -------------+--------------------------+----------+ | Amy Smith | 123 Park Ave , San Jose | 111111 | + -------------+--------------------------+----------+ | Bob Brown | 456 Taylor St , Cupertino | 222222 | + -------------+--------------------------+----------+ | Cathy Johnson | 789 Race Ave , Palo Alto | 333333 | + -------------+--------------------------+----------+ | Dora Williams | 134 Forest Ave , Menlo Park | 444444 | + -------------+--------------------------+----------+ Insert Using a TABLE Statement -- Assuming the visiting_students table has already been created and populated. SELECT * FROM visiting_students ; + -------------+---------------------+----------+ | name | address | student_id | + -------------+---------------------+----------+ | Fleur Laurent | 345 Copper St , London | 777777 | + -------------+---------------------+----------+ | Gordon Martin | 779 Lake Ave , Oxford | 888888 | + -------------+---------------------+----------+ INSERT INTO students TABLE visiting_students ; SELECT * FROM students ; + -------------+--------------------------+----------+ | name | address | student_id | + -------------+--------------------------+----------+ | Amy Smith | 123 Park Ave , San Jose | 111111 | + -------------+--------------------------+----------+ | Bob Brown | 456 Taylor St , Cupertino | 222222 | + -------------+--------------------------+----------+ | Cathy Johnson | 789 Race Ave , Palo Alto | 333333 | + -------------+--------------------------+----------+ | Dora Williams | 134 Forest Ave , Menlo Park | 444444 | + -------------+--------------------------+----------+ | Fleur Laurent | 345 Copper St , London | 777777 | + -------------+--------------------------+----------+ | Gordon Martin | 779 Lake Ave , Oxford | 888888 | + -------------+--------------------------+----------+ Insert Using a FROM Statement -- Assuming the applicants table has already been created and populated. SELECT * FROM applicants ; + -----------+--------------------------+----------+---------+ | name | address | student_id | qualified | + -----------+--------------------------+----------+---------+ | Helen Davis | 469 Mission St , San Diego | 999999 | true | + -----------+--------------------------+----------+---------+ | Ivy King | 367 Leigh Ave , Santa Clara | 101010 | false | + -----------+--------------------------+----------+---------+ | Jason Wang | 908 Bird St , Saratoga | 121212 | true | + -----------+--------------------------+----------+---------+ INSERT INTO students FROM applicants SELECT name , address , student_id WHERE qualified = true ; SELECT * FROM students ; + -------------+--------------------------+----------+ | name | address | student_id | + -------------+--------------------------+----------+ | Amy Smith | 123 Park Ave , San Jose | 111111 | + -------------+--------------------------+----------+ | Bob Brown | 456 Taylor St , Cupertino | 222222 | + -------------+--------------------------+----------+ | Cathy Johnson | 789 Race Ave , Palo Alto | 333333 | + -------------+--------------------------+----------+ | Dora Williams | 134 Forest Ave , Menlo Park | 444444 | + -------------+--------------------------+----------+ | Fleur Laurent | 345 Copper St , London | 777777 | + -------------+--------------------------+----------+ | Gordon Martin | 779 Lake Ave , Oxford | 888888 | + -------------+--------------------------+----------+ | Helen Davis | 469 Mission St , San Diego | 999999 | + -------------+--------------------------+----------+ | Jason Wang | 908 Bird St , Saratoga | 121212 | + -------------+--------------------------+----------+ Insert Using a Typed Date Literal for a Partition Column Value CREATE TABLE students ( name STRING , address STRING ) PARTITIONED BY ( birthday DATE ); INSERT INTO students PARTITION ( birthday = date '2019-01-02' ) VALUES ( 'Amy Smith' , '123 Park Ave, San Jose' ); SELECT * FROM students ; + -------------+-------------------------+-----------+ | name | address | birthday | + -------------+-------------------------+-----------+ | Amy Smith | 123 Park Ave , San Jose | 2019 - 01 - 02 | + -------------+-------------------------+-----------+ Insert with a column list INSERT INTO students ( address , name , student_id ) VALUES ( 'Hangzhou, China' , 'Kent Yao' , 11215016 ); SELECT * FROM students WHERE name = 'Kent Yao' ; + ---------+----------------------+----------+ | name | address | student_id | + ---------+----------------------+----------+ | Kent Yao | Hangzhou , China | 11215016 | + ---------+----------------------+----------+ Insert with both a partition spec and a column list INSERT INTO students PARTITION ( student_id = 11215017 ) ( address , name ) VALUES ( 'Hangzhou, China' , 'Kent Yao Jr.' ); SELECT * FROM students WHERE student_id = 11215017 ; + ------------+----------------------+----------+ | name | address | student_id | + ------------+----------------------+----------+ | Kent Yao Jr . | Hangzhou , China | 11215017 | + ------------+----------------------+----------+ Insert Overwrite Insert Using a VALUES Clause -- Assuming the students table has already been created and populated. SELECT * FROM students ; + -------------+--------------------------+----------+ | name | address | student_id | + -------------+--------------------------+----------+ | Amy Smith | 123 Park Ave , San Jose | 111111 | | Bob Brown | 456 Taylor St , Cupertino | 222222 | | Cathy Johnson | 789 Race Ave , Palo Alto | 333333 | | Dora Williams | 134 Forest Ave , Menlo Park | 444444 | | Fleur Laurent | 345 Copper St , London | 777777 | | Gordon Martin | 779 Lake Ave , Oxford | 888888 | | Helen Davis | 469 Mission St , San Diego | 999999 | | Jason Wang | 908 Bird St , Saratoga | 121212 | + -------------+--------------------------+----------+ INSERT OVERWRITE students VALUES ( 'Ashua Hill' , '456 Erica Ct, Cupertino' , 111111 ), ( 'Brian Reed' , '723 Kern Ave, Palo Alto' , 222222 ); SELECT * FROM students ; + ----------+-----------------------+----------+ | name | address | student_id | + ----------+-----------------------+----------+ | Ashua Hill | 456 Erica Ct , Cupertino | 111111 | | Brian Reed | 723 Kern Ave , Palo Alto | 222222 | + ----------+-----------------------+----------+ Insert Using a SELECT Statement -- Assuming the persons table has already been created and populated. SELECT * FROM persons ; + -------------+--------------------------+---------+ | name | address | ssn | + -------------+--------------------------+---------+ | Dora Williams | 134 Forest Ave , Menlo Park | 123456789 | + -------------+--------------------------+---------+ | Eddie Davis | 245 Market St , Milpitas | 345678901 | + -------------+--------------------------+---------+ INSERT OVERWRITE students PARTITION ( student_id = 222222 ) SELECT name , address FROM persons WHERE name = "Dora Williams" ; SELECT * FROM students ; + -------------+--------------------------+----------+ | name | address | student_id | + -------------+--------------------------+----------+ | Ashua Hill | 456 Erica Ct , Cupertino | 111111 | + -------------+--------------------------+----------+ | Dora Williams | 134 Forest Ave , Menlo Park | 222222 | + -------------+--------------------------+----------+ Insert Using a REPLACE WHERE Statement -- Assuming the persons and persons2 table has already been created and populated. SELECT * FROM persons ; + -------------+--------------------------+---------+ | name | address | ssn | + -------------+--------------------------+---------+ | Dora Williams | 134 Forest Ave , Menlo Park | 123456789 | + -------------+--------------------------+---------+ | Eddie Davis | 245 Market St , Milpitas | 345678901 | + -------------+--------------------------+---------+ SELECT * FROM persons2 ; + -------------+--------------------------+---------+ | name | address | ssn | + -------------+--------------------------+---------+ | Ashua Hill | 456 Erica Ct , Cupertino | 432795921 | + -------------+--------------------------+---------+ -- in an atomic operation, 1) delete rows with ssn = 123456789 and 2) insert rows from persons2 INSERT INTO persons REPLACE WHERE ssn = 123456789 SELECT * FROM persons2 SELECT * FROM persons ; + -------------+--------------------------+---------+ | name | address | ssn | + -------------+--------------------------+---------+ | Eddie Davis | 245 Market St , Milpitas | 345678901 | + -------------+--------------------------+---------+ | Ashua Hill | 456 Erica Ct , Cupertino | 432795921 | + -------------+--------------------------+---------+ Insert Using a TABLE Statement -- Assuming the visiting_students table has already been created and populated. SELECT * FROM visiting_students ; + -------------+---------------------+----------+ | name | address | student_id | + -------------+---------------------+----------+ | Fleur Laurent | 345 Copper St , London | 777777 | + -------------+---------------------+----------+ | Gordon Martin | 779 Lake Ave , Oxford | 888888 | + -------------+---------------------+----------+ INSERT OVERWRITE students TABLE visiting_students ; SELECT * FROM students ; + -------------+---------------------+----------+ | name | address | student_id | + -------------+---------------------+----------+ | Fleur Laurent | 345 Copper St , London | 777777 | + -------------+---------------------+----------+ | Gordon Martin | 779 Lake Ave , Oxford | 888888 | + -------------+---------------------+----------+ Insert Using a FROM Statement -- Assuming the applicants table has already been created and populated. SELECT * FROM applicants ; + -----------+--------------------------+----------+---------+ | name | address | student_id | qualified | + -----------+--------------------------+----------+---------+ | Helen Davis | 469 Mission St , San Diego | 999999 | true | + -----------+--------------------------+----------+---------+ | Ivy King | 367 Leigh Ave , Santa Clara | 101010 | false | + -----------+--------------------------+----------+---------+ | Jason Wang | 908 Bird St , Saratoga | 121212 | true | + -----------+--------------------------+----------+---------+ INSERT OVERWRITE students FROM applicants SELECT name , address , student_id WHERE qualified = true ; SELECT * FROM students ; + -----------+-------------------------+----------+ | name | address | student_id | + -----------+-------------------------+----------+ | Helen Davis | 469 Mission St , San Diego | 999999 | + -----------+-------------------------+----------+ | Jason Wang | 908 Bird St , Saratoga | 121212 | + -----------+-------------------------+----------+ Insert Using a Typed Date Literal for a Partition Column Value CREATE TABLE students ( name STRING , address STRING ) PARTITIONED BY ( birthday DATE ); INSERT INTO students PARTITION ( birthday = date '2019-01-02' ) VALUES ( 'Amy Smith' , '123 Park Ave, San Jose' ); SELECT * FROM students ; + -------------+-------------------------+-----------+ | name | address | birthday | + -------------+-------------------------+-----------+ | Amy Smith | 123 Park Ave , San Jose | 2019 - 01 - 02 | + -------------+-------------------------+-----------+ INSERT OVERWRITE students PARTITION ( birthday = date '2019-01-02' ) VALUES ( 'Jason Wang' , '908 Bird St, Saratoga' ); SELECT * FROM students ; + -----------+-------------------------+-----------+ | name | address | birthday | + -----------+-------------------------+-----------+ | Jason Wang | 908 Bird St , Saratoga | 2019 - 01 - 02 | + -----------+-------------------------+-----------+ Insert with a column list INSERT OVERWRITE students ( address , name , student_id ) VALUES ( 'Hangzhou, China' , 'Kent Yao' , 11215016 ); SELECT * FROM students WHERE name = 'Kent Yao' ; + ---------+----------------------+----------+ | name | address | student_id | + ---------+----------------------+----------+ | Kent Yao | Hangzhou , China | 11215016 | + ---------+----------------------+----------+ Insert with both a partition spec and a column list INSERT OVERWRITE students PARTITION ( student_id = 11215016 ) ( address , name ) VALUES ( 'Hangzhou, China' , 'Kent Yao Jr.' ); SELECT * FROM students WHERE student_id = 11215016 ; + ------------+----------------------+----------+ | name | address | student_id | + ------------+----------------------+----------+ | Kent Yao Jr . | Hangzhou , China | 11215016 | + ------------+----------------------+----------+ Related Statements INSERT OVERWRITE DIRECTORY statement

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-dml-insert-overwrite-directory.html
INSERT OVERWRITE DIRECTORY - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions INSERT OVERWRITE DIRECTORY Description The INSERT OVERWRITE DIRECTORY statement overwrites the existing data in the directory with the new values using either spark file format or Hive Serde. Hive support must be enabled to use Hive Serde. The inserted rows can be specified by value expressions or result from a query. Syntax INSERT OVERWRITE [ LOCAL ] DIRECTORY [ directory_path ] { spark_format | hive_format } { VALUES ( { value | NULL } [ , ... ] ) [ , ( ... ) ] | query } While spark_format is defined as USING file_format [ OPTIONS ( key = val [ , ... ] ) ] hive_format is defined as [ ROW FORMAT row_format ] [ STORED AS hive_serde ] Parameters directory_path Specifies the destination directory. The LOCAL keyword is used to specify that the directory is on the local file system. In spark file format, it can also be specified in OPTIONS using path , but directory_path and path option can not be both specified. file_format Specifies the file format to use for the insert. Valid options are TEXT , CSV , JSON , JDBC , PARQUET , ORC , HIVE , LIBSVM , or a fully qualified class name of a custom implementation of org.apache.spark.sql.execution.datasources.FileFormat . OPTIONS ( key = val [ , … ] ) Specifies one or more options for the writing of the file format. hive_format Specifies the file format to use for the insert. Both row_format and hive_serde are optional. ROW FORMAT SERDE can only be used with TEXTFILE , SEQUENCEFILE , or RCFILE , while ROW FORMAT DELIMITED can only be used with TEXTFILE . If both are not defined, spark uses TEXTFILE . row_format Specifies the row format for this insert. Valid options are SERDE clause and DELIMITED clause. SERDE clause can be used to specify a custom SerDe for this insert. Alternatively, DELIMITED clause can be used to specify the native SerDe and state the delimiter, escape character, null character, and so on. hive_serde Specifies the file format for this insert. Valid options are TEXTFILE , SEQUENCEFILE , RCFILE , ORC , PARQUET , and AVRO . You can also specify your own input and output format using INPUTFORMAT and OUTPUTFORMAT . VALUES ( { value | NULL } [ , … ] ) [ , ( … ) ] Specifies the values to be inserted. Either an explicitly specified value or a NULL can be inserted. A comma must be used to separate each value in the clause. More than one set of values can be specified to insert multiple rows. query A query that produces the rows to be inserted. It can be in one of following formats: a SELECT statement a Inline Table statement a FROM statement Examples Spark format INSERT OVERWRITE DIRECTORY '/tmp/destination' USING parquet OPTIONS ( col1 1 , col2 2 , col3 'test' ) SELECT * FROM test_table ; INSERT OVERWRITE DIRECTORY USING parquet OPTIONS ( 'path' '/tmp/destination' , col1 1 , col2 2 , col3 'test' ) SELECT * FROM test_table ; Hive format INSERT OVERWRITE LOCAL DIRECTORY '/tmp/destination' STORED AS orc SELECT * FROM test_table ; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/destination' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT * FROM test_table ; Related Statements INSERT TABLE statement

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-dml-load.html
LOAD DATA - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions LOAD DATA Description LOAD DATA statement loads the data into a Hive serde table from the user specified directory or file. If a directory is specified then all the files from the directory are loaded. If a file is specified then only the single file is loaded. Additionally the LOAD DATA statement takes an optional partition specification. When a partition is specified, the data files (when input source is a directory) or the single file (when input source is a file) are loaded into the partition of the target table. If the table is cached, the command clears cached data of the table and all its dependents that refer to it. The cache will be lazily filled when the next time the table or the dependents are accessed. Syntax LOAD DATA [ LOCAL ] INPATH path [ OVERWRITE ] INTO TABLE table_identifier [ partition_spec ] Parameters path Path of the file system. It can be either an absolute or a relative path. table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec An optional parameter that specifies a comma separated list of key and value pairs for partitions. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) LOCAL If specified, it causes the INPATH to be resolved against the local file system, instead of the default file system, which is typically a distributed storage. OVERWRITE By default, new data is appended to the table. If OVERWRITE is used, the table is instead overwritten with new data. Examples -- Example without partition specification. -- Assuming the students table has already been created and populated. SELECT * FROM students ; + ---------+----------------------+----------+ | name | address | student_id | + ---------+----------------------+----------+ | Amy Smith | 123 Park Ave , San Jose | 111111 | + ---------+----------------------+----------+ CREATE TABLE test_load ( name VARCHAR ( 64 ), address VARCHAR ( 64 ), student_id INT ) USING HIVE ; -- Assuming the students table is in '/user/hive/warehouse/' LOAD DATA LOCAL INPATH '/user/hive/warehouse/students' OVERWRITE INTO TABLE test_load ; SELECT * FROM test_load ; + ---------+----------------------+----------+ | name | address | student_id | + ---------+----------------------+----------+ | Amy Smith | 123 Park Ave , San Jose | 111111 | + ---------+----------------------+----------+ -- Example with partition specification. CREATE TABLE test_partition ( c1 INT , c2 INT , c3 INT ) PARTITIONED BY ( c2 , c3 ); INSERT INTO test_partition PARTITION ( c2 = 2 , c3 = 3 ) VALUES ( 1 ); INSERT INTO test_partition PARTITION ( c2 = 5 , c3 = 6 ) VALUES ( 4 ); INSERT INTO test_partition PARTITION ( c2 = 8 , c3 = 9 ) VALUES ( 7 ); SELECT * FROM test_partition ; + ---+---+---+ | c1 | c2 | c3 | + ---+---+---+ | 1 | 2 | 3 | | 4 | 5 | 6 | | 7 | 8 | 9 | + ---+---+---+ CREATE TABLE test_load_partition ( c1 INT , c2 INT , c3 INT ) USING HIVE PARTITIONED BY ( c2 , c3 ); -- Assuming the test_partition table is in '/user/hive/warehouse/' LOAD DATA LOCAL INPATH '/user/hive/warehouse/test_partition/c2=2/c3=3' OVERWRITE INTO TABLE test_load_partition PARTITION ( c2 = 2 , c3 = 3 ); SELECT * FROM test_load_partition ; + ---+---+---+ | c1 | c2 | c3 | + ---+---+---+ | 1 | 2 | 3 | + ---+---+---+

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select.html
SELECT - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SELECT Description Spark supports a SELECT statement and conforms to the ANSI SQL standard. Queries are used to retrieve result sets from one or more tables. The following section describes the overall query syntax and the sub-sections cover different constructs of a query along with examples. Syntax [ WITH with_query [ , ... ] ] select_statement [ { UNION | INTERSECT | EXCEPT } [ ALL | DISTINCT ] select_statement , ... ] [ ORDER BY { expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [ , ... ] } ] [ SORT BY { expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [ , ... ] } ] [ CLUSTER BY { expression [ , ... ] } ] [ DISTRIBUTE BY { expression [, ... ] } ] [ WINDOW { named_window [ , WINDOW named_window , ... ] } ] [ LIMIT { ALL | expression } ] While select_statement is defined as SELECT [ hints , ... ] [ ALL | DISTINCT ] { [ [ named_expression | regex_column_names ] [ , ... ] | TRANSFORM (...) ] } FROM { from_item [ , ... ] } [ PIVOT clause ] [ UNPIVOT clause ] [ LATERAL VIEW clause ] [ ... ] [ WHERE boolean_expression ] [ GROUP BY expression [ , ... ] ] [ HAVING boolean_expression ] Parameters with_query Specifies the common table expressions (CTEs) before the main query block. These table expressions are allowed to be referenced later in the FROM clause. This is useful to abstract out repeated subquery blocks in the FROM clause and improves readability of the query. hints Hints can be specified to help spark optimizer make better planning decisions. Currently spark supports hints that influence selection of join strategies and repartitioning of the data. ALL Select all matching rows from the relation and is enabled by default. DISTINCT Select all matching rows from the relation after removing duplicates in results. named_expression An expression with an assigned name. In general, it denotes a column expression. Syntax: expression [[AS] alias] from_item Specifies a source of input for the query. It can be one of the following: Table relation Join relation Pivot relation Unpivot relation Table-value function Inline table [ LATERAL ] ( Subquery ) File PIVOT The PIVOT clause is used for data perspective; We can get the aggregated values based on specific column value. UNPIVOT The UNPIVOT clause transforms columns into rows. It is the reverse of PIVOT , except for aggregation of values. LATERAL VIEW The LATERAL VIEW clause is used in conjunction with generator functions such as EXPLODE , which will generate a virtual table containing one or more rows. LATERAL VIEW will apply the rows to each original output row. WHERE Filters the result of the FROM clause based on the supplied predicates. GROUP BY Specifies the expressions that are used to group the rows. This is used in conjunction with aggregate functions (MIN, MAX, COUNT, SUM, AVG, etc.) to group rows based on the grouping expressions and aggregate values in each group. When a FILTER clause is attached to an aggregate function, only the matching rows are passed to that function. HAVING Specifies the predicates by which the rows produced by GROUP BY are filtered. The HAVING clause is used to filter rows after the grouping is performed. If HAVING is specified without GROUP BY, it indicates a GROUP BY without grouping expressions (global aggregate). ORDER BY Specifies an ordering of the rows of the complete result set of the query. The output rows are ordered across the partitions. This parameter is mutually exclusive with SORT BY , CLUSTER BY and DISTRIBUTE BY and can not be specified together. SORT BY Specifies an ordering by which the rows are ordered within each partition. This parameter is mutually exclusive with ORDER BY and CLUSTER BY and can not be specified together. CLUSTER BY Specifies a set of expressions that is used to repartition and sort the rows. Using this clause has the same effect of using DISTRIBUTE BY and SORT BY together. DISTRIBUTE BY Specifies a set of expressions by which the result rows are repartitioned. This parameter is mutually exclusive with ORDER BY and CLUSTER BY and can not be specified together. LIMIT Specifies the maximum number of rows that can be returned by a statement or subquery. This clause is mostly used in the conjunction with ORDER BY to produce a deterministic result. boolean_expression Specifies any expression that evaluates to a result type boolean . Two or more expressions may be combined together using the logical operators ( AND , OR ). expression Specifies a combination of one or more values, operators, and SQL functions that evaluates to a value. named_window Specifies aliases for one or more source window specifications. The source window specifications can be referenced in the widow definitions in the query. regex_column_names When spark.sql.parser.quotedRegexColumnNames is true, quoted identifiers (using backticks) in SELECT statement are interpreted as regular expressions and SELECT statement can take regex-based column specification. For example, below SQL will only take column c : SELECT `(a|b)?+.+` FROM ( SELECT 1 as a , 2 as b , 3 as c ) TRANSFORM Specifies a hive-style transform query specification to transform the input by forking and running user-specified command or script. Related Statements WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause CLUSTER BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause Common Table Expression Hints Inline Table File JOIN LIKE Predicate Set Operators TABLESAMPLE Table-valued Function Window Function CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause TRANSFORM Clause LATERAL Subquery

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-explain.html
EXPLAIN - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions EXPLAIN Description The EXPLAIN statement is used to provide logical/physical plans for an input statement. By default, this clause provides information about a physical plan only. Syntax EXPLAIN [ EXTENDED | CODEGEN | COST | FORMATTED ] statement Parameters EXTENDED Generates parsed logical plan, analyzed logical plan, optimized logical plan and physical plan. Parsed Logical plan is a unresolved plan that extracted from the query. Analyzed logical plans transforms which translates unresolvedAttribute and unresolvedRelation into fully typed objects. The optimized logical plan transforms through a set of optimization rules, resulting in the physical plan. CODEGEN Generates code for the statement, if any and a physical plan. COST If plan node statistics are available, generates a logical plan and the statistics. FORMATTED Generates two sections: a physical plan outline and node details. statement Specifies a SQL statement to be explained. Examples -- Default Output EXPLAIN select k , sum ( v ) from values ( 1 , 2 ), ( 1 , 3 ) t ( k , v ) group by k ; + ----------------------------------------------------+ | plan | + ----------------------------------------------------+ | == Physical Plan == * ( 2 ) HashAggregate ( keys = [ k # 33 ], functions = [ sum ( cast ( v # 34 as bigint ))]) +- Exchange hashpartitioning ( k # 33 , 200 ), true , [ id =# 59 ] +- * ( 1 ) HashAggregate ( keys = [ k # 33 ], functions = [ partial_sum ( cast ( v # 34 as bigint ))]) +- * ( 1 ) LocalTableScan [ k # 33 , v # 34 ] | + ---------------------------------------------------- -- Using Extended EXPLAIN EXTENDED select k , sum ( v ) from values ( 1 , 2 ), ( 1 , 3 ) t ( k , v ) group by k ; + ----------------------------------------------------+ | plan | + ----------------------------------------------------+ | == Parsed Logical Plan == 'Aggregate [' k ], [ 'k, unresolvedalias(' sum ( 'v), None)] +- ' SubqueryAlias `t` +- 'UnresolvedInlineTable [k, v], [List(1, 2), List(1, 3)] == Analyzed Logical Plan == k: int, sum(v): bigint Aggregate [k#47], [k#47, sum(cast(v#48 as bigint)) AS sum(v)#50L] +- SubqueryAlias `t` +- LocalRelation [k#47, v#48] == Optimized Logical Plan == Aggregate [k#47], [k#47, sum(cast(v#48 as bigint)) AS sum(v)#50L] +- LocalRelation [k#47, v#48] == Physical Plan == *(2) HashAggregate(keys=[k#47], functions=[sum(cast(v#48 as bigint))], output=[k#47, sum(v)#50L]) +- Exchange hashpartitioning(k#47, 200), true, [id=#79] +- *(1) HashAggregate(keys=[k#47], functions=[partial_sum(cast(v#48 as bigint))], output=[k#47, sum#52L]) +- *(1) LocalTableScan [k#47, v#48] | +----------------------------------------------------+ -- Using Formatted EXPLAIN FORMATTED select k, sum(v) from values (1, 2), (1, 3) t(k, v) group by k; +----------------------------------------------------+ | plan| +----------------------------------------------------+ | == Physical Plan == * HashAggregate (4) +- Exchange (3) +- * HashAggregate (2) +- * LocalTableScan (1) (1) LocalTableScan [codegen id : 1] Output: [k#19, v#20] (2) HashAggregate [codegen id : 1] Input: [k#19, v#20] (3) Exchange Input: [k#19, sum#24L] (4) HashAggregate [codegen id : 2] Input: [k#19, sum#24L] | +----------------------------------------------------+

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-cte.html
Common Table Expression (CTE) - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions Common Table Expression (CTE) Description A common table expression (CTE) defines a temporary result set that a user can reference possibly multiple times within the scope of a SQL statement. A CTE is used mainly in a SELECT statement. Syntax WITH common_table_expression [ , ... ] While common_table_expression is defined as expression_name [ ( column_name [ , ... ] ) ] [ AS ] ( query ) Parameters expression_name Specifies a name for the common table expression. query A SELECT statement . Examples -- CTE with multiple column aliases WITH t ( x , y ) AS ( SELECT 1 , 2 ) SELECT * FROM t WHERE x = 1 AND y = 2 ; + ---+---+ | x | y | + ---+---+ | 1 | 2 | + ---+---+ -- CTE in CTE definition WITH t AS ( WITH t2 AS ( SELECT 1 ) SELECT * FROM t2 ) SELECT * FROM t ; + ---+ | 1 | + ---+ | 1 | + ---+ -- CTE in subquery SELECT max ( c ) FROM ( WITH t ( c ) AS ( SELECT 1 ) SELECT * FROM t ); + ------+ | max ( c ) | + ------+ | 1 | + ------+ -- CTE in subquery expression SELECT ( WITH t AS ( SELECT 1 ) SELECT * FROM t ); + ----------------+ | scalarsubquery () | + ----------------+ | 1 | + ----------------+ -- CTE in CREATE VIEW statement CREATE VIEW v AS WITH t ( a , b , c , d ) AS ( SELECT 1 , 2 , 3 , 4 ) SELECT * FROM t ; SELECT * FROM v ; + ---+---+---+---+ | a | b | c | d | + ---+---+---+---+ | 1 | 2 | 3 | 4 | + ---+---+---+---+ -- If name conflict is detected in nested CTE, then AnalysisException is thrown by default. -- SET spark.sql.legacy.ctePrecedencePolicy = CORRECTED (which is recommended), -- inner CTE definitions take precedence over outer definitions. SET spark . sql . legacy . ctePrecedencePolicy = CORRECTED ; WITH t AS ( SELECT 1 ), t2 AS ( WITH t AS ( SELECT 2 ) SELECT * FROM t ) SELECT * FROM t2 ; + ---+ | 2 | + ---+ | 2 | + ---+ Related Statements SELECT

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-clusterby.html
CLUSTER BY Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions CLUSTER BY Clause Description The CLUSTER BY clause is used to first repartition the data based on the input expressions and then sort the data within each partition. This is semantically equivalent to performing a DISTRIBUTE BY followed by a SORT BY . This clause only ensures that the resultant rows are sorted within each partition and does not guarantee a total order of output. Syntax CLUSTER BY { expression [ , ... ] } Parameters expression Specifies combination of one or more values, operators and SQL functions that results in a value. Examples CREATE TABLE person ( name STRING , age INT ); INSERT INTO person VALUES ( 'Zen Hui' , 25 ), ( 'Anil B' , 18 ), ( 'Shone S' , 16 ), ( 'Mike A' , 25 ), ( 'John A' , 18 ), ( 'Jack N' , 16 ); -- Reduce the number of shuffle partitions to 2 to illustrate the behavior of `CLUSTER BY`. -- It's easier to see the clustering and sorting behavior with less number of partitions. SET spark . sql . shuffle . partitions = 2 ; -- Select the rows with no ordering. Please note that without any sort directive, the results -- of the query is not deterministic. It's included here to show the difference in behavior -- of a query when `CLUSTER BY` is not used vs when it's used. The query below produces rows -- where age column is not sorted. SELECT age , name FROM person ; + ---+-------+ | age | name | + ---+-------+ | 16 | Shone S | | 25 | Zen Hui | | 16 | Jack N | | 25 | Mike A | | 18 | John A | | 18 | Anil B | + ---+-------+ -- Produces rows clustered by age. Persons with same age are clustered together. -- In the query below, persons with age 18 and 25 are in first partition and the -- persons with age 16 are in the second partition. The rows are sorted based -- on age within each partition. SELECT age , name FROM person CLUSTER BY age ; + ---+-------+ | age | name | + ---+-------+ | 18 | John A | | 18 | Anil B | | 25 | Zen Hui | | 25 | Mike A | | 16 | Shone S | | 16 | Jack N | + ---+-------+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-distribute-by.html
DISTRIBUTE BY Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DISTRIBUTE BY Clause Description The DISTRIBUTE BY clause is used to repartition the data based on the input expressions. Unlike the CLUSTER BY clause, this does not sort the data within each partition. Syntax DISTRIBUTE BY { expression [ , ... ] } Parameters expression Specifies combination of one or more values, operators and SQL functions that results in a value. Examples CREATE TABLE person ( name STRING , age INT ); INSERT INTO person VALUES ( 'Zen Hui' , 25 ), ( 'Anil B' , 18 ), ( 'Shone S' , 16 ), ( 'Mike A' , 25 ), ( 'John A' , 18 ), ( 'Jack N' , 16 ); -- Reduce the number of shuffle partitions to 2 to illustrate the behavior of `DISTRIBUTE BY`. -- It's easier to see the clustering and sorting behavior with less number of partitions. SET spark . sql . shuffle . partitions = 2 ; -- Select the rows with no ordering. Please note that without any sort directive, the result -- of the query is not deterministic. It's included here to just contrast it with the -- behavior of `DISTRIBUTE BY`. The query below produces rows where age columns are not -- clustered together. SELECT age , name FROM person ; + ---+-------+ | age | name | + ---+-------+ | 16 | Shone S | | 25 | Zen Hui | | 16 | Jack N | | 25 | Mike A | | 18 | John A | | 18 | Anil B | + ---+-------+ -- Produces rows clustered by age. Persons with same age are clustered together. -- Unlike `CLUSTER BY` clause, the rows are not sorted within a partition. SELECT age , name FROM person DISTRIBUTE BY age ; + ---+-------+ | age | name | + ---+-------+ | 25 | Zen Hui | | 25 | Mike A | | 18 | John A | | 18 | Anil B | | 16 | Shone S | | 16 | Jack N | + ---+-------+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause CLUSTER BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-groupby.html
GROUP BY Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions GROUP BY Clause Description The GROUP BY clause is used to group the rows based on a set of specified grouping expressions and compute aggregations on the group of rows based on one or more specified aggregate functions. Spark also supports advanced aggregations to do multiple aggregations for the same input record set via GROUPING SETS , CUBE , ROLLUP clauses. The grouping expressions and advanced aggregations can be mixed in the GROUP BY clause and nested in a GROUPING SETS clause. See more details in the Mixed/Nested Grouping Analytics section. When a FILTER clause is attached to an aggregate function, only the matching rows are passed to that function. Syntax GROUP BY group_expression [ , group_expression [ , ... ] ] [ WITH { ROLLUP | CUBE } ] GROUP BY { group_expression | { ROLLUP | CUBE | GROUPING SETS } ( grouping_set [ , ...]) } [ , ... ] While aggregate functions are defined as aggregate_name ( [ DISTINCT ] expression [ , ... ] ) [ FILTER ( WHERE boolean_expression ) ] Parameters group_expression Specifies the criteria based on which the rows are grouped together. The grouping of rows is performed based on result values of the grouping expressions. A grouping expression may be a column name like GROUP BY a , a column position like GROUP BY 0 , or an expression like GROUP BY a + b . grouping_set A grouping set is specified by zero or more comma-separated expressions in parentheses. When the grouping set has only one element, parentheses can be omitted. For example, GROUPING SETS ((a), (b)) is the same as GROUPING SETS (a, b) . Syntax: { ( [ expression [ , ... ] ] ) | expression } GROUPING SETS Groups the rows for each grouping set specified after GROUPING SETS. For example, GROUP BY GROUPING SETS ((warehouse), (product)) is semantically equivalent to union of results of GROUP BY warehouse and GROUP BY product . This clause is a shorthand for a UNION ALL where each leg of the UNION ALL operator performs aggregation of each grouping set specified in the GROUPING SETS clause. Similarly, GROUP BY GROUPING SETS ((warehouse, product), (product), ()) is semantically equivalent to the union of results of GROUP BY warehouse, product , GROUP BY product and global aggregate. Note: For Hive compatibility Spark allows GROUP BY ... GROUPING SETS (...) . The GROUP BY expressions are usually ignored, but if it contains extra expressions than the GROUPING SETS expressions, the extra expressions will be included in the grouping expressions and the value is always null. For example, SELECT a, b, c FROM ... GROUP BY a, b, c GROUPING SETS (a, b) , the output of column c is always null. ROLLUP Specifies multiple levels of aggregations in a single statement. This clause is used to compute aggregations based on multiple grouping sets. ROLLUP is a shorthand for GROUPING SETS . For example, GROUP BY warehouse, product WITH ROLLUP or GROUP BY ROLLUP(warehouse, product) is equivalent to GROUP BY GROUPING SETS((warehouse, product), (warehouse), ()) . GROUP BY ROLLUP(warehouse, product, (warehouse, location)) is equivalent to GROUP BY GROUPING SETS((warehouse, product, location), (warehouse, product), (warehouse), ()) . The N elements of a ROLLUP specification results in N+1 GROUPING SETS . CUBE CUBE clause is used to perform aggregations based on combination of grouping columns specified in the GROUP BY clause. CUBE is a shorthand for GROUPING SETS . For example, GROUP BY warehouse, product WITH CUBE or GROUP BY CUBE(warehouse, product) is equivalent to GROUP BY GROUPING SETS((warehouse, product), (warehouse), (product), ()) . GROUP BY CUBE(warehouse, product, (warehouse, location)) is equivalent to GROUP BY GROUPING SETS((warehouse, product, location), (warehouse, product), (warehouse, location), (product, warehouse, location), (warehouse), (product), (warehouse, product), ()) . The N elements of a CUBE specification results in 2^N GROUPING SETS . Mixed/Nested Grouping Analytics A GROUP BY clause can include multiple group_expression s and multiple CUBE|ROLLUP|GROUPING SETS s. GROUPING SETS can also have nested CUBE|ROLLUP|GROUPING SETS clauses, e.g. GROUPING SETS(ROLLUP(warehouse, location), CUBE(warehouse, location)) , GROUPING SETS(warehouse, GROUPING SETS(location, GROUPING SETS(ROLLUP(warehouse, location), CUBE(warehouse, location)))) . CUBE|ROLLUP is just a syntax sugar for GROUPING SETS , please refer to the sections above for how to translate CUBE|ROLLUP to GROUPING SETS . group_expression can be treated as a single-group GROUPING SETS under this context. For multiple GROUPING SETS in the GROUP BY clause, we generate a single GROUPING SETS by doing a cross-product of the original GROUPING SETS s. For nested GROUPING SETS in the GROUPING SETS clause, we simply take its grouping sets and strip it. For example, GROUP BY warehouse, GROUPING SETS((product), ()), GROUPING SETS((location, size), (location), (size), ()) and GROUP BY warehouse, ROLLUP(product), CUBE(location, size) is equivalent to GROUP BY GROUPING SETS( (warehouse, product, location, size), (warehouse, product, location), (warehouse, product, size), (warehouse, product), (warehouse, location, size), (warehouse, location), (warehouse, size), (warehouse)) . GROUP BY GROUPING SETS(GROUPING SETS(warehouse), GROUPING SETS((warehouse, product))) is equivalent to GROUP BY GROUPING SETS((warehouse), (warehouse, product)) . aggregate_name Specifies an aggregate function name (MIN, MAX, COUNT, SUM, AVG, etc.). DISTINCT Removes duplicates in input rows before they are passed to aggregate functions. FILTER Filters the input rows for which the boolean_expression in the WHERE clause evaluates to true are passed to the aggregate function; other rows are discarded. Examples CREATE TABLE dealer ( id INT , city STRING , car_model STRING , quantity INT ); INSERT INTO dealer VALUES ( 100 , 'Fremont' , 'Honda Civic' , 10 ), ( 100 , 'Fremont' , 'Honda Accord' , 15 ), ( 100 , 'Fremont' , 'Honda CRV' , 7 ), ( 200 , 'Dublin' , 'Honda Civic' , 20 ), ( 200 , 'Dublin' , 'Honda Accord' , 10 ), ( 200 , 'Dublin' , 'Honda CRV' , 3 ), ( 300 , 'San Jose' , 'Honda Civic' , 5 ), ( 300 , 'San Jose' , 'Honda Accord' , 8 ); -- Sum of quantity per dealership. Group by `id`. SELECT id , sum ( quantity ) FROM dealer GROUP BY id ORDER BY id ; + ---+-------------+ | id | sum ( quantity ) | + ---+-------------+ | 100 | 32 | | 200 | 33 | | 300 | 13 | + ---+-------------+ -- Use column position in GROUP by clause. SELECT id , sum ( quantity ) FROM dealer GROUP BY 1 ORDER BY 1 ; + ---+-------------+ | id | sum ( quantity ) | + ---+-------------+ | 100 | 32 | | 200 | 33 | | 300 | 13 | + ---+-------------+ -- Multiple aggregations. -- 1. Sum of quantity per dealership. -- 2. Max quantity per dealership. SELECT id , sum ( quantity ) AS sum , max ( quantity ) AS max FROM dealer GROUP BY id ORDER BY id ; + ---+---+---+ | id | sum | max | + ---+---+---+ | 100 | 32 | 15 | | 200 | 33 | 20 | | 300 | 13 | 8 | + ---+---+---+ -- Count the number of distinct dealer cities per car_model. SELECT car_model , count ( DISTINCT city ) AS count FROM dealer GROUP BY car_model ; + ------------+-----+ | car_model | count | + ------------+-----+ | Honda Civic | 3 | | Honda CRV | 2 | | Honda Accord | 3 | + ------------+-----+ -- Sum of only 'Honda Civic' and 'Honda CRV' quantities per dealership. SELECT id , sum ( quantity ) FILTER ( WHERE car_model IN ( 'Honda Civic' , 'Honda CRV' ) ) AS `sum(quantity)` FROM dealer GROUP BY id ORDER BY id ; + ---+-------------+ | id | sum ( quantity ) | + ---+-------------+ | 100 | 17 | | 200 | 23 | | 300 | 5 | + ---+-------------+ -- Aggregations using multiple sets of grouping columns in a single statement. -- Following performs aggregations based on four sets of grouping columns. -- 1. city, car_model -- 2. city -- 3. car_model -- 4. Empty grouping set. Returns quantities for all city and car models. SELECT city , car_model , sum ( quantity ) AS sum FROM dealer GROUP BY GROUPING SETS (( city , car_model ), ( city ), ( car_model ), ()) ORDER BY city ; + ---------+------------+---+ | city | car_model | sum | + ---------+------------+---+ | null | null | 78 | | null | HondaAccord | 33 | | null | HondaCRV | 10 | | null | HondaCivic | 35 | | Dublin | null | 33 | | Dublin | HondaAccord | 10 | | Dublin | HondaCRV | 3 | | Dublin | HondaCivic | 20 | | Fremont | null | 32 | | Fremont | HondaAccord | 15 | | Fremont | HondaCRV | 7 | | Fremont | HondaCivic | 10 | | San Jose | null | 13 | | San Jose | HondaAccord | 8 | | San Jose | HondaCivic | 5 | + ---------+------------+---+ -- Group by processing with `ROLLUP` clause. -- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), ()) SELECT city , car_model , sum ( quantity ) AS sum FROM dealer GROUP BY city , car_model WITH ROLLUP ORDER BY city , car_model ; + ---------+------------+---+ | city | car_model | sum | + ---------+------------+---+ | null | null | 78 | | Dublin | null | 33 | | Dublin | HondaAccord | 10 | | Dublin | HondaCRV | 3 | | Dublin | HondaCivic | 20 | | Fremont | null | 32 | | Fremont | HondaAccord | 15 | | Fremont | HondaCRV | 7 | | Fremont | HondaCivic | 10 | | San Jose | null | 13 | | San Jose | HondaAccord | 8 | | San Jose | HondaCivic | 5 | + ---------+------------+---+ -- Group by processing with `CUBE` clause. -- Equivalent GROUP BY GROUPING SETS ((city, car_model), (city), (car_model), ()) SELECT city , car_model , sum ( quantity ) AS sum FROM dealer GROUP BY city , car_model WITH CUBE ORDER BY city , car_model ; + ---------+------------+---+ | city | car_model | sum | + ---------+------------+---+ | null | null | 78 | | null | HondaAccord | 33 | | null | HondaCRV | 10 | | null | HondaCivic | 35 | | Dublin | null | 33 | | Dublin | HondaAccord | 10 | | Dublin | HondaCRV | 3 | | Dublin | HondaCivic | 20 | | Fremont | null | 32 | | Fremont | HondaAccord | 15 | | Fremont | HondaCRV | 7 | | Fremont | HondaCivic | 10 | | San Jose | null | 13 | | San Jose | HondaAccord | 8 | | San Jose | HondaCivic | 5 | + ---------+------------+---+ --Prepare data for ignore nulls example CREATE TABLE person ( id INT , name STRING , age INT ); INSERT INTO person VALUES ( 100 , 'Mary' , NULL ), ( 200 , 'John' , 30 ), ( 300 , 'Mike' , 80 ), ( 400 , 'Dan' , 50 ); --Select the first row in column age SELECT FIRST ( age ) FROM person ; + --------------------+ | first ( age , false ) | + --------------------+ | NULL | + --------------------+ --Get the first row in column `age` ignore nulls,last row in column `id` and sum of column `id`. SELECT FIRST ( age IGNORE NULLS ), LAST ( id ), SUM ( id ) FROM person ; + -------------------+------------------+----------+ | first ( age , true ) | last ( id , false ) | sum ( id ) | + -------------------+------------------+----------+ | 30 | 400 | 1000 | + -------------------+------------------+----------+ Related Statements SELECT Main WHERE Clause HAVING Clause ORDER BY Clause SORT BY Clause CLUSTER BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-having.html
HAVING Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions HAVING Clause Description The HAVING clause is used to filter the results produced by GROUP BY based on the specified condition. It is often used in conjunction with a GROUP BY clause. Syntax HAVING boolean_expression Parameters boolean_expression Specifies any expression that evaluates to a result type boolean . Two or more expressions may be combined together using the logical operators ( AND , OR ). Note The expressions specified in the HAVING clause can only refer to: Constants Expressions that appear in GROUP BY Aggregate functions Examples CREATE TABLE dealer ( id INT , city STRING , car_model STRING , quantity INT ); INSERT INTO dealer VALUES ( 100 , 'Fremont' , 'Honda Civic' , 10 ), ( 100 , 'Fremont' , 'Honda Accord' , 15 ), ( 100 , 'Fremont' , 'Honda CRV' , 7 ), ( 200 , 'Dublin' , 'Honda Civic' , 20 ), ( 200 , 'Dublin' , 'Honda Accord' , 10 ), ( 200 , 'Dublin' , 'Honda CRV' , 3 ), ( 300 , 'San Jose' , 'Honda Civic' , 5 ), ( 300 , 'San Jose' , 'Honda Accord' , 8 ); -- `HAVING` clause referring to column in `GROUP BY`. SELECT city , sum ( quantity ) AS sum FROM dealer GROUP BY city HAVING city = 'Fremont' ; + -------+---+ | city | sum | + -------+---+ | Fremont | 32 | + -------+---+ -- `HAVING` clause referring to aggregate function. SELECT city , sum ( quantity ) AS sum FROM dealer GROUP BY city HAVING sum ( quantity ) > 15 ; + -------+---+ | city | sum | + -------+---+ | Dublin | 33 | | Fremont | 32 | + -------+---+ -- `HAVING` clause referring to aggregate function by its alias. SELECT city , sum ( quantity ) AS sum FROM dealer GROUP BY city HAVING sum > 15 ; + -------+---+ | city | sum | + -------+---+ | Dublin | 33 | | Fremont | 32 | + -------+---+ -- `HAVING` clause referring to a different aggregate function than what is present in -- `SELECT` list. SELECT city , sum ( quantity ) AS sum FROM dealer GROUP BY city HAVING max ( quantity ) > 15 ; + ------+---+ | city | sum | + ------+---+ | Dublin | 33 | + ------+---+ -- `HAVING` clause referring to constant expression. SELECT city , sum ( quantity ) AS sum FROM dealer GROUP BY city HAVING 1 > 0 ORDER BY city ; + --------+---+ | city | sum | + --------+---+ | Dublin | 33 | | Fremont | 32 | | San Jose | 13 | + --------+---+ -- `HAVING` clause without a `GROUP BY` clause. SELECT sum ( quantity ) AS sum FROM dealer HAVING sum ( quantity ) > 10 ; + ---+ | sum | + ---+ | 78 | + ---+ Related Statements SELECT Main WHERE Clause GROUP BY Clause ORDER BY Clause SORT BY Clause CLUSTER BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-hints.html
Hints - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions Hints Description Hints give users a way to suggest how Spark SQL to use specific approaches to generate its execution plan. Syntax /*+ hint [ , ... ] */ Partitioning Hints Partitioning hints allow users to suggest a partitioning strategy that Spark should follow. COALESCE , REPARTITION , and REPARTITION_BY_RANGE hints are supported and are equivalent to coalesce , repartition , and repartitionByRange Dataset APIs , respectively. The REBALANCE can only be used as a hint .These hints give users a way to tune performance and control the number of output files in Spark SQL. When multiple partitioning hints are specified, multiple nodes are inserted into the logical plan, but the leftmost hint is picked by the optimizer. Partitioning Hints Types COALESCE The COALESCE hint can be used to reduce the number of partitions to the specified number of partitions. It takes a partition number as a parameter. REPARTITION The REPARTITION hint can be used to repartition to the specified number of partitions using the specified partitioning expressions. It takes a partition number, column names, or both as parameters. REPARTITION_BY_RANGE The REPARTITION_BY_RANGE hint can be used to repartition to the specified number of partitions using the specified partitioning expressions. It takes column names and an optional partition number as parameters. REBALANCE The REBALANCE hint can be used to rebalance the query result output partitions, so that every partition is of a reasonable size (not too small and not too big). It can take column names as parameters, and try its best to partition the query result by these columns. This is a best-effort: if there are skews, Spark will split the skewed partitions, to make these partitions not too big. This hint is useful when you need to write the result of this query to a table, to avoid too small/big files. This hint is ignored if AQE is not enabled. Examples SELECT /*+ COALESCE(3) */ * FROM t ; SELECT /*+ REPARTITION(3) */ * FROM t ; SELECT /*+ REPARTITION(c) */ * FROM t ; SELECT /*+ REPARTITION(3, c) */ * FROM t ; SELECT /*+ REPARTITION_BY_RANGE(c) */ * FROM t ; SELECT /*+ REPARTITION_BY_RANGE(3, c) */ * FROM t ; SELECT /*+ REBALANCE */ * FROM t ; SELECT /*+ REBALANCE(3) */ * FROM t ; SELECT /*+ REBALANCE(c) */ * FROM t ; SELECT /*+ REBALANCE(3, c) */ * FROM t ; -- multiple partitioning hints EXPLAIN EXTENDED SELECT /*+ REPARTITION(100), COALESCE(500), REPARTITION_BY_RANGE(3, c) */ * FROM t ; == Parsed Logical Plan == 'UnresolvedHint REPARTITION, [100] +- ' UnresolvedHint COALESCE , [ 500 ] +- 'UnresolvedHint REPARTITION_BY_RANGE, [3, ' c ] +- 'Project [*] +- ' UnresolvedRelation [ t ] == Analyzed Logical Plan == name : string , c : int Repartition 100 , true +- Repartition 500 , false +- RepartitionByExpression [ c # 30 ASC NULLS FIRST ], 3 +- Project [ name # 29 , c # 30 ] +- SubqueryAlias spark_catalog . default . t +- Relation [ name # 29 , c # 30 ] parquet == Optimized Logical Plan == Repartition 100 , true +- Relation [ name # 29 , c # 30 ] parquet == Physical Plan == Exchange RoundRobinPartitioning ( 100 ), false , [ id =# 121 ] +- * ( 1 ) ColumnarToRow +- FileScan parquet default . t [ name # 29 , c # 30 ] Batched : true , DataFilters : [], Format : Parquet , Location : CatalogFileIndex [ file : / spark / spark - warehouse / t ], PartitionFilters : [], PushedFilters : [], ReadSchema : struct < name : string > Join Hints Join hints allow users to suggest the join strategy that Spark should use. Prior to Spark 3.0, only the BROADCAST Join Hint was supported. MERGE , SHUFFLE_HASH and SHUFFLE_REPLICATE_NL Joint Hints support was added in 3.0. When different join strategy hints are specified on both sides of a join, Spark prioritizes hints in the following order: BROADCAST over MERGE over SHUFFLE_HASH over SHUFFLE_REPLICATE_NL . When both sides are specified with the BROADCAST hint or the SHUFFLE_HASH hint, Spark will pick the build side based on the join type and the sizes of the relations. Since a given strategy may not support all join types, Spark is not guaranteed to use the join strategy suggested by the hint. Join Hints Types BROADCAST Suggests that Spark use broadcast join. The join side with the hint will be broadcast regardless of autoBroadcastJoinThreshold . If both sides of the join have the broadcast hints, the one with the smaller size (based on stats) will be broadcast. The aliases for BROADCAST are BROADCASTJOIN and MAPJOIN . MERGE Suggests that Spark use shuffle sort merge join. The aliases for MERGE are SHUFFLE_MERGE and MERGEJOIN . SHUFFLE_HASH Suggests that Spark use shuffle hash join. If both sides have the shuffle hash hints, Spark chooses the smaller side (based on stats) as the build side. SHUFFLE_REPLICATE_NL Suggests that Spark use shuffle-and-replicate nested loop join. Examples -- Join Hints for broadcast join SELECT /*+ BROADCAST(t1) */ * FROM t1 INNER JOIN t2 ON t1 . key = t2 . key ; SELECT /*+ BROADCASTJOIN (t1) */ * FROM t1 left JOIN t2 ON t1 . key = t2 . key ; SELECT /*+ MAPJOIN(t2) */ * FROM t1 right JOIN t2 ON t1 . key = t2 . key ; -- Join Hints for shuffle sort merge join SELECT /*+ SHUFFLE_MERGE(t1) */ * FROM t1 INNER JOIN t2 ON t1 . key = t2 . key ; SELECT /*+ MERGEJOIN(t2) */ * FROM t1 INNER JOIN t2 ON t1 . key = t2 . key ; SELECT /*+ MERGE(t1) */ * FROM t1 INNER JOIN t2 ON t1 . key = t2 . key ; -- Join Hints for shuffle hash join SELECT /*+ SHUFFLE_HASH(t1) */ * FROM t1 INNER JOIN t2 ON t1 . key = t2 . key ; -- Join Hints for shuffle-and-replicate nested loop join SELECT /*+ SHUFFLE_REPLICATE_NL(t1) */ * FROM t1 INNER JOIN t2 ON t1 . key = t2 . key ; -- When different join strategy hints are specified on both sides of a join, Spark -- prioritizes the BROADCAST hint over the MERGE hint over the SHUFFLE_HASH hint -- over the SHUFFLE_REPLICATE_NL hint. -- Spark will issue Warning in the following example -- org.apache.spark.sql.catalyst.analysis.HintErrorLogger: Hint (strategy=merge) -- is overridden by another hint and will not take effect. SELECT /*+ BROADCAST(t1), MERGE(t1, t2) */ * FROM t1 INNER JOIN t2 ON t1 . key = t2 . key ; Related Statements JOIN SELECT

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-inline-table.html
Inline Table - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions Inline Table Description An inline table is a temporary table created using a VALUES clause. Syntax VALUES ( expression [ , ... ] ) [ table_alias ] Parameters expression Specifies a combination of one or more values, operators and SQL functions that results in a value. table_alias Specifies a temporary name with an optional column name list. Syntax: [ AS ] table_name [ ( column_name [ , ... ] ) ] Examples -- single row, without a table alias SELECT * FROM VALUES ( "one" , 1 ); + ----+----+ | col1 | col2 | + ----+----+ | one | 1 | + ----+----+ -- three rows with a table alias SELECT * FROM VALUES ( "one" , 1 ), ( "two" , 2 ), ( "three" , null ) AS data ( a , b ); + -----+----+ | a | b | + -----+----+ | one | 1 | | two | 2 | | three | null | + -----+----+ -- complex types with a table alias SELECT * FROM VALUES ( "one" , array ( 0 , 1 )), ( "two" , array ( 2 , 3 )) AS data ( a , b ); + ---+------+ | a | b | + ---+------+ | one | [ 0 , 1 ] | | two | [ 2 , 3 ] | + ---+------+ Related Statements SELECT

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-file.html
File - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions File Description You can query a file with a specified format directly with SQL. Syntax file_format . `file_path` Parameters file_format Specifies a file format for a given file path, could be TEXTFILE, ORC, PARQUET, etc. file_path Specifies a file path with a given format. Examples -- PARQUET file SELECT * FROM parquet . `examples/src/main/resources/users.parquet` ; + ------+--------------+----------------+ | name | favorite_color | favorite_numbers | + ------+--------------+----------------+ | Alyssa | null | [ 3 , 9 , 15 , 20 ] | | Ben | red | [] | + ------+--------------+----------------+ -- ORC file SELECT * FROM orc . `examples/src/main/resources/users.orc` ; + ------+--------------+----------------+ | name | favorite_color | favorite_numbers | + ------+--------------+----------------+ | Alyssa | null | [ 3 , 9 , 15 , 20 ] | | Ben | red | [] | + ------+--------------+----------------+ -- JSON file SELECT * FROM json . `examples/src/main/resources/people.json` ; + ----+-------+ | age | name | + ----+-------+ | null | Michael | | 30 | Andy | | 19 | Justin | + ----+-------+ Related Statements SELECT

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-join.html
JOIN - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions JOIN Description A SQL join is used to combine rows from two relations based on join criteria. The following section describes the overall join syntax and the sub-sections cover different types of joins along with examples. Syntax relation { [ join_type ] JOIN [ LATERAL ] relation [ join_criteria ] | NATURAL join_type JOIN [ LATERAL ] relation } Parameters relation Specifies the relation to be joined. join_type Specifies the join type. Syntax: [ INNER ] | CROSS | LEFT [ OUTER ] | [ LEFT ] SEMI | RIGHT [ OUTER ] | FULL [ OUTER ] | [ LEFT ] ANTI join_criteria Specifies how the rows from one relation will be combined with the rows of another relation. Syntax: ON boolean_expression | USING ( column_name [ , ... ] ) boolean_expression Specifies an expression with a return type of boolean. Join Types Inner Join The inner join is the default join in Spark SQL. It selects rows that have matching values in both relations. Syntax: relation [ INNER ] JOIN relation [ join_criteria ] Left Join A left join returns all values from the left relation and the matched values from the right relation, or appends NULL if there is no match. It is also referred to as a left outer join. Syntax: relation LEFT [ OUTER ] JOIN relation [ join_criteria ] Right Join A right join returns all values from the right relation and the matched values from the left relation, or appends NULL if there is no match. It is also referred to as a right outer join. Syntax: relation RIGHT [ OUTER ] JOIN relation [ join_criteria ] Full Join A full join returns all values from both relations, appending NULL values on the side that does not have a match. It is also referred to as a full outer join. Syntax: relation FULL [ OUTER ] JOIN relation [ join_criteria ] Cross Join A cross join returns the Cartesian product of two relations. Syntax: relation CROSS JOIN relation [ join_criteria ] Semi Join A semi join returns values from the left side of the relation that has a match with the right. It is also referred to as a left semi join. Syntax: relation [ LEFT ] SEMI JOIN relation [ join_criteria ] Anti Join An anti join returns values from the left relation that has no match with the right. It is also referred to as a left anti join. Syntax: relation [ LEFT ] ANTI JOIN relation [ join_criteria ] Examples -- Use employee and department tables to demonstrate different type of joins. SELECT * FROM employee ; + ---+-----+------+ | id | name | deptno | + ---+-----+------+ | 105 | Chloe | 5 | | 103 | Paul | 3 | | 101 | John | 1 | | 102 | Lisa | 2 | | 104 | Evan | 4 | | 106 | Amy | 6 | + ---+-----+------+ SELECT * FROM department ; + ------+-----------+ | deptno | deptname | + ------+-----------+ | 3 | Engineering | | 2 | Sales | | 1 | Marketing | + ------+-----------+ -- Use employee and department tables to demonstrate inner join. SELECT id , name , employee . deptno , deptname FROM employee INNER JOIN department ON employee . deptno = department . deptno ; + ---+-----+------+-----------| | id | name | deptno | deptname | + ---+-----+------+-----------| | 103 | Paul | 3 | Engineering | | 101 | John | 1 | Marketing | | 102 | Lisa | 2 | Sales | + ---+-----+------+-----------| -- Use employee and department tables to demonstrate left join. SELECT id , name , employee . deptno , deptname FROM employee LEFT JOIN department ON employee . deptno = department . deptno ; + ---+-----+------+-----------| | id | name | deptno | deptname | + ---+-----+------+-----------| | 105 | Chloe | 5 | NULL | | 103 | Paul | 3 | Engineering | | 101 | John | 1 | Marketing | | 102 | Lisa | 2 | Sales | | 104 | Evan | 4 | NULL | | 106 | Amy | 6 | NULL | + ---+-----+------+-----------| -- Use employee and department tables to demonstrate right join. SELECT id , name , employee . deptno , deptname FROM employee RIGHT JOIN department ON employee . deptno = department . deptno ; + ---+-----+------+-----------| | id | name | deptno | deptname | + ---+-----+------+-----------| | 103 | Paul | 3 | Engineering | | 101 | John | 1 | Marketing | | 102 | Lisa | 2 | Sales | + ---+-----+------+-----------| -- Use employee and department tables to demonstrate full join. SELECT id , name , employee . deptno , deptname FROM employee FULL JOIN department ON employee . deptno = department . deptno ; + ---+-----+------+-----------| | id | name | deptno | deptname | + ---+-----+------+-----------| | 101 | John | 1 | Marketing | | 106 | Amy | 6 | NULL | | 103 | Paul | 3 | Engineering | | 105 | Chloe | 5 | NULL | | 104 | Evan | 4 | NULL | | 102 | Lisa | 2 | Sales | + ---+-----+------+-----------| -- Use employee and department tables to demonstrate cross join. SELECT id , name , employee . deptno , deptname FROM employee CROSS JOIN department ; + ---+-----+------+-----------| | id | name | deptno | deptname | + ---+-----+------+-----------| | 105 | Chloe | 5 | Engineering | | 105 | Chloe | 5 | Marketing | | 105 | Chloe | 5 | Sales | | 103 | Paul | 3 | Engineering | | 103 | Paul | 3 | Marketing | | 103 | Paul | 3 | Sales | | 101 | John | 1 | Engineering | | 101 | John | 1 | Marketing | | 101 | John | 1 | Sales | | 102 | Lisa | 2 | Engineering | | 102 | Lisa | 2 | Marketing | | 102 | Lisa | 2 | Sales | | 104 | Evan | 4 | Engineering | | 104 | Evan | 4 | Marketing | | 104 | Evan | 4 | Sales | | 106 | Amy | 4 | Engineering | | 106 | Amy | 4 | Marketing | | 106 | Amy | 4 | Sales | + ---+-----+------+-----------| -- Use employee and department tables to demonstrate semi join. SELECT * FROM employee SEMI JOIN department ON employee . deptno = department . deptno ; + ---+-----+------+ | id | name | deptno | + ---+-----+------+ | 103 | Paul | 3 | | 101 | John | 1 | | 102 | Lisa | 2 | + ---+-----+------+ -- Use employee and department tables to demonstrate anti join. SELECT * FROM employee ANTI JOIN department ON employee . deptno = department . deptno ; + ---+-----+------+ | id | name | deptno | + ---+-----+------+ | 105 | Chloe | 5 | | 104 | Evan | 4 | | 106 | Amy | 6 | + ---+-----+------+ Related Statements SELECT Hints LATERAL Subquery

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-like.html
LIKE Predicate - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions LIKE Predicate Description A LIKE predicate is used to search for a specific pattern. This predicate also supports multiple patterns with quantifiers include ANY , SOME and ALL . Syntax [ NOT ] { LIKE search_pattern [ ESCAPE esc_char ] | [ RLIKE | REGEXP ] regex_pattern } [ NOT ] { LIKE quantifiers ( search_pattern [ , ... ]) } Parameters search_pattern Specifies a string pattern to be searched by the LIKE clause. It can contain special pattern-matching characters: % matches zero or more characters. _ matches exactly one character. esc_char Specifies the escape character. The default escape character is \ . regex_pattern Specifies a regular expression search pattern to be searched by the RLIKE or REGEXP clause. quantifiers Specifies the predicate quantifiers include ANY , SOME and ALL . ANY or SOME means if one of the patterns matches the input, then return true; ALL means if all the patterns matches the input, then return true. Examples CREATE TABLE person ( id INT , name STRING , age INT ); INSERT INTO person VALUES ( 100 , 'John' , 30 ), ( 200 , 'Mary' , NULL ), ( 300 , 'Mike' , 80 ), ( 400 , 'Dan' , 50 ), ( 500 , 'Evan_w' , 16 ); SELECT * FROM person WHERE name LIKE 'M%' ; + ---+----+----+ | id | name | age | + ---+----+----+ | 300 | Mike | 80 | | 200 | Mary | null | + ---+----+----+ SELECT * FROM person WHERE name LIKE 'M_ry' ; + ---+----+----+ | id | name | age | + ---+----+----+ | 200 | Mary | null | + ---+----+----+ SELECT * FROM person WHERE name NOT LIKE 'M_ry' ; + ---+------+---+ | id | name | age | + ---+------+---+ | 500 | Evan_W | 16 | | 300 | Mike | 80 | | 100 | John | 30 | | 400 | Dan | 50 | + ---+------+---+ SELECT * FROM person WHERE name RLIKE 'M+' ; + ---+----+----+ | id | name | age | + ---+----+----+ | 300 | Mike | 80 | | 200 | Mary | null | + ---+----+----+ SELECT * FROM person WHERE name REGEXP 'M+' ; + ---+----+----+ | id | name | age | + ---+----+----+ | 300 | Mike | 80 | | 200 | Mary | null | + ---+----+----+ SELECT * FROM person WHERE name LIKE '% \_ %' ; + ---+------+---+ | id | name | age | + ---+------+---+ | 500 | Evan_W | 16 | + ---+------+---+ SELECT * FROM person WHERE name LIKE '%$_%' ESCAPE '$' ; + ---+------+---+ | id | name | age | + ---+------+---+ | 500 | Evan_W | 16 | + ---+------+---+ SELECT * FROM person WHERE name LIKE ALL ( '%an%' , '%an' ); + ---+----+----+ | id | name | age | + ---+----+----+ | 400 | Dan | 50 | + ---+----+----+ SELECT * FROM person WHERE name LIKE ANY ( '%an%' , '%an' ); + ---+------+---+ | id | name | age | + ---+------+---+ | 400 | Dan | 50 | | 500 | Evan_W | 16 | + ---+------+---+ SELECT * FROM person WHERE name LIKE SOME ( '%an%' , '%an' ); + ---+------+---+ | id | name | age | + ---+------+---+ | 400 | Dan | 50 | | 500 | Evan_W | 16 | + ---+------+---+ SELECT * FROM person WHERE name NOT LIKE ALL ( '%an%' , '%an' ); + ---+----+----+ | id | name | age | + ---+----+----+ | 100 | John | 30 | | 200 | Mary | null | | 300 | Mike | 80 | + ---+----+----+ SELECT * FROM person WHERE name NOT LIKE ANY ( '%an%' , '%an' ); + ---+------+----+ | id | name | age | + ---+------+----+ | 100 | John | 30 | | 200 | Mary | null | | 300 | Mike | 80 | | 500 | Evan_W | 16 | + ---+------+----+ SELECT * FROM person WHERE name NOT LIKE SOME ( '%an%' , '%an' ); + ---+------+----+ | id | name | age | + ---+------+----+ | 100 | John | 30 | | 200 | Mary | null | | 300 | Mike | 80 | | 500 | Evan_W | 16 | + ---+------+----+ Related Statements SELECT WHERE Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-limit.html
LIMIT Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions LIMIT Clause Description The LIMIT clause is used to constrain the number of rows returned by the SELECT statement. In general, this clause is used in conjunction with ORDER BY to ensure that the results are deterministic. Syntax LIMIT { ALL | integer_expression } Parameters ALL If specified, the query returns all the rows. In other words, no limit is applied if this option is specified. integer_expression Specifies a foldable expression that returns an integer. Examples CREATE TABLE person ( name STRING , age INT ); INSERT INTO person VALUES ( 'Zen Hui' , 25 ), ( 'Anil B' , 18 ), ( 'Shone S' , 16 ), ( 'Mike A' , 25 ), ( 'John A' , 18 ), ( 'Jack N' , 16 ); -- Select the first two rows. SELECT name , age FROM person ORDER BY name LIMIT 2 ; + ------+---+ | name | age | + ------+---+ | Anil B | 18 | | Jack N | 16 | + ------+---+ -- Specifying ALL option on LIMIT returns all the rows. SELECT name , age FROM person ORDER BY name LIMIT ALL ; + -------+---+ | name | age | + -------+---+ | Anil B | 18 | | Jack N | 16 | | John A | 18 | | Mike A | 25 | | Shone S | 16 | | Zen Hui | 25 | + -------+---+ -- A function expression as an input to LIMIT. SELECT name , age FROM person ORDER BY name LIMIT length ( 'SPARK' ); + -------+---+ | name | age | + -------+---+ | Anil B | 18 | | Jack N | 16 | | John A | 18 | | Mike A | 25 | | Shone S | 16 | + -------+---+ -- A non-foldable expression as an input to LIMIT is not allowed. SELECT name , age FROM person ORDER BY name LIMIT length ( name ); org . apache . spark . sql . AnalysisException : The limit expression must evaluate to a constant value ... Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause CLUSTER BY Clause DISTRIBUTE BY Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-offset.html
OFFSET Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions OFFSET Clause Description The OFFSET clause is used to specify the number of rows to skip before beginning to return rows returned by the SELECT statement. In general, this clause is used in conjunction with ORDER BY to ensure that the results are deterministic. Syntax OFFSET integer_expression Parameters integer_expression Specifies a foldable expression that returns an integer. Examples CREATE TABLE person ( name STRING , age INT ); INSERT INTO person VALUES ( 'Zen Hui' , 25 ), ( 'Anil B' , 18 ), ( 'Shone S' , 16 ), ( 'Mike A' , 25 ), ( 'John A' , 18 ), ( 'Jack N' , 16 ); -- Skip the first two rows. SELECT name , age FROM person ORDER BY name OFFSET 2 ; + -------+---+ | name | age | + -------+---+ | John A | 18 | | Mike A | 25 | | Shone S | 16 | | Zen Hui | 25 | + -------+---+ -- Skip the first two rows and returns the next three rows. SELECT name , age FROM person ORDER BY name LIMIT 3 OFFSET 2 ; + -------+---+ | name | age | + -------+---+ | John A | 18 | | Mike A | 25 | | Shone S | 16 | + -------+---+ -- A function expression as an input to OFFSET. SELECT name , age FROM person ORDER BY name OFFSET length ( 'SPARK' ); + -------+---+ | name | age | + -------+---+ | Zen Hui | 25 | + -------+---+ -- A non-foldable expression as an input to OFFSET is not allowed. SELECT name , age FROM person ORDER BY name OFFSET length ( name ); org . apache . spark . sql . AnalysisException : The offset expression must evaluate to a constant value ... Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause CLUSTER BY Clause DISTRIBUTE BY Clause LIMIT Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-orderby.html
ORDER BY Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions ORDER BY Clause Description The ORDER BY clause is used to return the result rows in a sorted manner in the user specified order. Unlike the SORT BY clause, this clause guarantees a total order in the output. Syntax ORDER BY { expression [ sort_direction | nulls_sort_order ] [ , ... ] } Parameters ORDER BY Specifies a comma-separated list of expressions along with optional parameters sort_direction and nulls_sort_order which are used to sort the rows. sort_direction Optionally specifies whether to sort the rows in ascending or descending order. The valid values for the sort direction are ASC for ascending and DESC for descending. If sort direction is not explicitly specified, then by default rows are sorted ascending. Syntax: [ ASC | DESC ] nulls_sort_order Optionally specifies whether NULL values are returned before/after non-NULL values. If null_sort_order is not specified, then NULLs sort first if sort order is ASC and NULLS sort last if sort order is DESC . If NULLS FIRST is specified, then NULL values are returned first regardless of the sort order. If NULLS LAST is specified, then NULL values are returned last regardless of the sort order. Syntax: [ NULLS { FIRST | LAST } ] Examples CREATE TABLE person ( id INT , name STRING , age INT ); INSERT INTO person VALUES ( 100 , 'John' , 30 ), ( 200 , 'Mary' , NULL ), ( 300 , 'Mike' , 80 ), ( 400 , 'Jerry' , NULL ), ( 500 , 'Dan' , 50 ); -- Sort rows by age. By default rows are sorted in ascending manner with NULL FIRST. SELECT name , age FROM person ORDER BY age ; + -----+----+ | name | age | + -----+----+ | Jerry | null | | Mary | null | | John | 30 | | Dan | 50 | | Mike | 80 | + -----+----+ -- Sort rows in ascending manner keeping null values to be last. SELECT name , age FROM person ORDER BY age NULLS LAST ; + -----+----+ | name | age | + -----+----+ | John | 30 | | Dan | 50 | | Mike | 80 | | Mary | null | | Jerry | null | + -----+----+ -- Sort rows by age in descending manner, which defaults to NULL LAST. SELECT name , age FROM person ORDER BY age DESC ; + -----+----+ | name | age | + -----+----+ | Mike | 80 | | Dan | 50 | | John | 30 | | Jerry | null | | Mary | null | + -----+----+ -- Sort rows in ascending manner keeping null values to be first. SELECT name , age FROM person ORDER BY age DESC NULLS FIRST ; + -----+----+ | name | age | + -----+----+ | Jerry | null | | Mary | null | | Mike | 80 | | Dan | 50 | | John | 30 | + -----+----+ -- Sort rows based on more than one column with each column having different -- sort direction. SELECT * FROM person ORDER BY name ASC , age DESC ; + ---+-----+----+ | id | name | age | + ---+-----+----+ | 500 | Dan | 50 | | 400 | Jerry | null | | 100 | John | 30 | | 200 | Mary | null | | 300 | Mike | 80 | + ---+-----+----+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause SORT BY Clause CLUSTER BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-setops.html
Set Operators - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions Set Operators Description Set operators are used to combine two input relations into a single one. Spark SQL supports three types of set operators: EXCEPT or MINUS INTERSECT UNION Note that input relations must have the same number of columns and compatible data types for the respective columns. EXCEPT EXCEPT and EXCEPT ALL return the rows that are found in one relation but not the other. EXCEPT (alternatively, EXCEPT DISTINCT ) takes only distinct rows while EXCEPT ALL does not remove duplicates from the result rows. Note that MINUS is an alias for EXCEPT . Syntax [ ( ] relation [ ) ] EXCEPT | MINUS [ ALL | DISTINCT ] [ ( ] relation [ ) ] Examples -- Use number1 and number2 tables to demonstrate set operators in this page. SELECT * FROM number1 ; + ---+ | c | + ---+ | 3 | | 1 | | 2 | | 2 | | 3 | | 4 | + ---+ SELECT * FROM number2 ; + ---+ | c | + ---+ | 5 | | 1 | | 2 | | 2 | + ---+ SELECT c FROM number1 EXCEPT SELECT c FROM number2 ; + ---+ | c | + ---+ | 3 | | 4 | + ---+ SELECT c FROM number1 MINUS SELECT c FROM number2 ; + ---+ | c | + ---+ | 3 | | 4 | + ---+ SELECT c FROM number1 EXCEPT ALL ( SELECT c FROM number2 ); + ---+ | c | + ---+ | 3 | | 3 | | 4 | + ---+ SELECT c FROM number1 MINUS ALL ( SELECT c FROM number2 ); + ---+ | c | + ---+ | 3 | | 3 | | 4 | + ---+ INTERSECT INTERSECT and INTERSECT ALL return the rows that are found in both relations. INTERSECT (alternatively, INTERSECT DISTINCT ) takes only distinct rows while INTERSECT ALL does not remove duplicates from the result rows. Syntax [ ( ] relation [ ) ] INTERSECT [ ALL | DISTINCT ] [ ( ] relation [ ) ] Examples ( SELECT c FROM number1 ) INTERSECT ( SELECT c FROM number2 ); + ---+ | c | + ---+ | 1 | | 2 | + ---+ ( SELECT c FROM number1 ) INTERSECT DISTINCT ( SELECT c FROM number2 ); + ---+ | c | + ---+ | 1 | | 2 | + ---+ ( SELECT c FROM number1 ) INTERSECT ALL ( SELECT c FROM number2 ); + ---+ | c | + ---+ | 1 | | 2 | | 2 | + ---+ UNION UNION and UNION ALL return the rows that are found in either relation. UNION (alternatively, UNION DISTINCT ) takes only distinct rows while UNION ALL does not remove duplicates from the result rows. Syntax [ ( ] relation [ ) ] UNION [ ALL | DISTINCT ] [ ( ] relation [ ) ] Examples ( SELECT c FROM number1 ) UNION ( SELECT c FROM number2 ); + ---+ | c | + ---+ | 1 | | 3 | | 5 | | 4 | | 2 | + ---+ ( SELECT c FROM number1 ) UNION DISTINCT ( SELECT c FROM number2 ); + ---+ | c | + ---+ | 1 | | 3 | | 5 | | 4 | | 2 | + ---+ SELECT c FROM number1 UNION ALL ( SELECT c FROM number2 ); + ---+ | c | + ---+ | 3 | | 1 | | 2 | | 2 | | 3 | | 4 | | 5 | | 1 | | 2 | | 2 | + ---+ Related Statements SELECT Statement

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-sortby.html
SORT BY Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SORT BY Clause Description The SORT BY clause is used to return the result rows sorted within each partition in the user specified order. When there is more than one partition SORT BY may return result that is partially ordered. This is different than ORDER BY clause which guarantees a total order of the output. Syntax SORT BY { expression [ sort_direction | nulls_sort_order ] [ , ... ] } Parameters SORT BY Specifies a comma-separated list of expressions along with optional parameters sort_direction and nulls_sort_order which are used to sort the rows within each partition. sort_direction Optionally specifies whether to sort the rows in ascending or descending order. The valid values for the sort direction are ASC for ascending and DESC for descending. If sort direction is not explicitly specified, then by default rows are sorted ascending. Syntax: [ ASC | DESC ] nulls_sort_order Optionally specifies whether NULL values are returned before/after non-NULL values. If null_sort_order is not specified, then NULLs sort first if sort order is ASC and NULLS sort last if sort order is DESC . If NULLS FIRST is specified, then NULL values are returned first regardless of the sort order. If NULLS LAST is specified, then NULL values are returned last regardless of the sort order. Syntax: [ NULLS { FIRST | LAST } ] Examples CREATE TABLE person ( zip_code INT , name STRING , age INT ); INSERT INTO person VALUES ( 94588 , 'Zen Hui' , 50 ), ( 94588 , 'Dan Li' , 18 ), ( 94588 , 'Anil K' , 27 ), ( 94588 , 'John V' , NULL ), ( 94511 , 'David K' , 42 ), ( 94511 , 'Aryan B.' , 18 ), ( 94511 , 'Lalit B.' , NULL ); -- Use `REPARTITION` hint to partition the data by `zip_code` to -- examine the `SORT BY` behavior. This is used in rest of the -- examples. -- Sort rows by `name` within each partition in ascending manner SELECT /*+ REPARTITION(zip_code) */ name , age , zip_code FROM person SORT BY name ; + --------+----+--------+ | name | age | zip_code | + --------+----+--------+ | Anil K | 27 | 94588 | | Dan Li | 18 | 94588 | | John V | null | 94588 | | Zen Hui | 50 | 94588 | | Aryan B . | 18 | 94511 | | David K | 42 | 94511 | | Lalit B . | null | 94511 | + --------+----+--------+ -- Sort rows within each partition using column position. SELECT /*+ REPARTITION(zip_code) */ name , age , zip_code FROM person SORT BY 1 ; + --------+----+--------+ | name | age | zip_code | + --------+----+--------+ | Anil K | 27 | 94588 | | Dan Li | 18 | 94588 | | John V | null | 94588 | | Zen Hui | 50 | 94588 | | Aryan B . | 18 | 94511 | | David K | 42 | 94511 | | Lalit B . | null | 94511 | + --------+----+--------+ -- Sort rows within partition in ascending manner keeping null values to be last. SELECT /*+ REPARTITION(zip_code) */ age , name , zip_code FROM person SORT BY age NULLS LAST ; + ----+--------+--------+ | age | name | zip_code | + ----+--------+--------+ | 18 | Dan Li | 94588 | | 27 | Anil K | 94588 | | 50 | Zen Hui | 94588 | | null | John V | 94588 | | 18 | Aryan B . | 94511 | | 42 | David K | 94511 | | null | Lalit B . | 94511 | + ----+--------+--------+ -- Sort rows by age within each partition in descending manner, which defaults to NULL LAST. SELECT /*+ REPARTITION(zip_code) */ age , name , zip_code FROM person SORT BY age DESC ; + ----+--------+--------+ | age | name | zip_code | + ----+--------+--------+ | 50 | Zen Hui | 94588 | | 27 | Anil K | 94588 | | 18 | Dan Li | 94588 | | null | John V | 94588 | | 42 | David K | 94511 | | 18 | Aryan B . | 94511 | | null | Lalit B . | 94511 | + ----+--------+--------+ -- Sort rows by age within each partition in descending manner keeping null values to be first. SELECT /*+ REPARTITION(zip_code) */ age , name , zip_code FROM person SORT BY age DESC NULLS FIRST ; + ----+--------+--------+ | age | name | zip_code | + ----+--------+--------+ | null | John V | 94588 | | 50 | Zen Hui | 94588 | | 27 | Anil K | 94588 | | 18 | Dan Li | 94588 | | null | Lalit B . | 94511 | | 42 | David K | 94511 | | 18 | Aryan B . | 94511 | + ----+--------+--------+ -- Sort rows within each partition based on more than one column with each column having -- different sort direction. SELECT /*+ REPARTITION(zip_code) */ name , age , zip_code FROM person SORT BY name ASC , age DESC ; + --------+----+--------+ | name | age | zip_code | + --------+----+--------+ | Anil K | 27 | 94588 | | Dan Li | 18 | 94588 | | John V | null | 94588 | | Zen Hui | 50 | 94588 | | Aryan B . | 18 | 94511 | | David K | 42 | 94511 | | Lalit B . | null | 94511 | + --------+----+--------+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause CLUSTER BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-sampling.html
Sampling Queries - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions Sampling Queries Description The TABLESAMPLE statement is used to sample the table. It supports the following sampling methods: TABLESAMPLE (x ROWS ): Sample the table down to the given number of rows. TABLESAMPLE (x PERCENT ): Sample the table down to the given percentage. Note that percentages are defined as a number between 0 and 100. TABLESAMPLE ( BUCKET x OUT OF y): Sample the table down to a x out of y fraction. Note: TABLESAMPLE returns the approximate number of rows or fraction requested. Syntax TABLESAMPLE ( { integer_expression | decimal_expression } PERCENT ) | TABLESAMPLE ( integer_expression ROWS ) | TABLESAMPLE ( BUCKET integer_expression OUT OF integer_expression ) Examples SELECT * FROM test ; + --+----+ | id | name | + --+----+ | 5 | Alex | | 8 | Lucy | | 2 | Mary | | 4 | Fred | | 1 | Lisa | | 9 | Eric | | 10 | Adam | | 6 | Mark | | 7 | Lily | | 3 | Evan | + --+----+ SELECT * FROM test TABLESAMPLE ( 50 PERCENT ); + --+----+ | id | name | + --+----+ | 5 | Alex | | 2 | Mary | | 4 | Fred | | 9 | Eric | | 10 | Adam | | 3 | Evan | + --+----+ SELECT * FROM test TABLESAMPLE ( 5 ROWS ); + --+----+ | id | name | + --+----+ | 5 | Alex | | 8 | Lucy | | 2 | Mary | | 4 | Fred | | 1 | Lisa | + --+----+ SELECT * FROM test TABLESAMPLE ( BUCKET 4 OUT OF 10 ); + --+----+ | id | name | + --+----+ | 8 | Lucy | | 2 | Mary | | 9 | Eric | | 6 | Mark | + --+----+ Related Statements SELECT

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-tvf.html
Table-valued Functions (TVF) - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions Table-valued Functions (TVF) Description A table-valued function (TVF) is a function that returns a relation or a set of rows. There are two types of TVFs in Spark SQL: a TVF that can be specified in a FROM clause, e.g. range; a TVF that can be specified in SELECT/LATERAL VIEW clauses, e.g. explode. Supported Table-valued Functions TVFs that can be specified in a FROM clause: Function Argument Type(s) Description range ( end ) Long Creates a table with a single LongType column named id , containing rows in a range from 0 to end (exclusive) with step value 1. range ( start, end ) Long, Long Creates a table with a single LongType column named id , containing rows in a range from start to end (exclusive) with step value 1. range ( start, end, step ) Long, Long, Long Creates a table with a single LongType column named id , containing rows in a range from start to end (exclusive) with step value. range ( start, end, step, numPartitions ) Long, Long, Long, Int Creates a table with a single LongType column named id , containing rows in a range from start to end (exclusive) with step value, with partition number numPartitions specified. TVFs that can be specified in SELECT/LATERAL VIEW clauses: Function Argument Type(s) Description explode ( expr ) Array/Map Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Unless specified otherwise, uses the default column name col for elements of the array or key and value for the elements of the map. explode_outer ( expr ) Array/Map Separates the elements of array expr into multiple rows, or the elements of map expr into multiple rows and columns. Unless specified otherwise, uses the default column name col for elements of the array or key and value for the elements of the map. inline ( expr ) Expression Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise. inline_outer ( expr ) Expression Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise. posexplode ( expr ) Array/Map Separates the elements of array expr into multiple rows with positions, or the elements of map expr into multiple rows and columns with positions. Unless specified otherwise, uses the column name pos for position, col for elements of the array or key and value for elements of the map. posexplode_outer ( expr ) Array/Map Separates the elements of array expr into multiple rows with positions, or the elements of map expr into multiple rows and columns with positions. Unless specified otherwise, uses the column name pos for position, col for elements of the array or key and value for elements of the map. stack ( n, expr1, …, exprk ) Seq[Expression] Separates expr1, …, exprk into n rows. Uses column names col0, col1, etc. by default unless specified otherwise. json_tuple ( jsonStr, p1, p2, …, pn ) Seq[Expression] Returns a tuple like the function get_json_object , but it takes multiple names. All the input parameters and output column types are string. parse_url ( url, partToExtract[, key] ) Seq[Expression] Extracts a part from a URL. Examples -- range call with end SELECT * FROM range ( 6 + cos ( 3 )); + ---+ | id | + ---+ | 0 | | 1 | | 2 | | 3 | | 4 | + ---+ -- range call with start and end SELECT * FROM range ( 5 , 10 ); + ---+ | id | + ---+ | 5 | | 6 | | 7 | | 8 | | 9 | + ---+ -- range call with numPartitions SELECT * FROM range ( 0 , 10 , 2 , 200 ); + ---+ | id | + ---+ | 0 | | 2 | | 4 | | 6 | | 8 | + ---+ -- range call with a table alias SELECT * FROM range ( 5 , 8 ) AS test ; + ---+ | id | + ---+ | 5 | | 6 | | 7 | + ---+ SELECT explode ( array ( 10 , 20 )); + ---+ | col | + ---+ | 10 | | 20 | + ---+ SELECT inline ( array ( struct ( 1 , 'a' ), struct ( 2 , 'b' ))); + ----+----+ | col1 | col2 | + ----+----+ | 1 | a | | 2 | b | + ----+----+ SELECT posexplode ( array ( 10 , 20 )); + ---+---+ | pos | col | + ---+---+ | 0 | 10 | | 1 | 20 | + ---+---+ SELECT stack ( 2 , 1 , 2 , 3 ); + ----+----+ | col0 | col1 | + ----+----+ | 1 | 2 | | 3 | null | + ----+----+ SELECT json_tuple ( '{"a":1, "b":2}' , 'a' , 'b' ); + ---+---+ | c0 | c1 | + ---+---+ | 1 | 2 | + ---+---+ SELECT parse_url ( 'http://spark.apache.org/path?query=1' , 'HOST' ); + -----------------------------------------------------+ | parse_url ( http : // spark . apache . org / path ? query = 1 , HOST ) | + -----------------------------------------------------+ | spark . apache . org | + -----------------------------------------------------+ -- Use explode in a LATERAL VIEW clause CREATE TABLE test ( c1 INT ); INSERT INTO test VALUES ( 1 ); INSERT INTO test VALUES ( 2 ); SELECT * FROM test LATERAL VIEW explode ( ARRAY ( 3 , 4 )) AS c2 ; + --+--+ | c1 | c2 | + --+--+ | 1 | 3 | | 1 | 4 | | 2 | 3 | | 2 | 4 | + --+--+ Related Statements SELECT LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-where.html
SELECT - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions WHERE clause Description The WHERE clause is used to limit the results of the FROM clause of a query or a subquery based on the specified condition. Syntax WHERE boolean_expression Parameters boolean_expression Specifies any expression that evaluates to a result type boolean . Two or more expressions may be combined together using the logical operators ( AND , OR ). Examples CREATE TABLE person ( id INT , name STRING , age INT ); INSERT INTO person VALUES ( 100 , 'John' , 30 ), ( 200 , 'Mary' , NULL ), ( 300 , 'Mike' , 80 ), ( 400 , 'Dan' , 50 ); -- Comparison operator in `WHERE` clause. SELECT * FROM person WHERE id > 200 ORDER BY id ; + ---+----+---+ | id | name | age | + ---+----+---+ | 300 | Mike | 80 | | 400 | Dan | 50 | + ---+----+---+ -- Comparison and logical operators in `WHERE` clause. SELECT * FROM person WHERE id = 200 OR id = 300 ORDER BY id ; + ---+----+----+ | id | name | age | + ---+----+----+ | 200 | Mary | null | | 300 | Mike | 80 | + ---+----+----+ -- IS NULL expression in `WHERE` clause. SELECT * FROM person WHERE id > 300 OR age IS NULL ORDER BY id ; + ---+----+----+ | id | name | age | + ---+----+----+ | 200 | Mary | null | | 400 | Dan | 50 | + ---+----+----+ -- Function expression in `WHERE` clause. SELECT * FROM person WHERE length ( name ) > 3 ORDER BY id ; + ---+----+----+ | id | name | age | + ---+----+----+ | 100 | John | 30 | | 200 | Mary | null | | 300 | Mike | 80 | + ---+----+----+ -- `BETWEEN` expression in `WHERE` clause. SELECT * FROM person WHERE id BETWEEN 200 AND 300 ORDER BY id ; + ---+----+----+ | id | name | age | + ---+----+----+ | 200 | Mary | null | | 300 | Mike | 80 | + ---+----+----+ -- Scalar Subquery in `WHERE` clause. SELECT * FROM person WHERE age > ( SELECT avg ( age ) FROM person ); + ---+----+---+ | id | name | age | + ---+----+---+ | 300 | Mike | 80 | + ---+----+---+ -- Correlated Subquery in `WHERE` clause. SELECT * FROM person AS parent WHERE EXISTS ( SELECT 1 FROM person AS child WHERE parent . id = child . id AND child . age IS NULL ); + ---+----+----+ | id | name | age | + ---+----+----+ | 200 | Mary | null | + ---+----+----+ Related Statements SELECT Main GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause CLUSTER BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-aggregate.html
Aggregate Functions - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions Aggregate Functions Description Aggregate functions operate on values across rows to perform mathematical calculations such as sum, average, counting, minimum/maximum values, standard deviation, and estimation, as well as some non-mathematical operations. Syntax aggregate_function ( input1 [, input2 , ...]) FILTER ( WHERE boolean_expression ) Parameters aggregate_function Please refer to the Built-in Aggregation Functions document for a complete list of Spark aggregate functions. boolean_expression Specifies any expression that evaluates to a result type boolean. Two or more expressions may be combined together using the logical operators ( AND, OR ). Examples Please refer to the Built-in Aggregation Functions document for all the examples of Spark aggregate functions. Ordered-Set Aggregate Functions These aggregate Functions use different syntax than the other aggregate functions so that to specify an expression (typically a column name) by which to order the values. Syntax { PERCENTILE_CONT | PERCENTILE_DISC } ( percentile ) WITHIN GROUP ( ORDER BY { order_by_expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [ , ... ] } ) FILTER ( WHERE boolean_expression ) Parameters percentile The percentile of the value that you want to find. The percentile must be a constant between 0.0 and 1.0. order_by_expression The expression (typically a column name) by which to order the values before aggregating them. boolean_expression Specifies any expression that evaluates to a result type boolean. Two or more expressions may be combined together using the logical operators ( AND, OR ). Examples CREATE OR REPLACE TEMPORARY VIEW basic_pays AS SELECT * FROM VALUES ( 'Diane Murphy' , 'Accounting' , 8435 ), ( 'Mary Patterson' , 'Accounting' , 9998 ), ( 'Jeff Firrelli' , 'Accounting' , 8992 ), ( 'William Patterson' , 'Accounting' , 8870 ), ( 'Gerard Bondur' , 'Accounting' , 11472 ), ( 'Anthony Bow' , 'Accounting' , 6627 ), ( 'Leslie Jennings' , 'IT' , 8113 ), ( 'Leslie Thompson' , 'IT' , 5186 ), ( 'Julie Firrelli' , 'Sales' , 9181 ), ( 'Steve Patterson' , 'Sales' , 9441 ), ( 'Foon Yue Tseng' , 'Sales' , 6660 ), ( 'George Vanauf' , 'Sales' , 10563 ), ( 'Loui Bondur' , 'SCM' , 10449 ), ( 'Gerard Hernandez' , 'SCM' , 6949 ), ( 'Pamela Castillo' , 'SCM' , 11303 ), ( 'Larry Bott' , 'SCM' , 11798 ), ( 'Barry Jones' , 'SCM' , 10586 ) AS basic_pays ( employee_name , department , salary ); SELECT * FROM basic_pays ; + -----------------+----------+------+ | employee_name | department | salary | + -----------------+----------+------+ | Anthony Bow | Accounting | 6627 | | Barry Jones | SCM | 10586 | | Diane Murphy | Accounting | 8435 | | Foon Yue Tseng | Sales | 6660 | | George Vanauf | Sales | 10563 | | Gerard Bondur | Accounting | 11472 | | Gerard Hernandez | SCM | 6949 | | Jeff Firrelli | Accounting | 8992 | | Julie Firrelli | Sales | 9181 | | Larry Bott | SCM | 11798 | | Leslie Jennings | IT | 8113 | | Leslie Thompson | IT | 5186 | | Loui Bondur | SCM | 10449 | | Mary Patterson | Accounting | 9998 | | Pamela Castillo | SCM | 11303 | | Steve Patterson | Sales | 9441 | | William Patterson | Accounting | 8870 | + -----------------+----------+------+ SELECT department , percentile_cont ( 0 . 25 ) WITHIN GROUP ( ORDER BY salary ) AS pc1 , percentile_cont ( 0 . 25 ) WITHIN GROUP ( ORDER BY salary ) FILTER ( WHERE employee_name LIKE '%Bo%' ) AS pc2 , percentile_cont ( 0 . 25 ) WITHIN GROUP ( ORDER BY salary DESC ) AS pc3 , percentile_cont ( 0 . 25 ) WITHIN GROUP ( ORDER BY salary DESC ) FILTER ( WHERE employee_name LIKE '%Bo%' ) AS pc4 , percentile_disc ( 0 . 25 ) WITHIN GROUP ( ORDER BY salary ) AS pd1 , percentile_disc ( 0 . 25 ) WITHIN GROUP ( ORDER BY salary ) FILTER ( WHERE employee_name LIKE '%Bo%' ) AS pd2 , percentile_disc ( 0 . 25 ) WITHIN GROUP ( ORDER BY salary DESC ) AS pd3 , percentile_disc ( 0 . 25 ) WITHIN GROUP ( ORDER BY salary DESC ) FILTER ( WHERE employee_name LIKE '%Bo%' ) AS pd4 FROM basic_pays GROUP BY department ORDER BY department ; + ----------+-------+--------+-------+--------+-----+-----+-----+-----+ | department | pc1 | pc2 | pc3 | pc4 | pd1 | pd2 | pd3 | pd4 | + ----------+-------+--------+-------+--------+-----+-----+-----+-----+ | Accounting | 8543 . 75 | 7838 . 25 | 9746 . 5 | 10260 . 75 | 8435 | 6627 | 9998 | 11472 | | IT | 5917 . 75 | NULL | 7381 . 25 | NULL | 5186 | NULL | 8113 | NULL | | Sales | 8550 . 75 | NULL | 9721 . 5 | NULL | 6660 | NULL | 10563 | NULL | | SCM | 10449 . 0 | 10786 . 25 | 11303 . 0 | 11460 . 75 | 10449 | 10449 | 11303 | 11798 | + ----------+-------+--------+-------+--------+-----+-----+-----+-----+ Related Statements SELECT

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-window.html
Window Functions - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions Window Functions Description Window functions operate on a group of rows, referred to as a window, and calculate a return value for each row based on the group of rows. Window functions are useful for processing tasks such as calculating a moving average, computing a cumulative statistic, or accessing the value of rows given the relative position of the current row. Syntax window_function [ nulls_option ] OVER ( [ { PARTITION | DISTRIBUTE } BY partition_col_name = partition_col_val ( [ , ... ] ) ] { ORDER | SORT } BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [ , ... ] [ window_frame ] ) Parameters window_function Ranking Functions Syntax: RANK | DENSE_RANK | PERCENT_RANK | NTILE | ROW_NUMBER Analytic Functions Syntax: CUME_DIST | LAG | LEAD | NTH_VALUE | FIRST_VALUE | LAST_VALUE Aggregate Functions Syntax: MAX | MIN | COUNT | SUM | AVG | ... Please refer to the Built-in Aggregation Functions document for a complete list of Spark aggregate functions. nulls_option Specifies whether or not to skip null values when evaluating the window function. RESPECT NULLS means not skipping null values, while IGNORE NULLS means skipping. If not specified, the default is RESPECT NULLS . Syntax: { IGNORE | RESPECT } NULLS Note: Only LAG | LEAD | NTH_VALUE | FIRST_VALUE | LAST_VALUE can be used with IGNORE NULLS . window_frame Specifies which row to start the window on and where to end it. Syntax: { RANGE | ROWS } { frame_start | BETWEEN frame_start AND frame_end } frame_start and frame_end have the following syntax: Syntax: UNBOUNDED PRECEDING | offset PRECEDING | CURRENT ROW | offset FOLLOWING | UNBOUNDED FOLLOWING offset: specifies the offset from the position of the current row. Note: If frame_end is omitted it defaults to CURRENT ROW . Examples CREATE TABLE employees ( name STRING , dept STRING , salary INT , age INT ); INSERT INTO employees VALUES ( "Lisa" , "Sales" , 10000 , 35 ); INSERT INTO employees VALUES ( "Evan" , "Sales" , 32000 , 38 ); INSERT INTO employees VALUES ( "Fred" , "Engineering" , 21000 , 28 ); INSERT INTO employees VALUES ( "Alex" , "Sales" , 30000 , 33 ); INSERT INTO employees VALUES ( "Tom" , "Engineering" , 23000 , 33 ); INSERT INTO employees VALUES ( "Jane" , "Marketing" , 29000 , 28 ); INSERT INTO employees VALUES ( "Jeff" , "Marketing" , 35000 , 38 ); INSERT INTO employees VALUES ( "Paul" , "Engineering" , 29000 , 23 ); INSERT INTO employees VALUES ( "Chloe" , "Engineering" , 23000 , 25 ); SELECT * FROM employees ; + -----+-----------+------+-----+ | name | dept | salary | age | + -----+-----------+------+-----+ | Chloe | Engineering | 23000 | 25 | | Fred | Engineering | 21000 | 28 | | Paul | Engineering | 29000 | 23 | | Helen | Marketing | 29000 | 40 | | Tom | Engineering | 23000 | 33 | | Jane | Marketing | 29000 | 28 | | Jeff | Marketing | 35000 | 38 | | Evan | Sales | 32000 | 38 | | Lisa | Sales | 10000 | 35 | | Alex | Sales | 30000 | 33 | + -----+-----------+------+-----+ SELECT name , dept , salary , RANK () OVER ( PARTITION BY dept ORDER BY salary ) AS rank FROM employees ; + -----+-----------+------+----+ | name | dept | salary | rank | + -----+-----------+------+----+ | Lisa | Sales | 10000 | 1 | | Alex | Sales | 30000 | 2 | | Evan | Sales | 32000 | 3 | | Fred | Engineering | 21000 | 1 | | Tom | Engineering | 23000 | 2 | | Chloe | Engineering | 23000 | 2 | | Paul | Engineering | 29000 | 4 | | Helen | Marketing | 29000 | 1 | | Jane | Marketing | 29000 | 1 | | Jeff | Marketing | 35000 | 3 | + -----+-----------+------+----+ SELECT name , dept , salary , DENSE_RANK () OVER ( PARTITION BY dept ORDER BY salary ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS dense_rank FROM employees ; + -----+-----------+------+----------+ | name | dept | salary | dense_rank | + -----+-----------+------+----------+ | Lisa | Sales | 10000 | 1 | | Alex | Sales | 30000 | 2 | | Evan | Sales | 32000 | 3 | | Fred | Engineering | 21000 | 1 | | Tom | Engineering | 23000 | 2 | | Chloe | Engineering | 23000 | 2 | | Paul | Engineering | 29000 | 3 | | Helen | Marketing | 29000 | 1 | | Jane | Marketing | 29000 | 1 | | Jeff | Marketing | 35000 | 2 | + -----+-----------+------+----------+ SELECT name , dept , age , CUME_DIST () OVER ( PARTITION BY dept ORDER BY age RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS cume_dist FROM employees ; + -----+-----------+------+------------------+ | name | dept | age | cume_dist | + -----+-----------+------+------------------+ | Alex | Sales | 33 | 0 . 3333333333333333 | | Lisa | Sales | 35 | 0 . 6666666666666666 | | Evan | Sales | 38 | 1 . 0 | | Paul | Engineering | 23 | 0 . 25 | | Chloe | Engineering | 25 | 0 . 75 | | Fred | Engineering | 28 | 0 . 25 | | Tom | Engineering | 33 | 1 . 0 | | Jane | Marketing | 28 | 0 . 3333333333333333 | | Jeff | Marketing | 38 | 0 . 6666666666666666 | | Helen | Marketing | 40 | 1 . 0 | + -----+-----------+------+------------------+ SELECT name , dept , salary , MIN ( salary ) OVER ( PARTITION BY dept ORDER BY salary ) AS min FROM employees ; + -----+-----------+------+-----+ | name | dept | salary | min | + -----+-----------+------+-----+ | Lisa | Sales | 10000 | 10000 | | Alex | Sales | 30000 | 10000 | | Evan | Sales | 32000 | 10000 | | Helen | Marketing | 29000 | 29000 | | Jane | Marketing | 29000 | 29000 | | Jeff | Marketing | 35000 | 29000 | | Fred | Engineering | 21000 | 21000 | | Tom | Engineering | 23000 | 21000 | | Chloe | Engineering | 23000 | 21000 | | Paul | Engineering | 29000 | 21000 | + -----+-----------+------+-----+ SELECT name , salary , LAG ( salary ) OVER ( PARTITION BY dept ORDER BY salary ) AS lag , LEAD ( salary , 1 , 0 ) OVER ( PARTITION BY dept ORDER BY salary ) AS lead FROM employees ; + -----+-----------+------+-----+-----+ | name | dept | salary | lag | lead | + -----+-----------+------+-----+-----+ | Lisa | Sales | 10000 | NULL | 30000 | | Alex | Sales | 30000 | 10000 | 32000 | | Evan | Sales | 32000 | 30000 | 0 | | Fred | Engineering | 21000 | NULL | 23000 | | Chloe | Engineering | 23000 | 21000 | 23000 | | Tom | Engineering | 23000 | 23000 | 29000 | | Paul | Engineering | 29000 | 23000 | 0 | | Helen | Marketing | 29000 | NULL | 29000 | | Jane | Marketing | 29000 | 29000 | 35000 | | Jeff | Marketing | 35000 | 29000 | 0 | + -----+-----------+------+-----+-----+ SELECT id , v , LEAD ( v , 0 ) IGNORE NULLS OVER w lead , LAG ( v , 0 ) IGNORE NULLS OVER w lag , NTH_VALUE ( v , 2 ) IGNORE NULLS OVER w nth_value , FIRST_VALUE ( v ) IGNORE NULLS OVER w first_value , LAST_VALUE ( v ) IGNORE NULLS OVER w last_value FROM test_ignore_null WINDOW w AS ( ORDER BY id ) ORDER BY id ; + --+----+----+----+---------+-----------+----------+ | id | v | lead | lag | nth_value | first_value | last_value | + --+----+----+----+---------+-----------+----------+ | 0 | NULL | NULL | NULL | NULL | NULL | NULL | | 1 | x | x | x | NULL | x | x | | 2 | NULL | NULL | NULL | NULL | x | x | | 3 | NULL | NULL | NULL | NULL | x | x | | 4 | y | y | y | y | x | y | | 5 | NULL | NULL | NULL | y | x | y | | 6 | z | z | z | y | x | z | | 7 | v | v | v | y | x | v | | 8 | NULL | NULL | NULL | y | x | v | + --+----+----+----+---------+-----------+----------+ Related Statements SELECT

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-case.html
CASE Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions CASE Clause Description CASE clause uses a rule to return a specific result based on the specified condition, similar to if/else statements in other programming languages. Syntax CASE [ expression ] { WHEN boolean_expression THEN then_expression } [ ... ] [ ELSE else_expression ] END Parameters boolean_expression Specifies any expression that evaluates to a result type boolean . Two or more expressions may be combined together using the logical operators ( AND , OR ). then_expression Specifies the then expression based on the boolean_expression condition; then_expression and else_expression should all be same type or coercible to a common type. else_expression Specifies the default expression; then_expression and else_expression should all be same type or coercible to a common type. Examples CREATE TABLE person ( id INT , name STRING , age INT ); INSERT INTO person VALUES ( 100 , 'John' , 30 ), ( 200 , 'Mary' , NULL ), ( 300 , 'Mike' , 80 ), ( 400 , 'Dan' , 50 ); SELECT id , CASE WHEN id > 200 THEN 'bigger' ELSE 'small' END FROM person ; + ------+--------------------------------------------------+ | id | CASE WHEN ( id > 200 ) THEN bigger ELSE small END | + ------+--------------------------------------------------+ | 100 | small | | 200 | small | | 300 | bigger | | 400 | bigger | + ------+--------------------------------------------------+ SELECT id , CASE id WHEN 100 then 'bigger' WHEN id > 300 THEN '300' ELSE 'small' END FROM person ; + ------+-----------------------------------------------------------------------------------------------+ | id | CASE WHEN ( id = 100 ) THEN bigger WHEN ( id = CAST (( id > 300 ) AS INT )) THEN 300 ELSE small END | + ------+-----------------------------------------------------------------------------------------------+ | 100 | bigger | | 200 | small | | 300 | small | | 400 | small | + ------+-----------------------------------------------------------------------------------------------+ SELECT * FROM person WHERE CASE 1 = 1 WHEN 100 THEN 'big' WHEN 200 THEN 'bigger' WHEN 300 THEN 'biggest' ELSE 'small' END = 'small' ; + ------+-------+-------+ | id | name | age | + ------+-------+-------+ | 100 | John | 30 | | 200 | Mary | NULL | | 300 | Mike | 80 | | 400 | Dan | 50 | + ------+-------+-------+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-pivot.html
PIVOT Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions PIVOT Clause Description The PIVOT clause is used for data perspective. We can get the aggregated values based on specific column values, which will be turned to multiple columns used in SELECT clause. The PIVOT clause can be specified after the table name or subquery. Syntax PIVOT ( { aggregate_expression [ AS aggregate_expression_alias ] } [ , ... ] FOR column_list IN ( expression_list ) ) Parameters aggregate_expression Specifies an aggregate expression (SUM(a), COUNT(DISTINCT b), etc.). aggregate_expression_alias Specifies an alias for the aggregate expression. column_list Contains columns in the FROM clause, which specifies the columns we want to replace with new columns. We can use brackets to surround the columns, such as (c1, c2) . expression_list Specifies new columns, which are used to match values in column_list as the aggregating condition. We can also add aliases for them. Examples CREATE TABLE person ( id INT , name STRING , age INT , class INT , address STRING ); INSERT INTO person VALUES ( 100 , 'John' , 30 , 1 , 'Street 1' ), ( 200 , 'Mary' , NULL , 1 , 'Street 2' ), ( 300 , 'Mike' , 80 , 3 , 'Street 3' ), ( 400 , 'Dan' , 50 , 4 , 'Street 4' ); SELECT * FROM person PIVOT ( SUM ( age ) AS a , AVG ( class ) AS c FOR name IN ( 'John' AS john , 'Mike' AS mike ) ); + ------+-----------+---------+---------+---------+---------+ | id | address | john_a | john_c | mike_a | mike_c | + ------+-----------+---------+---------+---------+---------+ | 200 | Street 2 | NULL | NULL | NULL | NULL | | 100 | Street 1 | 30 | 1 . 0 | NULL | NULL | | 300 | Street 3 | NULL | NULL | 80 | 3 . 0 | | 400 | Street 4 | NULL | NULL | NULL | NULL | + ------+-----------+---------+---------+---------+---------+ SELECT * FROM person PIVOT ( SUM ( age ) AS a , AVG ( class ) AS c FOR ( name , age ) IN (( 'John' , 30 ) AS c1 , ( 'Mike' , 40 ) AS c2 ) ); + ------+-----------+-------+-------+-------+-------+ | id | address | c1_a | c1_c | c2_a | c2_c | + ------+-----------+-------+-------+-------+-------+ | 200 | Street 2 | NULL | NULL | NULL | NULL | | 100 | Street 1 | 30 | 1 . 0 | NULL | NULL | | 300 | Street 3 | NULL | NULL | NULL | NULL | | 400 | Street 4 | NULL | NULL | NULL | NULL | + ------+-----------+-------+-------+-------+-------+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-unpivot.html
UNPIVOT Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions UNPIVOT Clause Description The UNPIVOT clause transforms multiple columns into multiple rows used in SELECT clause. The UNPIVOT clause can be specified after the table name or subquery. Syntax UNPIVOT [ { INCLUDE | EXCLUDE } NULLS ] ( { single_value_column_unpivot | multi_value_column_unpivot } ) [[ AS ] alias ] single_value_column_unpivot : values_column FOR name_column IN ( unpivot_column [[ AS ] alias ] [, ...]) multi_value_column_unpivot : ( values_column [, ...]) FOR name_column IN (( unpivot_column [, ...]) [[ AS ] alias ] [, ...]) Parameters unpivot_column Contains columns in the FROM clause, which specifies the columns we want to unpivot. name_column The name for the column that holds the names of the unpivoted columns. values_column The name for the column that holds the values of the unpivoted columns. Examples CREATE TABLE sales_quarterly ( year INT , q1 INT , q2 INT , q3 INT , q4 INT ); INSERT INTO sales_quarterly VALUES ( 2020 , null , 1000 , 2000 , 2500 ), ( 2021 , 2250 , 3200 , 4200 , 5900 ), ( 2022 , 4200 , 3100 , null , null ); -- column names are used as unpivot columns SELECT * FROM sales_quarterly UNPIVOT ( sales FOR quarter IN ( q1 , q2 , q3 , q4 ) ); + ------+---------+-------+ | year | quarter | sales | + ------+---------+-------+ | 2020 | q2 | 1000 | | 2020 | q3 | 2000 | | 2020 | q4 | 2500 | | 2021 | q1 | 2250 | | 2021 | q2 | 3200 | | 2021 | q3 | 4200 | | 2021 | q4 | 5900 | | 2022 | q1 | 4200 | | 2022 | q2 | 3100 | + ------+---------+-------+ -- NULL values are excluded by default, they can be included -- unpivot columns can be alias -- unpivot result can be referenced via its alias SELECT up . * FROM sales_quarterly UNPIVOT INCLUDE NULLS ( sales FOR quarter IN ( q1 AS Q1 , q2 AS Q2 , q3 AS Q3 , q4 AS Q4 ) ) AS up ; + ------+---------+-------+ | year | quarter | sales | + ------+---------+-------+ | 2020 | Q1 | NULL | | 2020 | Q2 | 1000 | | 2020 | Q3 | 2000 | | 2020 | Q4 | 2500 | | 2021 | Q1 | 2250 | | 2021 | Q2 | 3200 | | 2021 | Q3 | 4200 | | 2021 | Q4 | 5900 | | 2022 | Q1 | 4200 | | 2022 | Q2 | 3100 | | 2022 | Q3 | NULL | | 2022 | Q4 | NULL | + ------+---------+-------+ -- multiple value columns can be unpivoted per row SELECT * FROM sales_quarterly UNPIVOT EXCLUDE NULLS ( ( first_quarter , second_quarter ) FOR half_of_the_year IN ( ( q1 , q2 ) AS H1 , ( q3 , q4 ) AS H2 ) ); + ------+------------------+---------------+----------------+ | id | half_of_the_year | first_quarter | second_quarter | + ------+------------------+---------------+----------------+ | 2020 | H1 | NULL | 1000 | | 2020 | H2 | 2000 | 2500 | | 2021 | H1 | 2250 | 3200 | | 2021 | H2 | 4200 | 5900 | | 2022 | H1 | 4200 | 3100 | + ------+------------------+---------------+----------------+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-lateral-view.html
LATERAL VIEW Clause - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions LATERAL VIEW Clause Description The LATERAL VIEW clause is used in conjunction with generator functions such as EXPLODE , which will generate a virtual table containing one or more rows. LATERAL VIEW will apply the rows to each original output row. Syntax LATERAL VIEW [ OUTER ] generator_function ( expression [ , ... ] ) [ table_alias ] AS column_alias [ , ... ] Parameters OUTER If OUTER specified, returns null if an input array/map is empty or null. generator_function Specifies a generator function (EXPLODE, INLINE, etc.). table_alias The alias for generator_function , which is optional. column_alias Lists the column aliases of generator_function , which may be used in output rows. We may have multiple aliases if generator_function have multiple output columns. Examples CREATE TABLE person ( id INT , name STRING , age INT , class INT , address STRING ); INSERT INTO person VALUES ( 100 , 'John' , 30 , 1 , 'Street 1' ), ( 200 , 'Mary' , NULL , 1 , 'Street 2' ), ( 300 , 'Mike' , 80 , 3 , 'Street 3' ), ( 400 , 'Dan' , 50 , 4 , 'Street 4' ); SELECT * FROM person LATERAL VIEW EXPLODE ( ARRAY ( 30 , 60 )) tableName AS c_age LATERAL VIEW EXPLODE ( ARRAY ( 40 , 80 )) AS d_age ; + ------+-------+-------+--------+-----------+--------+--------+ | id | name | age | class | address | c_age | d_age | + ------+-------+-------+--------+-----------+--------+--------+ | 100 | John | 30 | 1 | Street 1 | 30 | 40 | | 100 | John | 30 | 1 | Street 1 | 30 | 80 | | 100 | John | 30 | 1 | Street 1 | 60 | 40 | | 100 | John | 30 | 1 | Street 1 | 60 | 80 | | 200 | Mary | NULL | 1 | Street 2 | 30 | 40 | | 200 | Mary | NULL | 1 | Street 2 | 30 | 80 | | 200 | Mary | NULL | 1 | Street 2 | 60 | 40 | | 200 | Mary | NULL | 1 | Street 2 | 60 | 80 | | 300 | Mike | 80 | 3 | Street 3 | 30 | 40 | | 300 | Mike | 80 | 3 | Street 3 | 30 | 80 | | 300 | Mike | 80 | 3 | Street 3 | 60 | 40 | | 300 | Mike | 80 | 3 | Street 3 | 60 | 80 | | 400 | Dan | 50 | 4 | Street 4 | 30 | 40 | | 400 | Dan | 50 | 4 | Street 4 | 30 | 80 | | 400 | Dan | 50 | 4 | Street 4 | 60 | 40 | | 400 | Dan | 50 | 4 | Street 4 | 60 | 80 | + ------+-------+-------+--------+-----------+--------+--------+ SELECT c_age , COUNT ( 1 ) FROM person LATERAL VIEW EXPLODE ( ARRAY ( 30 , 60 )) AS c_age LATERAL VIEW EXPLODE ( ARRAY ( 40 , 80 )) AS d_age GROUP BY c_age ; + --------+-----------+ | c_age | count ( 1 ) | + --------+-----------+ | 60 | 8 | | 30 | 8 | + --------+-----------+ SELECT * FROM person LATERAL VIEW EXPLODE ( ARRAY ()) tableName AS c_age ; + -----+-------+------+--------+----------+--------+ | id | name | age | class | address | c_age | + -----+-------+------+--------+----------+--------+ + -----+-------+------+--------+----------+--------+ SELECT * FROM person LATERAL VIEW OUTER EXPLODE ( ARRAY ()) tableName AS c_age ; + ------+-------+-------+--------+-----------+--------+ | id | name | age | class | address | c_age | + ------+-------+-------+--------+-----------+--------+ | 100 | John | 30 | 1 | Street 1 | NULL | | 200 | Mary | NULL | 1 | Street 2 | NULL | | 300 | Mike | 80 | 3 | Street 3 | NULL | | 400 | Dan | 50 | 4 | Street 4 | NULL | + ------+-------+-------+--------+-----------+--------+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-lateral-subquery.html
LATERAL SUBQUERY - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions LATERAL SUBQUERY Description LATERAL SUBQUERY is a subquery that is preceded by the keyword LATERAL . It provides a way to reference columns in the preceding FROM clause. Without the LATERAL keyword, subqueries can only refer to columns in the outer query, but not in the FROM clause. LATERAL SUBQUERY makes the complicated queries simpler and more efficient. Syntax [ LATERAL ] primary_relation [ join_relation ] Parameters primary_relation Specifies the primary relation. It can be one of the following: Table relation Aliased query Syntax: ( query ) [ [ AS ] alias ] Aliased relation Syntax: ( relation ) [ [ AS ] alias ] Table-value function Inline table join_relation Specifies a Join relation . Examples CREATE TABLE t1 ( c1 INT , c2 INT ); INSERT INTO t1 VALUES ( 0 , 1 ), ( 1 , 2 ); CREATE TABLE t2 ( c1 INT , c2 INT ); INSERT INTO t2 VALUES ( 0 , 2 ), ( 0 , 3 ); SELECT * FROM t1 , LATERAL ( SELECT * FROM t2 WHERE t1 . c1 = t2 . c1 ); + --------+-------+--------+-------+ | t1 . c1 | t1 . c2 | t2 . c1 | t2 . c2 | + -------+--------+--------+-------+ | 0 | 1 | 0 | 3 | | 0 | 1 | 0 | 2 | + -------+--------+--------+-------+ SELECT a , b , c FROM t1 , LATERAL ( SELECT c1 + c2 AS a ), LATERAL ( SELECT c1 - c2 AS b ), LATERAL ( SELECT a * b AS c ); + --------+-------+--------+ | a | b | c | + -------+--------+--------+ | 3 | - 1 | - 3 | | 1 | - 1 | - 1 | + -------+--------+--------+ Related Statements SELECT Main JOIN

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-qry-select-transform.html
TRANSFORM - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions TRANSFORM Description The TRANSFORM clause is used to specify a Hive-style transform query specification to transform the inputs by running a user-specified command or script. Spark’s script transform supports two modes: Hive support disabled: Spark script transform can run with spark.sql.catalogImplementation=in-memory or without SparkSession.builder.enableHiveSupport() . In this case, now Spark only uses the script transform with ROW FORMAT DELIMITED and treats all values passed to the script as strings. Hive support enabled: When Spark is run with spark.sql.catalogImplementation=hive or Spark SQL is started with SparkSession.builder.enableHiveSupport() , Spark can use the script transform with both Hive SerDe and ROW FORMAT DELIMITED . Syntax SELECT TRANSFORM ( expression [ , ... ] ) [ ROW FORMAT row_format ] [ RECORDWRITER record_writer_class ] USING command_or_script [ AS ( [ col_name [ col_type ] ] [ , ... ] ) ] [ ROW FORMAT row_format ] [ RECORDREADER record_reader_class ] Parameters expression Specifies a combination of one or more values, operators and SQL functions that results in a value. row_format Specifies the row format for input and output. See HIVE FORMAT for more syntax details. RECORDWRITER Specifies a fully-qualified class name of a custom RecordWriter. The default value is org.apache.hadoop.hive.ql.exec.TextRecordWriter . RECORDREADER Specifies a fully-qualified class name of a custom RecordReader. The default value is org.apache.hadoop.hive.ql.exec.TextRecordReader . command_or_script Specifies a command or a path to script to process data. ROW FORMAT DELIMITED BEHAVIOR When Spark uses ROW FORMAT DELIMITED format: Spark uses the character \u0001 as the default field delimiter and this delimiter can be overridden by FIELDS TERMINATED BY . Spark uses the character \n as the default line delimiter and this delimiter can be overridden by LINES TERMINATED BY . Spark uses a string \N as the default NULL value in order to differentiate NULL values from the literal string NULL . This delimiter can be overridden by NULL DEFINED AS . Spark casts all columns to STRING and combines columns by tabs before feeding to the user script. For complex types such as ARRAY / MAP / STRUCT , Spark uses to_json casts it to an input JSON string and uses from_json to convert the result output JSON string to ARRAY / MAP / STRUCT data. COLLECTION ITEMS TERMINATED BY and MAP KEYS TERMINATED BY are delimiters to split complex data such as ARRAY / MAP / STRUCT , Spark uses to_json and from_json to handle complex data types with JSON format. So COLLECTION ITEMS TERMINATED BY and MAP KEYS TERMINATED BY won’t work in default row format. The standard output of the user script is treated as tab-separated STRING columns. Any cell containing only a string \N is re-interpreted as a literal NULL value, and then the resulting STRING column will be cast to the data types specified in col_type . If the actual number of output columns is less than the number of specified output columns, additional output columns will be filled with NULL . For example: output tabs: 1, 2 output columns: A: INT, B INT, C: INT result: +---+---+------+ | a| b| c| +---+---+------+ | 1| 2| NULL| +---+---+------+ If the actual number of output columns is more than the number of specified output columns, the output columns only select the corresponding columns, and the remaining part will be discarded. For example, if the output has three tabs and there are only two output columns: output tabs: 1, 2, 3 output columns: A: INT, B INT result: +---+---+ | a| b| +---+---+ | 1| 2| +---+---+ If there is no AS clause after USING my_script , the output schema is key: STRING, value: STRING . The key column contains all the characters before the first tab and the value column contains the remaining characters after the first tab. If there are no tabs, Spark returns the NULL value. For example: output tabs: 1, 2, 3 output columns: result: +-----+-------+ | key| value| +-----+-------+ | 1| 2| +-----+-------+ output tabs: 1, 2 output columns: result: +-----+-------+ | key| value| +-----+-------+ | 1| NULL| +-----+-------+ Hive SerDe behavior When Hive support is enabled and Hive SerDe mode is used: Spark uses the Hive SerDe org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe by default, so columns are cast to STRING and combined by tabs before feeding to the user script. All literal NULL values are converted to a string \N in order to differentiate literal NULL values from the literal string NULL . The standard output of the user script is treated as tab-separated STRING columns, any cell containing only a string \N is re-interpreted as a NULL value, and then the resulting STRING column will be cast to the data type specified in col_type . If the actual number of output columns is less than the number of specified output columns, additional output columns will be filled with NULL . If the actual number of output columns is more than the number of specified output columns, the output columns only select the corresponding columns, and the remaining part will be discarded. If there is no AS clause after USING my_script , the output schema is key: STRING, value: STRING . The key column contains all the characters before the first tab and the value column contains the remaining characters after the first tab. If there is no tab, Spark returns the NULL value. These defaults can be overridden with ROW FORMAT SERDE or ROW FORMAT DELIMITED . Examples CREATE TABLE person ( zip_code INT , name STRING , age INT ); INSERT INTO person VALUES ( 94588 , 'Zen Hui' , 50 ), ( 94588 , 'Dan Li' , 18 ), ( 94588 , 'Anil K' , 27 ), ( 94588 , 'John V' , NULL ), ( 94511 , 'David K' , 42 ), ( 94511 , 'Aryan B.' , 18 ), ( 94511 , 'Lalit B.' , NULL ); -- With specified output without data type SELECT TRANSFORM ( zip_code , name , age ) USING 'cat' AS ( a , b , c ) FROM person WHERE zip_code > 94511 ; + -------+---------+-----+ | a | b | c | + -------+---------+-----+ | 94588 | Anil K | 27 | | 94588 | John V | NULL | | 94588 | Zen Hui | 50 | | 94588 | Dan Li | 18 | + -------+---------+-----+ -- With specified output with data type SELECT TRANSFORM ( zip_code , name , age ) USING 'cat' AS ( a STRING , b STRING , c STRING ) FROM person WHERE zip_code > 94511 ; + -------+---------+-----+ | a | b | c | + -------+---------+-----+ | 94588 | Anil K | 27 | | 94588 | John V | NULL | | 94588 | Zen Hui | 50 | | 94588 | Dan Li | 18 | + -------+---------+-----+ -- Using ROW FORMAT DELIMITED SELECT TRANSFORM ( name , age ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY ' \n ' NULL DEFINED AS 'NULL' USING 'cat' AS ( name_age string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '@' LINES TERMINATED BY ' \n ' NULL DEFINED AS 'NULL' FROM person ; + ---------------+ | name_age | + ---------------+ | Anil K , 27 | | John V , null | | ryan B ., 18 | | David K , 42 | | Zen Hui , 50 | | Dan Li , 18 | | Lalit B ., null | + ---------------+ -- Using Hive Serde SELECT TRANSFORM ( zip_code , name , age ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ( 'field.delim' = ' \t ' ) USING 'cat' AS ( a STRING , b STRING , c STRING ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ( 'field.delim' = ' \t ' ) FROM person WHERE zip_code > 94511 ; + -------+---------+-----+ | a | b | c | + -------+---------+-----+ | 94588 | Anil K | 27 | | 94588 | John V | NULL | | 94588 | Zen Hui | 50 | | 94588 | Dan Li | 18 | + -------+---------+-----+ -- Schema-less mode SELECT TRANSFORM ( zip_code , name , age ) USING 'cat' FROM person WHERE zip_code > 94500 ; + -------+---------------------+ | key | value | + -------+---------------------+ | 94588 | Anil K 27 | | 94588 | John V \ N | | 94511 | Aryan B . 18 | | 94511 | David K 42 | | 94588 | Zen Hui 50 | | 94588 | Dan Li 18 | | 94511 | Lalit B . \ N | + -------+---------------------+ Related Statements SELECT Main WHERE Clause GROUP BY Clause HAVING Clause ORDER BY Clause SORT BY Clause DISTRIBUTE BY Clause LIMIT Clause OFFSET Clause CASE Clause PIVOT Clause UNPIVOT Clause LATERAL VIEW Clause

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-resource-mgmt-add-file.html
ADD FILE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions ADD FILE Description ADD FILE can be used to add a single file as well as a directory to the list of resources. The added resource can be listed using LIST FILE . Syntax ADD { FILE | FILES } resource_name [ ... ] Parameters resource_name The name of the file or directory to be added. Examples ADD FILE / tmp / test ; ADD FILE "/path/to/file/abc.txt" ; ADD FILE '/another/test.txt' ; ADD FILE "/path with space/abc.txt" ; ADD FILE "/path/to/some/directory" ; ADD FILES "/path with space/cde.txt" '/path with space/fgh.txt' ; Related Statements LIST FILE LIST JAR LIST ARCHIVE ADD JAR ADD ARCHIVE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-resource-mgmt-add-jar.html
ADD JAR - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions ADD JAR Description ADD JAR adds a JAR file to the list of resources. The added JAR file can be listed using LIST JAR . Syntax ADD { JAR | JARS } file_name [ ... ] Parameters file_name The name of the JAR file to be added. It could be either on a local file system or a distributed file system or an Ivy URI. Apache Ivy is a popular dependency manager focusing on flexibility and simplicity. Now we support two parameter in URI query string: transitive: whether to download dependent jars related to your ivy URL. The parameter name is case-sensitive, and the parameter value is case-insensitive. If multiple transitive parameters are specified, the last one wins. exclude: exclusion list during downloading Ivy URI jar and dependent jars. User can write Ivy URI such as: ivy://group:module:version ivy://group:module:version?transitive=[true|false] ivy://group:module:version?transitive=[true|false]&exclude=group:module,group:module Examples ADD JAR / tmp / test . jar ; ADD JAR "/path/to/some.jar" ; ADD JAR '/some/other.jar' ; ADD JAR "/path with space/abc.jar" ; ADD JARS "/path with space/def.jar" '/path with space/ghi.jar' ; ADD JAR "ivy://group:module:version" ; ADD JAR "ivy://group:module:version?transitive=false" ADD JAR "ivy://group:module:version?transitive=true" ADD JAR "ivy://group:module:version?exclude=group:module&transitive=true" Related Statements LIST JAR ADD FILE LIST FILE ADD ARCHIVE LIST ARCHIVE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-analyze-table.html
ANALYZE TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions ANALYZE TABLE Description The ANALYZE TABLE statement collects statistics about one specific table or all the tables in one specified database, that are to be used by the query optimizer to find a better query execution plan. Syntax ANALYZE TABLE table_identifier [ partition_spec ] COMPUTE STATISTICS [ NOSCAN | FOR COLUMNS col [ , ... ] | FOR ALL COLUMNS ] ANALYZE TABLES [ { FROM | IN } database_name ] COMPUTE STATISTICS [ NOSCAN ] Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec An optional parameter that specifies a comma separated list of key and value pairs for partitions. When specified, partition statistics is returned. Syntax: PARTITION ( partition_col_name [ = partition_col_val ] [ , ... ] ) { FROM | IN } database_name Specifies the name of the database to be analyzed. Without a database name, ANALYZE collects all tables in the current database that the current user has permission to analyze. NOSCAN Collects only the table’s size in bytes (which does not require scanning the entire table). FOR COLUMNS col [ , … ] | FOR ALL COLUMNS Collects column statistics for each column specified, or alternatively for every column, as well as table statistics. If no analyze option is specified, both number of rows and size in bytes are collected. Examples CREATE DATABASE school_db ; USE school_db ; CREATE TABLE teachers ( name STRING , teacher_id INT ); INSERT INTO teachers VALUES ( 'Tom' , 1 ), ( 'Jerry' , 2 ); CREATE TABLE students ( name STRING , student_id INT ) PARTITIONED BY ( student_id ); INSERT INTO students VALUES ( 'Mark' , 111111 ), ( 'John' , 222222 ); ANALYZE TABLE students COMPUTE STATISTICS NOSCAN ; DESC EXTENDED students ; + --------------------+--------------------+-------+ | col_name | data_type | comment | + --------------------+--------------------+-------+ | name | string | null | | student_id | int | null | | ... | ... | ... | | Statistics | 864 bytes | | | ... | ... | ... | + --------------------+--------------------+-------+ ANALYZE TABLE students COMPUTE STATISTICS ; DESC EXTENDED students ; + --------------------+--------------------+-------+ | col_name | data_type | comment | + --------------------+--------------------+-------+ | name | string | null | | student_id | int | null | | ... | ... | ... | | Statistics | 864 bytes , 2 rows | | | ... | ... | ... | + --------------------+--------------------+-------+ ANALYZE TABLE students PARTITION ( student_id = 111111 ) COMPUTE STATISTICS ; DESC EXTENDED students PARTITION ( student_id = 111111 ); + --------------------+--------------------+-------+ | col_name | data_type | comment | + --------------------+--------------------+-------+ | name | string | null | | student_id | int | null | | ... | ... | ... | | Partition Statistics | 432 bytes , 1 rows | | | ... | ... | ... | + --------------------+--------------------+-------+ ANALYZE TABLE students COMPUTE STATISTICS FOR COLUMNS name ; DESC EXTENDED students name ; + --------------+----------+ | info_name | info_value | + --------------+----------+ | col_name | name | | data_type | string | | comment | NULL | | min | NULL | | max | NULL | | num_nulls | 0 | | distinct_count | 2 | | avg_col_len | 4 | | max_col_len | 4 | | histogram | NULL | + --------------+----------+ ANALYZE TABLES IN school_db COMPUTE STATISTICS NOSCAN ; DESC EXTENDED teachers ; + --------------------+--------------------+-------+ | col_name | data_type | comment | + --------------------+--------------------+-------+ | name | string | null | | teacher_id | int | null | | ... | ... | ... | | Statistics | 1382 bytes | | | ... | ... | ... | + --------------------+--------------------+-------+ DESC EXTENDED students ; + --------------------+--------------------+-------+ | col_name | data_type | comment | + --------------------+--------------------+-------+ | name | string | null | | student_id | int | null | | ... | ... | ... | | Statistics | 864 bytes | | | ... | ... | ... | + --------------------+--------------------+-------+ ANALYZE TABLES COMPUTE STATISTICS ; DESC EXTENDED teachers ; + --------------------+--------------------+-------+ | col_name | data_type | comment | + --------------------+--------------------+-------+ | name | string | null | | teacher_id | int | null | | ... | ... | ... | | Statistics | 1382 bytes , 2 rows | | | ... | ... | ... | + --------------------+--------------------+-------+ DESC EXTENDED students ; + --------------------+--------------------+-------+ | col_name | data_type | comment | + --------------------+--------------------+-------+ | name | string | null | | student_id | int | null | | ... | ... | ... | | Statistics | 864 bytes , 2 rows | | | ... | ... | ... | + --------------------+--------------------+-------+

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-cache-cache-table.html
CACHE TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions CACHE TABLE Description CACHE TABLE statement caches contents of a table or output of a query with the given storage level. If a query is cached, then a temp view will be created for this query. This reduces scanning of the original files in future queries. Syntax CACHE [ LAZY ] TABLE table_identifier [ OPTIONS ( 'storageLevel' [ = ] value ) ] [ [ AS ] query ] Parameters LAZY Only cache the table when it is first used, instead of immediately. table_identifier Specifies the table or view name to be cached. The table or view name may be optionally qualified with a database name. Syntax: [ database_name. ] table_name OPTIONS ( ‘storageLevel’ [ = ] value ) OPTIONS clause with storageLevel key and value pair. A Warning is issued when a key other than storageLevel is used. The valid options for storageLevel are: NONE DISK_ONLY DISK_ONLY_2 DISK_ONLY_3 MEMORY_ONLY MEMORY_ONLY_2 MEMORY_ONLY_SER MEMORY_ONLY_SER_2 MEMORY_AND_DISK MEMORY_AND_DISK_2 MEMORY_AND_DISK_SER MEMORY_AND_DISK_SER_2 OFF_HEAP An Exception is thrown when an invalid value is set for storageLevel . If storageLevel is not explicitly set using OPTIONS clause, the default storageLevel is set to MEMORY_AND_DISK . query A query that produces the rows to be cached. It can be in one of following formats: a SELECT statement a TABLE statement a FROM statement Examples CACHE TABLE testCache OPTIONS ( 'storageLevel' 'DISK_ONLY' ) SELECT * FROM testData ; Related Statements CLEAR CACHE UNCACHE TABLE REFRESH TABLE REFRESH REFRESH FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-cache-clear-cache.html
CLEAR CACHE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions CLEAR CACHE Description CLEAR CACHE removes the entries and associated data from the in-memory and/or on-disk cache for all cached tables and views. Syntax CLEAR CACHE Examples CLEAR CACHE ; Related Statements CACHE TABLE UNCACHE TABLE REFRESH TABLE REFRESH REFRESH FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-describe-database.html
DESCRIBE DATABASE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DESCRIBE DATABASE Description â DESCRIBE DATABASE statement returns the metadata of an existing database. The metadata information includes database name, database comment, and database location on the filesystem. If the optional EXTENDED option is specified, it returns the basic metadata information along with the database properties. The DATABASE and SCHEMA are interchangeable. Syntax { DESC | DESCRIBE } DATABASE [ EXTENDED ] db_name Parameters db_name Specifies a name of an existing database or an existing schema in the system. If the name does not exist, an exception is thrown. Examples -- Create employees DATABASE CREATE DATABASE employees COMMENT 'For software companies' ; -- Describe employees DATABASE. -- Returns Database Name, Description and Root location of the filesystem -- for the employees DATABASE. DESCRIBE DATABASE employees ; + -------------------------+-----------------------------+ | database_description_item | database_description_value | + -------------------------+-----------------------------+ | Database Name | employees | | Description | For software companies | | Location | file : / Users / Temp / employees . db | + -------------------------+-----------------------------+ -- Create employees DATABASE CREATE DATABASE employees COMMENT 'For software companies' ; -- Alter employees database to set DBPROPERTIES ALTER DATABASE employees SET DBPROPERTIES ( 'Create-by' = 'Kevin' , 'Create-date' = '09/01/2019' ); -- Describe employees DATABASE with EXTENDED option to return additional database properties DESCRIBE DATABASE EXTENDED employees ; + -------------------------+---------------------------------------------+ | database_description_item | database_description_value | + -------------------------+---------------------------------------------+ | Database Name | employees | | Description | For software companies | | Location | file : / Users / Temp / employees . db | | Properties | (( Create - by , kevin ), ( Create - date , 09 / 01 / 2019 )) | + -------------------------+---------------------------------------------+ -- Create deployment SCHEMA CREATE SCHEMA deployment COMMENT 'Deployment environment' ; -- Describe deployment, the DATABASE and SCHEMA are interchangeable, their meaning are the same. DESC DATABASE deployment ; + -------------------------+------------------------------+ | database_description_item | database_description_value | + -------------------------+------------------------------+ | Database Name | deployment | | Description | Deployment environment | | Location | file : / Users / Temp / deployment . db | + -------------------------+------------------------------+ Related Statements DESCRIBE FUNCTION DESCRIBE TABLE DESCRIBE QUERY

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-describe-function.html
DESCRIBE FUNCTION - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DESCRIBE FUNCTION Description DESCRIBE FUNCTION statement returns the basic metadata information of an existing function. The metadata information includes the function name, implementing class and the usage details. If the optional EXTENDED option is specified, the basic metadata information is returned along with the extended usage information. Syntax { DESC | DESCRIBE } FUNCTION [ EXTENDED ] function_name Parameters function_name Specifies a name of an existing function in the system. The function name may be optionally qualified with a database name. If function_name is qualified with a database then the function is resolved from the user specified database, otherwise it is resolved from the current database. Syntax: [ database_name. ] function_name Examples -- Describe a builtin scalar function. -- Returns function name, implementing class and usage DESC FUNCTION abs ; + -------------------------------------------------------------------+ | function_desc | + -------------------------------------------------------------------+ | Function : abs | | Class : org . apache . spark . sql . catalyst . expressions . Abs | | Usage : abs ( expr ) - Returns the absolute value of the numeric value . | + -------------------------------------------------------------------+ -- Describe a builtin scalar function. -- Returns function name, implementing class and usage and examples. DESC FUNCTION EXTENDED abs ; + -------------------------------------------------------------------+ | function_desc | + -------------------------------------------------------------------+ | Function : abs | | Class : org . apache . spark . sql . catalyst . expressions . Abs | | Usage : abs ( expr ) - Returns the absolute value of the numeric value . | | Extended Usage : | | Examples : | | > SELECT abs ( - 1 ); | | 1 | | | + -------------------------------------------------------------------+ -- Describe a builtin aggregate function DESC FUNCTION max ; + --------------------------------------------------------------+ | function_desc | + --------------------------------------------------------------+ | Function : max | | Class : org . apache . spark . sql . catalyst . expressions . aggregate . Max | | Usage : max ( expr ) - Returns the maximum value of `expr` . | + --------------------------------------------------------------+ -- Describe a builtin user defined aggregate function -- Returns function name, implementing class and usage and examples. DESC FUNCTION EXTENDED explode + ---------------------------------------------------------------+ | function_desc | + ---------------------------------------------------------------+ | Function : explode | | Class : org . apache . spark . sql . catalyst . expressions . Explode | | Usage : explode ( expr ) - Separates the elements of array `expr` | | into multiple rows , or the elements of map `expr` into | | multiple rows and columns . Unless specified otherwise , uses | | the default column name `col` for elements of the array or | | `key` and `value` for the elements of the map . | | Extended Usage : | | Examples : | | > SELECT explode ( array ( 10 , 20 )); | | 10 | | 20 | + ---------------------------------------------------------------+ Related Statements DESCRIBE DATABASE DESCRIBE TABLE DESCRIBE QUERY

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-describe-query.html
DESCRIBE QUERY - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DESCRIBE QUERY Description The DESCRIBE QUERY statement is used to return the metadata of output of a query. A shorthand DESC may be used instead of DESCRIBE to describe the query output. Syntax { DESC | DESCRIBE } [ QUERY ] input_statement Parameters QUERY This clause is optional and may be omitted. input_statement Specifies a result set producing statement and may be one of the following: a SELECT statement a CTE(Common table expression) statement an INLINE TABLE statement a TABLE statement a FROM statement` Please refer to select-statement for a detailed syntax of the query parameter. Examples -- Create table `person` CREATE TABLE person ( name STRING , age INT COMMENT 'Age column' , address STRING ); -- Returns column metadata information for a simple select query DESCRIBE QUERY SELECT age , sum ( age ) FROM person GROUP BY age ; + --------+---------+----------+ | col_name | data_type | comment | + --------+---------+----------+ | age | int | Age column | | sum ( age ) | bigint | null | + --------+---------+----------+ -- Returns column metadata information for common table expression (`CTE`). DESCRIBE QUERY WITH all_names_cte AS ( SELECT name from person ) SELECT * FROM all_names_cte ; + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | name | string | null | + --------+---------+-------+ -- Returns column metadata information for an inline table. DESC QUERY VALUES ( 100 , 'John' , 10000 . 20 D ) AS employee ( id , name , salary ); + --------+---------+-------+ | col_name | data_type | comment | + --------+---------+-------+ | id | int | null | | name | string | null | | salary | double | null | + --------+---------+-------+ -- Returns column metadata information for `TABLE` statement. DESC QUERY TABLE person ; + --------+---------+----------+ | col_name | data_type | comment | + --------+---------+----------+ | name | string | null | | age | int | Agecolumn | | address | string | null | + --------+---------+----------+ -- Returns column metadata information for a `FROM` statement. -- `QUERY` clause is optional and can be omitted. DESCRIBE FROM person SELECT age ; + --------+---------+----------+ | col_name | data_type | comment | + --------+---------+----------+ | age | int | Agecolumn | + --------+---------+----------+ Related Statements DESCRIBE DATABASE DESCRIBE TABLE DESCRIBE FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-describe-table.html
DESCRIBE TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions DESCRIBE TABLE Description DESCRIBE TABLE statement returns the basic metadata information of a table. The metadata information includes column name, column type and column comment. Optionally a partition spec or column name may be specified to return the metadata pertaining to a partition or column respectively. Syntax { DESC | DESCRIBE } [ TABLE ] [ format ] table_identifier [ partition_spec ] [ col_name ] Parameters format Specifies the optional format of describe output. If EXTENDED is specified then additional metadata information (such as parent database, owner, and access time) is returned. table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec An optional parameter that specifies a comma separated list of key and value pairs for partitions. When specified, additional partition metadata is returned. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) col_name An optional parameter that specifies the column name that needs to be described. The supplied column name may be optionally qualified. Parameters partition_spec and col_name are mutually exclusive and can not be specified together. Currently nested columns are not allowed to be specified. Syntax: [ database_name. ] [ table_name. ] column_name Examples -- Creates a table `customer`. Assumes current database is `salesdb`. CREATE TABLE customer ( cust_id INT , state VARCHAR ( 20 ), name STRING COMMENT 'Short name' ) USING parquet PARTITIONED BY ( state ); INSERT INTO customer PARTITION ( state = 'AR' ) VALUES ( 100 , 'Mike' ); -- Returns basic metadata information for unqualified table `customer` DESCRIBE TABLE customer ; + -----------------------+---------+----------+ | col_name | data_type | comment | + -----------------------+---------+----------+ | cust_id | int | null | | name | string | Short name | | state | string | null | |# Partition Information | | | | # col_name | data_type | comment | | state | string | null | + -----------------------+---------+----------+ -- Returns basic metadata information for qualified table `customer` DESCRIBE TABLE salesdb . customer ; + -----------------------+---------+----------+ | col_name | data_type | comment | + -----------------------+---------+----------+ | cust_id | int | null | | name | string | Short name | | state | string | null | |# Partition Information | | | | # col_name | data_type | comment | | state | string | null | + -----------------------+---------+----------+ -- Returns additional metadata such as parent database, owner, access time etc. DESCRIBE TABLE EXTENDED customer ; + ----------------------------+------------------------------+----------+ | col_name | data_type | comment | + ----------------------------+------------------------------+----------+ | cust_id | int | null | | name | string | Short name | | state | string | null | | # Partition Information | | | | # col_name | data_type | comment | | state | string | null | | | | | |# Detailed Table Information | | | | Database | default | | | Table | customer | | | Owner | < TABLE OWNER >| | | Created Time | Tue Apr 07 22 : 56 : 34 JST 2020 | | | Last Access | UNKNOWN | | | Created By | < SPARK VERSION >| | | Type | MANAGED | | | Provider | parquet | | | Location | file : / tmp / salesdb . db / custom ... | | | Serde Library | org . apache . hadoop . hive . ql . i ... | | | InputFormat | org . apache . hadoop . hive . ql . i ... | | | OutputFormat | org . apache . hadoop . hive . ql . i ... | | | Partition Provider | Catalog | | + ----------------------------+------------------------------+----------+ -- Returns partition metadata such as partitioning column name, column type and comment. DESCRIBE TABLE EXTENDED customer PARTITION ( state = 'AR' ); + ------------------------------+------------------------------+----------+ | col_name | data_type | comment | + ------------------------------+------------------------------+----------+ | cust_id | int | null | | name | string | Short name | | state | string | null | | # Partition Information | | | | # col_name | data_type | comment | | state | string | null | | | | | |# Detailed Partition Inform ... | | | | Database | default | | | Table | customer | | | Partition Values | [ state = AR ] | | | Location | file : / tmp / salesdb . db / custom ... | | | Serde Library | org . apache . hadoop . hive . ql . i ... | | | InputFormat | org . apache . hadoop . hive . ql . i ... | | | OutputFormat | org . apache . hadoop . hive . ql . i ... | | | Storage Properties | [ serialization . format = 1 , pa ... | | | Partition Parameters | { transient_lastDdlTime = 1586 ... | | | Created Time | Tue Apr 07 23 : 05 : 43 JST 2020 | | | Last Access | UNKNOWN | | | Partition Statistics | 659 bytes | | | | | | | # Storage Information | | | | Location | file : / tmp / salesdb . db / custom ... | | | Serde Library | org . apache . hadoop . hive . ql . i ... | | | InputFormat | org . apache . hadoop . hive . ql . i ... | | | OutputFormat | org . apache . hadoop . hive . ql . i ... | | + ------------------------------+------------------------------+----------+ -- Returns the metadata for `name` column. -- Optional `TABLE` clause is omitted and column is fully qualified. DESCRIBE customer salesdb . customer . name ; + ---------+----------+ | info_name | info_value | + ---------+----------+ | col_name | name | | data_type | string | | comment | Short name | + ---------+----------+ Related Statements DESCRIBE DATABASE DESCRIBE QUERY DESCRIBE FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-resource-mgmt-list-file.html
LIST FILE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions LIST FILE Description LIST FILE lists the resources added by ADD FILE . Syntax LIST { FILE | FILES } file_name [ ... ] Examples ADD FILE / tmp / test ; ADD FILE / tmp / test_2 ; LIST FILE ; -- output for LIST FILE file : / private / tmp / test file : / private / tmp / test_2 LIST FILE / tmp / test / some / random / file / another / random / file --output file : / private / tmp / test Related Statements ADD FILE ADD JAR ADD ARCHIVE LIST JAR LIST ARCHIVE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-resource-mgmt-list-jar.html
LIST JAR - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions LIST JAR Description LIST JAR lists the JARs added by ADD JAR . Syntax LIST { JAR | JARS } file_name [ ... ] Examples ADD JAR / tmp / test . jar ; ADD JAR / tmp / test_2 . jar ; LIST JAR ; -- output for LIST JAR spark : // 192 . 168 . 1 . 112 : 62859 / jars / test . jar spark : // 192 . 168 . 1 . 112 : 62859 / jars / test_2 . jar LIST JAR / tmp / test . jar / some / random . jar / another / random . jar ; -- output spark : // 192 . 168 . 1 . 112 : 62859 / jars / test . jar Related Statements ADD JAR ADD FILE ADD ARCHIVE LIST FILE LIST ARCHIVE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-cache-refresh.html
REFRESH - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions REFRESH Description REFRESH is used to invalidate and refresh all the cached data (and the associated metadata) for all Datasets that contains the given data source path. Path matching is by prefix, i.e. “/” would invalidate everything that is cached. Syntax REFRESH resource_path Parameters resource_path The path of the resource that is to be refreshed. Examples -- The Path is resolved using the datasource's File Index. CREATE TABLE test ( ID INT ) using parquet ; INSERT INTO test SELECT 1000 ; CACHE TABLE test ; INSERT INTO test SELECT 100 ; REFRESH "hdfs://path/to/table" ; Related Statements CACHE TABLE CLEAR CACHE UNCACHE TABLE REFRESH TABLE REFRESH FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-cache-refresh-table.html
REFRESH TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions REFRESH TABLE Description REFRESH TABLE statement invalidates the cached entries, which include data and metadata of the given table or view. The invalidated cache is populated in lazy manner when the cached table or the query associated with it is executed again. Syntax REFRESH [ TABLE ] table_identifier Parameters table_identifier Specifies a table name, which is either a qualified or unqualified name that designates a table/view. If no database identifier is provided, it refers to a temporary view or a table/view in the current database. Syntax: [ database_name. ] table_name Examples -- The cached entries of the table will be refreshed -- The table is resolved from the current database as the table name is unqualified. REFRESH TABLE tbl1 ; -- The cached entries of the view will be refreshed or invalidated -- The view is resolved from tempDB database, as the view name is qualified. REFRESH TABLE tempDB . view1 ; Related Statements CACHE TABLE CLEAR CACHE UNCACHE TABLE REFRESH REFRESH FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-cache-refresh-function.html
REFRESH FUNCTION - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions REFRESH FUNCTION Description REFRESH FUNCTION statement invalidates the cached function entry, which includes a class name and resource location of the given function. The invalidated cache is populated right away. Note that REFRESH FUNCTION only works for permanent functions. Refreshing native functions or temporary functions will cause an exception. Syntax REFRESH FUNCTION function_identifier Parameters function_identifier Specifies a function name, which is either a qualified or unqualified name. If no database identifier is provided, uses the current database. Syntax: [ database_name. ] function_name Examples -- The cached entry of the function will be refreshed -- The function is resolved from the current database as the function name is unqualified. REFRESH FUNCTION func1 ; -- The cached entry of the function will be refreshed -- The function is resolved from tempDB database as the function name is qualified. REFRESH FUNCTION db1 . func1 ; Related Statements CACHE TABLE CLEAR CACHE UNCACHE TABLE REFRESH TABLE REFRESH

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-conf-mgmt-reset.html
RESET - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions RESET Description The RESET command resets runtime configurations specific to the current session which were set via the SET command to their default values. Syntax RESET ; RESET configuration_key ; Parameters (none) Reset any runtime configurations specific to the current session which were set via the SET command to their default values. configuration_key Restore the value of the configuration_key to the default value. If the default value is undefined, the configuration_key will be removed. Examples -- Reset any runtime configurations specific to the current session which were set via the SET command to their default values. RESET ; -- If you start your application with --conf spark.foo=bar and set spark.foo=foobar in runtime, the example below will restore it to 'bar'. If spark.foo is not specified during starting, the example below will remove this config from the SQLConf. It will ignore nonexistent keys. RESET spark . abc ; Related Statements SET

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-conf-mgmt-set.html
SET - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SET Description The SET command sets a property, returns the value of an existing property or returns all SQLConf properties with value and meaning. Syntax SET SET [ - v ] SET property_key [ = property_value ] Parameters -v Outputs the key, value and meaning of existing SQLConf properties. property_key Returns the value of specified property key. property_key=property_value Sets the value for a given property key. If an old value exists for a given property key, then it gets overridden by the new value. Examples -- Set a property. SET spark . sql . variable . substitute = false ; -- List all SQLConf properties with value and meaning. SET - v ; -- List all SQLConf properties with value for current session. SET ; -- List the value of specified property key. SET spark . sql . variable . substitute ; + -----------------------------+-----+ | key | value | + -----------------------------+-----+ | spark . sql . variable . substitute | false | + -----------------------------+-----+ Related Statements RESET

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-columns.html
SHOW COLUMNS - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW COLUMNS Description Returns the list of columns in a table. If the table does not exist, an exception is thrown. Syntax SHOW COLUMNS table_identifier [ database ] Parameters table_identifier Specifies the table name of an existing table. The table may be optionally qualified with a database name. Syntax: { IN | FROM } [ database_name . ] table_name Note: Keywords IN and FROM are interchangeable. database Specifies an optional database name. The table is resolved from this database when it is specified. When this parameter is specified then table name should not be qualified with a different database name. Syntax: { IN | FROM } database_name Note: Keywords IN and FROM are interchangeable. Examples -- Create `customer` table in `salesdb` database; USE salesdb ; CREATE TABLE customer ( cust_cd INT , name VARCHAR ( 100 ), cust_addr STRING ); -- List the columns of `customer` table in current database. SHOW COLUMNS IN customer ; + ---------+ | col_name | + ---------+ | cust_cd | | name | | cust_addr | + ---------+ -- List the columns of `customer` table in `salesdb` database. SHOW COLUMNS IN salesdb . customer ; + ---------+ | col_name | + ---------+ | cust_cd | | name | | cust_addr | + ---------+ -- List the columns of `customer` table in `salesdb` database SHOW COLUMNS IN customer IN salesdb ; + ---------+ | col_name | + ---------+ | cust_cd | | name | | cust_addr | + ---------+ Related Statements DESCRIBE TABLE SHOW TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-create-table.html
SHOW CREATE TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW CREATE TABLE Description SHOW CREATE TABLE returns the CREATE TABLE statement or CREATE VIEW statement that was used to create a given table or view. SHOW CREATE TABLE on a non-existent table or a temporary view throws an exception. Syntax SHOW CREATE TABLE table_identifier [ AS SERDE ] Parameters table_identifier Specifies a table or view name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name AS SERDE Generates Hive DDL for a Hive SerDe table. Examples CREATE TABLE test ( c INT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE TBLPROPERTIES ( 'prop1' = 'value1' , 'prop2' = 'value2' ); SHOW CREATE TABLE test ; + ----------------------------------------------------+ | createtab_stmt | + ----------------------------------------------------+ | CREATE TABLE `default` . `test` ( `c` INT ) USING text TBLPROPERTIES ( 'transient_lastDdlTime' = '1586269021' , 'prop1' = 'value1' , 'prop2' = 'value2' ) + ----------------------------------------------------+ SHOW CREATE TABLE test AS SERDE ; + ------------------------------------------------------------------------------+ | createtab_stmt | + ------------------------------------------------------------------------------+ | CREATE TABLE `default` . `test` ( `c` INT ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ( 'serialization.format' = ',' , 'field.delim' = ',' ) STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' TBLPROPERTIES ( 'prop1' = 'value1' , 'prop2' = 'value2' , 'transient_lastDdlTime' = '1641800515' ) + ------------------------------------------------------------------------------+ Related Statements CREATE TABLE CREATE VIEW

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-databases.html
SHOW DATABASES - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW DATABASES Description Lists the databases that match an optionally supplied regular expression pattern. If no pattern is supplied then the command lists all the databases in the system. Please note that the usage of SCHEMAS and DATABASES are interchangeable and mean the same thing. Syntax SHOW { DATABASES | SCHEMAS } [ LIKE regex_pattern ] Parameters regex_pattern Specifies a regular expression pattern that is used to filter the results of the statement. Except for * and | character, the pattern works like a regular expression. * alone matches 0 or more characters and | is used to separate multiple different regular expressions, any of which can match. The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive. Examples -- Create database. Assumes a database named `default` already exists in -- the system. CREATE DATABASE payroll_db ; CREATE DATABASE payments_db ; -- Lists all the databases. SHOW DATABASES ; + ------------+ | databaseName | + ------------+ | default | | payments_db | | payroll_db | + ------------+ -- Lists databases with name starting with string pattern `pay` SHOW DATABASES LIKE 'pay*' ; + ------------+ | databaseName | + ------------+ | payments_db | | payroll_db | + ------------+ -- Lists all databases. Keywords SCHEMAS and DATABASES are interchangeable. SHOW SCHEMAS ; + ------------+ | databaseName | + ------------+ | default | | payments_db | | payroll_db | + ------------+ Related Statements DESCRIBE DATABASE CREATE DATABASE ALTER DATABASE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-functions.html
SHOW FUNCTIONS - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW FUNCTIONS Description Returns the list of functions after applying an optional regex pattern. Given number of functions supported by Spark is quite large, this statement in conjunction with describe function may be used to quickly find the function and understand its usage. The LIKE clause is optional and supported only for compatibility with other systems. Syntax SHOW [ function_kind ] FUNCTIONS [ { FROM | IN } database_name ] [ LIKE regex_pattern ] Parameters function_kind Specifies the name space of the function to be searched upon. The valid name spaces are : USER - Looks up the function(s) among the user defined functions. SYSTEM - Looks up the function(s) among the system defined functions. ALL - Looks up the function(s) among both user and system defined functions. { FROM | IN } database_name Specifies the database name from which functions are listed. regex_pattern Specifies a regular expression pattern that is used to filter the results of the statement. Except for * and | character, the pattern works like a regular expression. * alone matches 0 or more characters and | is used to separate multiple different regular expressions, any of which can match. The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive. Examples -- List a system function `trim` by searching both user defined and system -- defined functions. SHOW FUNCTIONS trim ; + --------+ | function | + --------+ | trim | + --------+ -- List a system function `concat` by searching system defined functions. SHOW SYSTEM FUNCTIONS concat ; + --------+ | function | + --------+ | concat | + --------+ -- List a qualified function `max` from database `salesdb`. SHOW SYSTEM FUNCTIONS FROM salesdb LIKE 'max' ; + --------+ | function | + --------+ | max | + --------+ -- List all functions starting with `t` SHOW FUNCTIONS LIKE 't*' ; + -----------------+ | function | + -----------------+ | tan | | tanh | | timestamp | | tinyint | | to_csv | | to_date | | to_json | | to_timestamp | | to_unix_timestamp | | to_utc_timestamp | | transform | | transform_keys | | transform_values | | translate | | trim | | trunc | | typeof | + -----------------+ -- List all functions starting with `yea` or `windo` SHOW FUNCTIONS LIKE 'yea*|windo*' ; + --------+ | function | + --------+ | window | | year | + --------+ -- Use normal regex pattern to list function names that has 4 characters -- with `t` as the starting character. SHOW FUNCTIONS LIKE 't[a-z][a-z][a-z]' ; + --------+ | function | + --------+ | tanh | | trim | + --------+ Related Statements DESCRIBE FUNCTION

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-partitions.html
SHOW PARTITIONS - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW PARTITIONS Description The SHOW PARTITIONS statement is used to list partitions of a table. An optional partition spec may be specified to return the partitions matching the supplied partition spec. Syntax SHOW PARTITIONS table_identifier [ partition_spec ] Parameters table_identifier Specifies a table name, which may be optionally qualified with a database name. Syntax: [ database_name. ] table_name partition_spec An optional parameter that specifies a comma separated list of key and value pairs for partitions. When specified, the partitions that match the partition specification are returned. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) Examples -- create a partitioned table and insert a few rows. USE salesdb ; CREATE TABLE customer ( id INT , name STRING ) PARTITIONED BY ( state STRING , city STRING ); INSERT INTO customer PARTITION ( state = 'CA' , city = 'Fremont' ) VALUES ( 100 , 'John' ); INSERT INTO customer PARTITION ( state = 'CA' , city = 'San Jose' ) VALUES ( 200 , 'Marry' ); INSERT INTO customer PARTITION ( state = 'AZ' , city = 'Peoria' ) VALUES ( 300 , 'Daniel' ); -- Lists all partitions for table `customer` SHOW PARTITIONS customer ; + ----------------------+ | partition | + ----------------------+ | state = AZ / city = Peoria | | state = CA / city = Fremont | | state = CA / city = San Jose | + ----------------------+ -- Lists all partitions for the qualified table `customer` SHOW PARTITIONS salesdb . customer ; + ----------------------+ | partition | + ----------------------+ | state = AZ / city = Peoria | | state = CA / city = Fremont | | state = CA / city = San Jose | + ----------------------+ -- Specify a full partition spec to list specific partition SHOW PARTITIONS customer PARTITION ( state = 'CA' , city = 'Fremont' ); + ---------------------+ | partition | + ---------------------+ | state = CA / city = Fremont | + ---------------------+ -- Specify a partial partition spec to list the specific partitions SHOW PARTITIONS customer PARTITION ( state = 'CA' ); + ----------------------+ | partition | + ----------------------+ | state = CA / city = Fremont | | state = CA / city = San Jose | + ----------------------+ -- Specify a partial spec to list specific partition SHOW PARTITIONS customer PARTITION ( city = 'San Jose' ); + ----------------------+ | partition | + ----------------------+ | state = CA / city = San Jose | + ----------------------+ Related Statements CREATE TABLE INSERT STATEMENT DESCRIBE TABLE SHOW TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-table.html
SHOW TABLE EXTENDED - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW TABLE EXTENDED Description SHOW TABLE EXTENDED will show information for all tables matching the given regular expression. Output includes basic table information and file system information like Last Access , Created By , Type , Provider , Table Properties , Location , Serde Library , InputFormat , OutputFormat , Storage Properties , Partition Provider , Partition Columns and Schema . If a partition specification is present, it outputs the given partition’s file-system-specific information such as Partition Parameters and Partition Statistics . Note that a table regex cannot be used with a partition specification. Syntax SHOW TABLE EXTENDED [ { IN | FROM } database_name ] LIKE regex_pattern [ partition_spec ] Parameters { IN | FROM } database_name Specifies database name. If not provided, will use the current database. regex_pattern Specifies the regular expression pattern that is used to filter out unwanted tables. Except for * and | character, the pattern works like a regular expression. * alone matches 0 or more characters and | is used to separate multiple different regular expressions, any of which can match. The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive. partition_spec An optional parameter that specifies a comma separated list of key and value pairs for partitions. Note that a table regex cannot be used with a partition specification. Syntax: PARTITION ( partition_col_name = partition_col_val [ , ... ] ) Examples -- Assumes `employee` table created with partitioned by column `grade` CREATE TABLE employee ( name STRING , grade INT ) PARTITIONED BY ( grade ); INSERT INTO employee PARTITION ( grade = 1 ) VALUES ( 'sam' ); INSERT INTO employee PARTITION ( grade = 2 ) VALUES ( 'suj' ); -- Show the details of the table SHOW TABLE EXTENDED LIKE 'employee' ; + --------+---------+-----------+--------------------------------------------------------------+ | database | tableName | isTemporary | information | + --------+---------+-----------+--------------------------------------------------------------+ | default | employee | false | Database : default Table : employee Owner : root Created Time : Fri Aug 30 15 : 10 : 21 IST 2019 Last Access : Thu Jan 01 05 : 30 : 00 IST 1970 Created By : Spark 3 . 0 . 0 - SNAPSHOT Type : MANAGED Provider : hive Table Properties : [ transient_lastDdlTime = 1567158021 ] Location : file : / opt / spark1 / spark / spark - warehouse / employee Serde Library : org . apache . hadoop . hive . serde2 . lazy . LazySimpleSerDe InputFormat : org . apache . hadoop . mapred . TextInputFormat OutputFormat : org . apache . hadoop . hive . ql . io . HiveIgnoreKeyTextOutputFormat Storage Properties : [ serialization . format = 1 ] Partition Provider : Catalog Partition Columns : [ `grade` ] Schema : root | -- name: string (nullable = true) | -- grade: integer (nullable = true) + --------+---------+-----------+--------------------------------------------------------------+ -- showing the multiple table details with pattern matching SHOW TABLE EXTENDED LIKE 'employe*' ; + --------+---------+-----------+--------------------------------------------------------------+ | database | tableName | isTemporary | information | + --------+---------+-----------+--------------------------------------------------------------+ | default | employee | false | Database : default Table : employee Owner : root Created Time : Fri Aug 30 15 : 10 : 21 IST 2019 Last Access : Thu Jan 01 05 : 30 : 00 IST 1970 Created By : Spark 3 . 0 . 0 - SNAPSHOT Type : MANAGED Provider : hive Table Properties : [ transient_lastDdlTime = 1567158021 ] Location : file : / opt / spark1 / spark / spark - warehouse / employee Serde Library : org . apache . hadoop . hive . serde2 . lazy . LazySimpleSerDe InputFormat : org . apache . hadoop . mapred . TextInputFormat OutputFormat : org . apache . hadoop . hive . ql . io . HiveIgnoreKeyTextOutputFormat Storage Properties : [ serialization . format = 1 ] Partition Provider : Catalog Partition Columns : [ `grade` ] Schema : root | -- name: string (nullable = true) | -- grade: integer (nullable = true) | default | employee1 | false | Database : default Table : employee1 Owner : root Created Time : Fri Aug 30 15 : 22 : 33 IST 2019 Last Access : Thu Jan 01 05 : 30 : 00 IST 1970 Created By : Spark 3 . 0 . 0 - SNAPSHOT Type : MANAGED Provider : hive Table Properties : [ transient_lastDdlTime = 1567158753 ] Location : file : / opt / spark1 / spark / spark - warehouse / employee1 Serde Library : org . apache . hadoop . hive . serde2 . lazy . LazySimpleSerDe InputFormat : org . apache . hadoop . mapred . TextInputFormat OutputFormat : org . apache . hadoop . hive . ql . io . HiveIgnoreKeyTextOutputFormat Storage Properties : [ serialization . format = 1 ] Partition Provider : Catalog Schema : root | -- name: string (nullable = true) + --------+---------+----------+---------------------------------------------------------------+ -- show partition file system details SHOW TABLE EXTENDED IN default LIKE 'employee' PARTITION ( grade = 1 ); + --------+---------+-----------+--------------------------------------------------------------+ | database | tableName | isTemporary | information | + --------+---------+-----------+--------------------------------------------------------------+ | default | employee | false | Partition Values : [ grade = 1 ] Location : file : / opt / spark1 / spark / spark - warehouse / employee / grade = 1 Serde Library : org . apache . hadoop . hive . serde2 . lazy . LazySimpleSerDe InputFormat : org . apache . hadoop . mapred . TextInputFormat OutputFormat : org . apache . hadoop . hive . ql . io . HiveIgnoreKeyTextOutputFormat Storage Properties : [ serialization . format = 1 ] Partition Parameters : { rawDataSize =- 1 , numFiles = 1 , transient_lastDdlTime = 1567158221 , totalSize = 4 , COLUMN_STATS_ACCURATE = false , numRows =- 1 } Created Time : Fri Aug 30 15 : 13 : 41 IST 2019 Last Access : Thu Jan 01 05 : 30 : 00 IST 1970 Partition Statistics : 4 bytes | + --------+---------+-----------+--------------------------------------------------------------+ -- show partition file system details with regex fails as shown below SHOW TABLE EXTENDED IN default LIKE 'empl*' PARTITION ( grade = 1 ); Error : Error running query : org . apache . spark . sql . catalyst . analysis . NoSuchTableException : Table or view 'emplo*' not found in database 'default' ; ( state = , code = 0 ) Related Statements CREATE TABLE DESCRIBE TABLE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-tables.html
SHOW TABLES - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW TABLES Description The SHOW TABLES statement returns all the tables for an optionally specified database. Additionally, the output of this statement may be filtered by an optional matching pattern. If no database is specified then the tables are returned from the current database. Syntax SHOW TABLES [ { FROM | IN } database_name ] [ LIKE regex_pattern ] Parameters { FROM | IN } database_name Specifies the database name from which tables are listed. regex_pattern Specifies the regular expression pattern that is used to filter out unwanted tables. Except for * and | character, the pattern works like a regular expression. * alone matches 0 or more characters and | is used to separate multiple different regular expressions, any of which can match. The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive. Examples -- List all tables in default database SHOW TABLES ; + --------+---------+-----------+ | database | tableName | isTemporary | + --------+---------+-----------+ | default | sam | false | | default | sam1 | false | | default | suj | false | + --------+---------+-----------+ -- List all tables from userdb database SHOW TABLES FROM userdb ; + --------+---------+-----------+ | database | tableName | isTemporary | + --------+---------+-----------+ | userdb | user1 | false | | userdb | user2 | false | + --------+---------+-----------+ -- List all tables in userdb database SHOW TABLES IN userdb ; + --------+---------+-----------+ | database | tableName | isTemporary | + --------+---------+-----------+ | userdb | user1 | false | | userdb | user2 | false | + --------+---------+-----------+ -- List all tables from default database matching the pattern `sam*` SHOW TABLES FROM default LIKE 'sam*' ; + --------+---------+-----------+ | database | tableName | isTemporary | + --------+---------+-----------+ | default | sam | false | | default | sam1 | false | + --------+---------+-----------+ -- List all tables matching the pattern `sam*|suj` SHOW TABLES LIKE 'sam*|suj' ; + --------+---------+-----------+ | database | tableName | isTemporary | + --------+---------+-----------+ | default | sam | false | | default | sam1 | false | | default | suj | false | + --------+---------+-----------+ Related Statements CREATE TABLE DROP TABLE CREATE DATABASE DROP DATABASE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-tblproperties.html
SHOW TBLPROPERTIES - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW TBLPROPERTIES Description This statement returns the value of a table property given an optional value for a property key. If no key is specified then all the properties are returned. Syntax SHOW TBLPROPERTIES table_identifier [ ( unquoted_property_key | property_key_as_string_literal ) ] Parameters table_identifier Specifies the table name of an existing table. The table may be optionally qualified with a database name. Syntax: [ database_name. ] table_name unquoted_property_key Specifies the property key in unquoted form. The key may consists of multiple parts separated by dot. Syntax: [ key_part1 ] [ .key_part2 ] [ ... ] property_key_as_string_literal Specifies a property key value as a string literal. Note Property value returned by this statement excludes some properties that are internal to spark and hive. The excluded properties are : All the properties that start with prefix spark.sql Property keys such as: EXTERNAL , comment All the properties generated internally by hive to store statistics. Some of these properties are: numFiles , numPartitions , numRows . Examples -- create a table `customer` in database `salesdb` USE salesdb ; CREATE TABLE customer ( cust_code INT , name VARCHAR ( 100 ), cust_addr STRING ) TBLPROPERTIES ( 'created.by.user' = 'John' , 'created.date' = '01-01-2001' ); -- show all the user specified properties for table `customer` SHOW TBLPROPERTIES customer ; + ---------------------+----------+ | key | value | + ---------------------+----------+ | created . by . user | John | | created . date | 01 - 01 - 2001 | | transient_lastDdlTime | 1567554931 | + ---------------------+----------+ -- show all the user specified properties for a qualified table `customer` -- in database `salesdb` SHOW TBLPROPERTIES salesdb . customer ; + ---------------------+----------+ | key | value | + ---------------------+----------+ | created . by . user | John | | created . date | 01 - 01 - 2001 | | transient_lastDdlTime | 1567554931 | + ---------------------+----------+ -- show value for unquoted property key `created.by.user` SHOW TBLPROPERTIES customer ( created . by . user ); + -----+ | value | + -----+ | John | + -----+ -- show value for property `created.date`` specified as string literal SHOW TBLPROPERTIES customer ( 'created.date' ); + ----------+ | value | + ----------+ | 01 - 01 - 2001 | + ----------+ Related Statements CREATE TABLE ALTER TABLE SET TBLPROPERTIES SHOW TABLES SHOW TABLE EXTENDED

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-show-views.html
SHOW VIEWS - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions SHOW VIEWS Description The SHOW VIEWS statement returns all the views for an optionally specified database. Additionally, the output of this statement may be filtered by an optional matching pattern. If no database is specified then the views are returned from the current database. If the specified database is global temporary view database, we will list global temporary views. Note that the command also lists local temporary views regardless of a given database. Syntax SHOW VIEWS [ { FROM | IN } database_name ] [ LIKE regex_pattern ] Parameters { FROM | IN } database_name Specifies the database name from which views are listed. regex_pattern Specifies the regular expression pattern that is used to filter out unwanted views. Except for * and | character, the pattern works like a regular expression. * alone matches 0 or more characters and | is used to separate multiple different regular expressions, any of which can match. The leading and trailing blanks are trimmed in the input pattern before processing. The pattern match is case-insensitive. Examples -- Create views in different databases, also create global/local temp views. CREATE VIEW sam AS SELECT id , salary FROM employee WHERE name = 'sam' ; CREATE VIEW sam1 AS SELECT id , salary FROM employee WHERE name = 'sam1' ; CREATE VIEW suj AS SELECT id , salary FROM employee WHERE name = 'suj' ; USE userdb ; CREATE VIEW user1 AS SELECT id , salary FROM default . employee WHERE name = 'user1' ; CREATE VIEW user2 AS SELECT id , salary FROM default . employee WHERE name = 'user2' ; USE default ; CREATE GLOBAL TEMP VIEW temp1 AS SELECT 1 AS col1 ; CREATE TEMP VIEW temp2 AS SELECT 1 AS col1 ; -- List all views in default database SHOW VIEWS ; + -------------+------------+--------------+ | namespace | viewName | isTemporary | + -------------+------------+--------------+ | default | sam | false | | default | sam1 | false | | default | suj | false | | | temp2 | true | + -------------+------------+--------------+ -- List all views from userdb database SHOW VIEWS FROM userdb ; + -------------+------------+--------------+ | namespace | viewName | isTemporary | + -------------+------------+--------------+ | userdb | user1 | false | | userdb | user2 | false | | | temp2 | true | + -------------+------------+--------------+ -- List all views in global temp view database SHOW VIEWS IN global_temp ; + -------------+------------+--------------+ | namespace | viewName | isTemporary | + -------------+------------+--------------+ | global_temp | temp1 | true | | | temp2 | true | + -------------+------------+--------------+ -- List all views from default database matching the pattern `sam*` SHOW VIEWS FROM default LIKE 'sam*' ; + -----------+------------+--------------+ | namespace | viewName | isTemporary | + -----------+------------+--------------+ | default | sam | false | | default | sam1 | false | + -----------+------------+--------------+ -- List all views from the current database matching the pattern `sam|sujï½temp*` SHOW VIEWS LIKE 'sam|suj|temp*' ; + -------------+------------+--------------+ | namespace | viewName | isTemporary | + -------------+------------+--------------+ | default | sam | false | | default | suj | false | | | temp2 | true | + -------------+------------+--------------+ Related statements CREATE VIEW DROP VIEW CREATE DATABASE DROP DATABASE

URL: https://spark.apache.org/docs/3.5.0/sql-ref-syntax-aux-cache-uncache-table.html
UNCACHE TABLE - Spark 3.5.0 Documentation 3.5.0 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) PySpark (Python on Spark) API Docs Scala Java Python R SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling Security Hardware Provisioning Migration Guide Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide SQL Reference ANSI Compliance Data Types Datetime Pattern Number Pattern Functions Identifiers Literals Null Semantics SQL Syntax Data Definition Statements Data Manipulation Statements Data Retrieval(Queries) Auxiliary Statements Error Conditions UNCACHE TABLE Description UNCACHE TABLE removes the entries and associated data from the in-memory and/or on-disk cache for a given table or view. The underlying entries should already have been brought to cache by previous CACHE TABLE operation. UNCACHE TABLE on a non-existent table throws an exception if IF EXISTS is not specified. Syntax UNCACHE TABLE [ IF EXISTS ] table_identifier Parameters table_identifier Specifies the table or view name to be uncached. The table or view name may be optionally qualified with a database name. Syntax: [ database_name. ] table_name Examples UNCACHE TABLE t1 ; Related Statements CACHE TABLE CLEAR CACHE REFRESH TABLE REFRESH REFRESH FUNCTION

